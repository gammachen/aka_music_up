## 深度学习全书 -

## 公式+推导+代码+TensorFlow 全程案例

$$
B E H \exists H \odot\Xi
$$

$$
\frac{1} {2 0} \equiv\frac{1} {2 0} \times\frac{1} {2 0} \times\frac{1} {2 0} \times\frac{1} {2 0} \times\frac{1} {2 0} \times\frac{1} {2 0} \times\frac{1} {2 0} \times\frac{1} {2 0}
$$
$$
\lambda\equiv\vert\pm\vert H \pm\vert\pm\vert\pm\vert\vert H \vert=\vert\pm\vert\vert
$$
$$
\pi=\frac{1} {2} \pi=\pi\pi\pi\pi\pi\frac{1} {2} \pi\frac{1} {2} \pi\frac{1} {2} \pi\frac{1} {2} \pi\frac{1} {2} \pi\frac{1} {2} \pi\frac{1} {2} \pi\frac{1} {2} \pi\frac{1} {2} \pi\frac{1} {2} \pi\frac{1} {2} \pi\frac{1} {2} \pi\frac{1} {2} \pi\frac{1} {2} \pi\frac{1} {2} \pi\frac{1} {2} \pi\frac{1} {2} \pi\frac{1} {2} \pi\cdot\pi\cdot\pi\cdot\pi
$$
$$
A \vert\begin{matrix} \because\sqrt{7} \times\frac{1} {2} \end{matrix} \therefore\vert\begin{matrix} \\ \times\sqrt{7} \times\frac{1} {2} \end{matrix} \times\vert\begin{matrix} \\ \therefore\\ \sqrt{7} \end{matrix} \times\vert\begin{matrix} \\ \sqrt{7} \end{matrix} \vert\times\vert\frac{1} {2} \vert\end{matrix}
$$
神经网络原理与实例
卷积神经网络（CNN
$$
\boxed{\Xi}=\pi\Delta\pi\cup\left( \ Y O L O \right)
$$
$$
\therefore\frac{2 0 0} {3} \times\frac{1} {3} \times\frac{1 0} {3} \times\frac{1 0} {3} \times\frac{1 0} {3} \times\frac{1 0} {3} \times\frac{1 0} {3} \times\frac{1 0} {3} \times\frac{1 0} {3} \times\frac{1 0} {3} \times\frac{1 0} {3} \times\frac{1 0} {3} \times\frac{1 0} {3} \times\frac{1 0} {3} \times\frac{1 0} {3} \cdot\cdot\cdot
$$

$$
\frac{1} {1-\lambda} \pm\pi+\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\ .
$$

车牌辨识(ANPR)
$$
\lambda H_{1 1} \vert\vert=\vert\vert
$$
$$
\pm\sqrt{x} \times\frac{1} {3} \sqrt{x} \times\sqrt{x} \times\frac{1} {3} \times\frac{1} {3} ( G A N )
$$
$$
\^{\star} \approx\mathrm{A E} \left( D e e p F a \tt K e \right)
$$
$$
\begin{matrix} \therefore\phi\times\pm\Delta=\Delta\cup\Delta\cup\Delta=\Delta\cup\Delta=\Delta=\Delta=\Delta\Delta=\Delta=\Delta=\Delta=\Delta=\Delta.} \\ \end{matrix}
$$
$$
\mathbb{M} \pm\mathbb{M} \frac{\mathrm{n o t}} {\mathrm{n o t}} \times( \mathbf{C h a t B o t} )
$$
$$
\therefore\boxed{\Xi} \boxed{\vert\vert\vert\vert\vert\vert\vert} ( \mathbf{A S R} )
$$
$$
3 \times\angle C \cong3 ( R L )
$$

## 深度学习全书

## 公式+推导+代码+TensorFlow 全程案例

$$
B E B B H \textcircled{=} \frac{\mathbf{E}} {\mathbf{E}}
$$

$$
\frac{\Delta} {\Delta}=\frac{\Delta} {\Delta}=\frac{\Delta} {\Delta}=\frac{\Delta} {\Delta}=\frac{\Delta} {\Delta}=\frac{\Delta} {\Delta}=\frac{\Delta} {\Delta}=\frac{\Delta} {\Delta}=\frac{\Delta} {\Delta}=\frac{\Delta} {\Delta}=\frac{\Delta} {\Delta}=\frac{\Delta} {\Delta}=\frac{\Delta} {\Delta}=\frac{\Delta} {\Delta}=\frac{\Delta} {\Delta}=\frac{\Delta} {\Delta}
$$
$$
\lambda\equiv\vert\pm\vert\pm\vert\pm\vert\pm\vert\pm\vert\pm\vert\pm\vert\pm\vert\pm\vert\vert\pm\vert\vert\pm\vert\vert\pm\vert\vert\vert\pm\vert\vert\vert\vert\Xi\vert\vert\vert\Xi\vert\vert\vert\Xi\vert\vert\Xi\vert\vert\Xi\vert\vert\Xi\vert\Xi\vert\Xi\vert\Xi\vert\Xi\vert\Xi\vert\Xi\vert\Xi\vert
$$
$$
\frac{\pi} {R}=\frac{1} {R} F+F \frac{1} {R}=\frac{1} {R}=\frac{1} {R} F+\frac{1} {R}=\frac{1} {R} F+\frac{1} {R}
$$
 $\mu$ 专题完整程序实例
$$
\therefore\angle\angle\angle\angle\angle E E ( g )
$$
$$
\textcircled{5}=\frac{3+\frac{4} {2}} {2} \pm\frac{4} {2} \pm\frac{4} {2} ( C N N )
$$
$$
\boxed{\boxed{m} \neq\frac{1} {2} \pi\cdot\square( \mathsf{Y O L O} )}
$$
$$
x_{3}=x_{3}=x_{3}=x_{4} ( O C R )
$$

$$
\frac{4 \vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\
$$
人脸识别
$$
\pm\vert\overrightarrow{x} \times\vert+\overrightarrow{n} \vert\overrightarrow{x} \vert\cong\vert\overrightarrow{n} ( G A N )
$$
$$
\^{\star} \star\mathbb{A} \equiv\left( \texttt{D e e p F a k e} \right)
$$
$$
\therefore\Delta\times\pm\Delta=\Delta=( N L P )
$$
$$
\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\ \ 1 \ 1 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 7 \ \ 7 \ \ 7 \ 7 \ \ \ \ 7 \ \ 7 \ 7 \ 7 \ \ 7 \ 7 \ \ \ 7 \ \ 7 \ 7 \ 7 \ 7 \ \ 7 \ \ 7 \ 7 \ \ 7 \ \ 7 \ \ \ 7 \ 7 \ 7 \ \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7
$$
$$
\therefore\frac{i \pi\vert} {\vert\Xi\vert} \vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\vert\end\end{matrix} \rangle\rangle\rangle\rangle\rangle\vert\rangle\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\cdot\\ \\ \\ \\ \\ \\ \\ \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
$$
$$
3 E / / c^{2}=\Xi( R L )
$$

$$
\Xi=\pm[ \pm] \pm[ \pm] \equiv\pm[ \pm] \pm[ \pm] \pm[ \pm] \pm[ \pm] \pm[ \pm] \pm[ \pm] \pm[ \pm] \pm[ \pm] \pm[ \pm] \pm[ \pm] \pm[ \pm] \hspace{. 5 c m} \equiv\Xi^{\prime} \equiv[ \pm] \pm[ \pm] \mp[ \pm[ \pm] \pm[ \pm] \mp[ \pm] \mp[ \pm] \hspace{. 5 c m} \equiv\Xi^{\prime} \in\Gamma^{\prime} \times[ \pm[ \pm] \mp[ \pm] \mp[ \pm[ \pm] \mp[ \pm] \mp[ \pm] \hspace{. 5 c m} \Xi^{\prime} \equiv[ \pm[ \pm] \mp[ \pm[ \pm] \mp[ \pm] \mp[ \pm] \mp[ \pm] \mp[ \pm] \mp[ \pm] \hspace{. 5 c m}
$$

书名：深度学习全书公式 $^\dagger$ 推导 $\dagger$ 代码＋TensorFlow全程案例

本作品由清华大学出版社有限 $\big<$ 司授权知乎（北京智者天下科技有限 $\operatorname{Z p} ( \overline{{\Xi}} )$ 电子版制作与发行

 $\small\Gamma_{\mathrm{F}}$ 者：洪锦魁.陈昭明

版权方：清华大学出版社

$$
\mathrm{I S B N \colon~ 9 7 8 7 3 0 2 6 1 0 3 0 4}
$$

◎版权所有·侵权必究

$$
\frac{3+1} {1 0 0} \frac{4} {1 0 0} \frac{5 0} {1 0 0} \frac{5 0} {1 0 0} \frac{5 0} {1 0 0} \frac{5} {1 0 0} \frac{5} {1 0 0}
$$

## 深度学习全书公式+推导+代码+TensorFlow 全程案例

陈昭明◎著

浦羊大学山服社

全书共15章，分为 $5$ 篇，第一篇说明深度学习的概念，包括数理基础，特点是结合编程解题，加深读者印象，第二篇说明
TensorFlow 的学习地图，从张量、自动微分、梯度下降乃至神经层的实践，逐步解构神经网络，第三篇介绍CNN算法、影像应用、 转移学习等，第四篇则进入自然语言处理及语音识别的领域，介绍 RNN/BERT/Transtormer算法、相关应用等，最 $\sqrt{\Xi}$ ，介绍强化学习的基础知识，包括马尔可大决策过程、动态规划、蒙特卡洛、Q Learning算法，当然，还有相关案例实践。

第 $1$ 章介绍AI的发展趋势，鉴古知今，了解前两波 $\mathbf{A} \mathbf{I}$ 失败的原因，比较第三波发展的差异性。

第 $2$ 章介绍深度学习必各的统计/数学基础，不仅要理解相关知 $\grave{\imath} \mathbb{R}$ ，也力求能撰写程序解题。

第 $3$ 章介绍TensorFlow基本功能，包括张量运算、自动微分及神经网络层的组成，并说明梯度下降法求解的过程。

第 $4$ 章开始实践，依照机器学习的一项流程，撰写完整的范例，包括Web、桌面程序

## 内容简介

各章详述如下：

第 $5$ 章介绍TensorFlow 进阶功能，包括各种工具，如 TensorBoard、 TensorFlow Serving、Callbacks.

第 $6 \! \sim\! 1 0$ 章介绍图像/视讯的算法及各式应用第 $1 1 \! \sim\! 1 4$ 章介绍自然语言处理、语音及各式应用。 第 $1 5$ 章介绍 AlphaGo的基础一一强化学习算法。 本书范例程序代码全部可以通过扫描二维码获取。

第 $1 1 \! \sim\! 1 4$ 章介绍自然语言处理、语音及各式应用。

 $1 5$ 章介绍AlphaGo 的基础一一强化学习算法。 第

本书范例程序代码全部可以通过扫描二维码获取。

成功大学统计系学士、清华大学工业工程研究所硕士。曾任职于BM、工研院电通所、软件开发公司、ERP 顾问公司、电信公司、财经数据库公司，目前担任Python、机器学习、深度学习、 AloT讲师。获2018年1厂邦帮忙铁人赛 $\mathbf{A} \mathbf{I}$ 组冠军、2021年厂邦帮忙铁人赛 $\mathbf{A} \mathbf{I}$ 组优选。

## 作者简介

## 陈昭明

## 近年出版：

开发者传授 $\mathbf{P y}$ Torch秘籍

在厂邦帮忙累计发布130多篇文章

## 洪锦魁

中国台湾计算机专家，门图书知名作者

## 近年出版：

机器学习数学基础一本通（Python版） oPython 数据科学零基础一本通
+Python入门很简单
+Python王者归来
算法零基础一本通（Python 版）

机器学习数学基础一本通（Pyhon版）

Python数据科学零基础一本通

$$
\bullet\mathrm{P y t h o n} \ > \Gamma\brack\Xi\ddag\Xi\nsubseteq
$$

$$
\bullet\mathrm{P y t h o n} \pm\pm1 \exists\exists\Re
$$

算法零基础一本通（Python 版） ## +R语言迈向大数据之路【加强版

![](figures/9-0-FIGURE.jpg)

图2.13、二次方曲线执行结果

![](figures/9-2-FIGURE.jpg)

图2.46 Student'st分布的概率密度函数

![](figures/9-4-FIGURE.jpg)

图 $6. 7$ 各式卷积执行结果

a）原图： (b）灰阶化

![](figures/10-0-FIGURE.jpg)

图 $6. 2 0$ 颜色数据增补

![](figures/10-2-FIGURE.jpg)

图6.29左上角的图像为原图，左下角的图像显示了辨识热 $\overline{{\chi}}$ ，即猴子的头和颈部都是辨识的主要关键区域

![](figures/11-0-FIGURE.jpg)

冬 $8. 2$ 目标检测类型

(图片来源：Detection and

![](figures/11-3-FIGURE.jpg)

图 $8. 2 5$ 区域推荐

![](figures/11-5-FIGURE.jpg)

图92，语义分割

![](figures/12-0-FIGURE.jpg)

 $9. 3$ 实例分割冬

![](figures/12-2-FIGURE.jpg)

图9.21显 $\overline{{\Pi}}$ 屏蔽
=

![](figures/12-4-FIGURE.jpg)

图 $1 0. 3 3$ ColorGAN

(图片来源：「Colorization Using ConvNet and GAN Ii

![](figures/12-7-FIGURE.jpg)

110.48 CycleGAN的功能展示冬

![](figures/13-1-FIGURE.jpg)

12.13 实际数据和预测数据图表冬

![](figures/13-3-FIGURE.jpg)

12.14绘制实际数据及预测数据图表冬

![](figures/13-5-FIGURE.jpg)

图12.15绘制实际数据与预测数据图表

![](figures/13-7-FIGURE.jpg)

12.16绘制实际数据和预测数据图表图

![](figures/14-1-FIGURE.jpg)

图 $1 4. 2 7$ 准确率绘图

![](figures/14-3-FIGURE.jpg)

图14.28、训练与验证准确率绘图

![](figures/14-5-FIGURE.jpg)

图14.33准确率绘图

笔者 ${\cal M}$ 事机器学习教育行业已有四年，其间也在「T邦帮忙」 撰写过上百篇文章

(https://thelo.ithome.com.tw/users/20001976/articles），从学员及读者的回馈中获得了许多宝贵意见，期望能将整个历程集结成册，同时，相关领域的发展也在飞速变化，过往的文章内容需要翻新，因此笔者借机再重整思路，思考如何能将算法的原理解释得更简易清晰，协助读者跨入AI的门坎，另外，也尽量避免流于空谈，增加应用范例，希望能使学生实现即学即用，不要有过多理论的探讨。

AI是一个将数据转化为知识的过程，算法就是过程中的生产设备，最后产出物是模型，再将模型植入各种硬件装置，如计算机、 手机、智能音箱、汽车、医疗诊断仪器等，这些装置就拥有了特殊专长的智能，再进一步整合各项技术就可以构建出智能制造、智能金融、智能交通、智慧医疗、智能城市、智能家庭等应用系统。AI 的应用领域如比广阔，个人精力和能力有限，唯有从基础扎根，再扩及有兴趣的领域，因此，笔者撰写这本书的初哀，就是希望读者在扎根的过程中，贡献一点微薄的力量。

(1）笔者身为统计人，希望能以统计/数学为出发点，介绍深度学习必备的数理基础，但又不希望内文有太多数学 $\big< \lambda$ 式的推导

## 前言

## 为何撰写本书

## 本书主要的特点让离开校园已久的在职者看到大量数学符号心生恐惧，因此，本书尝试以程序设计取代定理证明，缩短学习历程，增进学习乐趣。

(2）TensorFlow2.X 版有巨大的变动，默认模式改为Eager Execution，并以Keras为主力，整合Tensorflow 其他模块，形成完整的架构，本书期望对TensorFlow架构作完整性的介绍，并非只是介绍 Keras 而已

(3）算法介绍以理解为主，辅以大量图表说明，摒弃长篇大论。

(4）完整的范例程序及各种算法的延伸应用，以实用为要， 希望能触发读者灵感，能在项目或产品内应用

(5）介绍日益普及的算法与相关套件的使用，如YOLO（目标检测) GAN（生成对抗网络） DeepFake（深度伪造）、OCR (光学文字辨识）、人脸识别、BERT/Transtomer、ChatBot、强化学习、语音识别（ASR）等。

(1）深度学习的入门者：必须熟悉Python程序语言及机器学习的基本概念

(2）数据工程师：以应用系统开发为职业志向，希望能应用各种算法，进行实际操作。

## 目标对象

(3）信息工作者：希望能扩展深度学习知识领域

(4）从事其他领域的二作’希望能一窥深度学习奥秘者

(1）第1章介绍 $\mathbf{A} \mathbf{I}$ 的发展趋势，鉴古知今， $\exists\mid$ 导读者了解前两波 $\mathbf{A} \mathbf{I}$ 失败的原因，比较第三波发展的差异性

(2)第 $2$ 章介绍深度学习必备的统计/数学基础，读者不仅要理解相关知 $\grave{\imath} \P$ ，也要力求能撰写程序解题

(3第 $3$ 章介绍TensorFlow的基本功能，包括张量
(Tensor）运算、自动微分及神经网络模型的组成，并说明梯度下降法求解的过程。

(4)第 $4$ 章开始实作，依照机器学习十项流程，撰写完整的范例，包括\Web、桌面程序等

(5)第 $5$ 章介绍TensorFlow 进阶功能，包括各种工具，如 TensorBoard、TensorFlow Serving、Calbacks等

原本笔者计划整理过往文章集结成书，但由于相关技术发展太快， $\mathit{\Pi}$ 乎全部重新撰写编排，因比耗时较长，因个人能力有限，还是有许多问题成为遗珠之憾，仍待后续努力，编写过程中感谢栾大

## 阅读重点

(6）第 $6 \! \sim\! 1 0$ 章介绍图像/视频的算法及各式应用第

(第 $1 1 \! \sim\! 1 4$ 章介绍自然语言处理、语音及各式应用

(8)第 $1 5$ 章介绍AlphaGo的基础一一强化学习算法。

本书范例程序代码和参考文献全部可以通过扫描二维码获取。

## 致谢成、申美莹在编辑、校正、封面构想环节的尽心协助，也感谢清华大学出版社的大力支持，使本书得以顺利出版，最后要借此书，纪念一位挚爱的亲人。

书中内容如有疏漏、谬误之处或有其他建议，欢迎广大读者来信指教。

$$
\begin{array} {l} {\mathrm{B F B B H}} \\ {2 0 2 2-0 8} \\ \end{array}
$$

$$
\mathbf{B} \mathbf{F} \mathbf{H} \mathbf{H} \mathbf{H} \mathbf{H}
$$

$$
2 0 2 2-0 8
$$

![](figures/18-5-FIGURE.jpg)

![](figures/18-6-FIGURE.jpg)

教学课件.ppx

(2）人工智能、数据科学、数据挖掘、机器学习、深度学习到底有何关联？

 $( 6 )$ 先学哪一种深度学习框架比较好？是TensorFlow还是 PyTorch?

在本篇内容中，第 $1$ 章先对以上的问题做简单的介绍，第 $2$ 章则介绍深度学习 $\grave{\varPsi}$ 备的数学与统计基础知识。有别于学校教学的是，我们会偏重在程序的实践。

## 第一篇深度学习导论

在进入深度学习的殿堂前，学生通常会问以下的问题

1）人二智能已历经三波浪潮，这一波是否又将进入寒冬？

(3）机器学习开发流程与一般应用系统开发有何差异?

(4）深度学习的学习路径为何？建议从哪里开始?

(5）为什么要先学习数学与统计，才能学好深度学习？

## (7）如何准备开发环境? ## 第1章深度学习导论

## 第1章

## 深度学习导论

人工智能（Arifical Inteligence, A）并不是最近几年才兴起的，目前已经是它第三波热潮了，前两波热潮都经历了十余年，就迈入了寒冬，这一波热潮至今已超过十年（2010至今），是否又将迈入寒冬呢？如图1.1所示，我们就先来重点口顾一下人工智能的三波浪潮吧。

(1）1956年在达特茅斯（Darmouth）学院举办了AI 会议， 确立了第一波浪潮的开始。

$$
( \mathrm{P e r c e p t r o n} )
$$
即简易的神经网络，然而当时并无法解决复杂多
层的神经网络问题，直至1980年代才想出解决办法。

(3）1969年美国国防部高级研究规则局（DARPA）基于投资报酬率过低，决定缩减Al研究经费，AI的发展迈入了第一波寒冬。

## 1-1 人工智能的三波浪潮

![](figures/21-5-FIGURE.jpg)

图 $1. 1$ 人工智能的三波浪潮

(2）1957年Frank Rosenblatt创建了感知器

（4）1980年专家系统（Expert Systems）兴起，企图将各行各业专家的内隐知识外显为一条条的规则，从而建立起专家系统， 不过因不切实际，且需要使用大型且昂贵的计算机设备，才能够建构相关系统，适逢个人计算机（PC）的流行，相较之 $\mathrm{F}$ ，AI的发展势头就被掩盖下去 $\overline{{\j}}$ ，至此， $\mathbf{A} \mathbf{I}$ 的发展迈入了第二波寒冬。

(5）2012年多伦多大学Geofrey Hinton研发团队利用分布式计算环境及大量影像数据，结合过往的神经网络知识，开发 $\overline{{\jmath}}$ 
AlexNet神经网络，参加了lmageNet影像辨识大赛，该神经网络大放异彩，把错误率降低了十几个百分比，就此兴起了AI的第三波浪潮，至今方兴未艾。

第三波热潮至今也已超过十年，是否又将迈入寒冬呢？如图 1.2所示，观察这一波热潮，相较于过去前两波，具备了以下几个优势。

![](figures/22-3-FIGURE.jpg)

图 $1. 2$ 第三波 $\mathbf{A} \mathbf{I}$ 浪潮的触媒

## 1，发展方式由下往上

先架构基础功能，从影像、语音、文字辨识开始，再逐步往上构建各式的应用，如自动驾驶（Self Driving) 聊天机器 $\mathcal{A}$ 
(ChatBot) 机器人（Robot) 智慧医疗、智慧城市等，这种由下往上的发展方式比较扎实。

(1）摩尔定律的发展速度： $\mathrm{I C}$ 上可容纳的晶体管数目，约每隔18个月至两年便会增加一倍，简单来说，就是CPU每隔两年便会提速 $- 1 \frac{\pi} {\pi}$ ，过去50年均循比轨迹发展，此定律在未来一年也可能会继续适用，之后量子计算机（Quantum Computing）等新科技可能会继续接棒，日前计算机要计算几百年的工作，量子计算机预计只要 $3 0$ 分钟即可完成，如果成真，那时又将是另一番光景。

(2）云端数据中心的建立：大型IT $\big< \g$ 司在世界各地兴建大型的数据中 $\imath\grave{\mathrm{L}}$ ，采取「用多少付多少」（Pay as you go）的模式。日于模型训练时，通常需要大量运算，因此采用云端方案，一般企业就不需在前期购买昂贵的设备，仅需支付必要的运算费用，也不需冗长的采购流程，只要几分钟就能开通（Provisioning）所需设备，省钱省时。

(3）GPU/NPU的开发：深度学习主要是以矩阵运算为主， GPU这方面比 CPU 快很多倍，专门生产GPU的 NVIDIA $\big<$ 司因而大放异彩，市值超越了Intel $\angle\ >$ 司。当然其他硬件及系统厂商 $\overline{{\Upsilon}}$ 会错失如此良机，因此各式的NPU（Neural-network Processing Unit）或XPU纷纷出笼，积极抢食这块「蛋糕」，各项产品使得运算速度越来越快，模型训练的时间大幅缩短，通常模型调校需要反复训练，如果能在短时间得到答案，对数据科学家而言，也将是一

## 2.。硬件的快速发展大福音。另外，连接现场装置的计算机（Raspberry pi、Jetson Nano、Auduino等），体积越来越 $\imath\rfloor$ ，运算能力越来越强，对于边缘运算也有很大的帮助，如监视器、无人机等。

过去由于计算能力有限，许多无法在短时间训练完成的算法一一解封，尤其是神经网络，现在已经可以建构上百层的模型，运算的参数也可以高达上兆个，都能在短时间内调校出最佳模型，因此，模型设计就可以更复杂，算法逻辑也能够更完备。

人工智能必须依赖大量数据，让计算机学习，从中挖掘知识， 近年来因特网（Internet） $\mathcal{R}$ 手机（Mobile）盛行，企业可以透过社群媒体搜集到大量数据，再加上物联网（IoT）也可以借由传感器产生源源不断的数据，作为深度学习的「养分」（训练数据），而这些大型的网络 $\big< \g$ 司资金充足，可以雇佣大量的人力，进行数据标注，确保数据的质量，使得训练出来的模型越趋精准。

因此，根据以上的趋势发展，笔者猜测第三波的热潮在短期内应该不会迈入寒冬。

## 3.算法推陈出新

## 4.大量数据的搜集及标注（Labeling Al的发展划分为三个阶段，每一阶段的重点分别为人工智能、 机器学习（Machine Learning）、深度学习（Deep Learning)
每一阶段都在缩小范围，聚焦在特定的算法，机器学习是人工智能的部分领域，而深度学习又属于机器学习的部分算法，如图1.3所 $\overline{{\Pi}}$ 。

而一般教育机构规划 $\mathbf{A} \mathbf{I}$ 的学习地图即依照这个轨迹，逐步深 $\lambda$ 各项技术，通常分为四个阶段，如图1.4所示。

## 1-2 AI的学习地图

![](figures/25-3-FIGURE.jpg)

图1.3 $\mathrm{A I} \ \Xi$ 个阶段的重点

![](figures/25-5-FIGURE.jpg)

图1.4 AI学习地图

(1）数据科学（Data Science）入门：内容包括Python/R程序语言、数据分析（Data Analysis) 大数据平台（Hadoop、 Spark）等。

(2）机器学习：包含一些典型的算法，如回归、Logistio 归、支持向量机（SVM）、K-means 聚类算法等，这些算法虽然简单，但却非常实用，比较容易在一般企业内普遍性地导入。通常机器学习的大致分类如图1.5所示。

最新的发展还有半监督学习（Semi-supervised Learning) 自我学习（Self Learning） 联合学习（Federated Learning) 等， $\overline{{\Pi}} \mathrm{\Xi}$ 一而足，我们千万不要被分类限制了想象。

另外，数据挖掘（Data Mining）与机器学习的算法大量重叠， 其间的差异在于，数据挖掘是着重挖掘数据的隐藏样态
$$
( \mathrm{P a t t e r n} )
$$
而机器学习则着重于预测。

(3）深度学习：深度学习属于机器学习中的一环，所谓深度 (Deep）是指多层式架构的模型，如各种神经网络（Neural
Network）模型、强化学习（Reinforcement Learning, RL）算法等，以多层的神经层或try-and-error 的方式实现以优化
(Optimization）或反复的方式求解。

![](figures/26-5-FIGURE.jpg)

 $1. 5$ 机器学习分类图

(4）实务及专题探讨（Capstone Project 将各种算法应用于各类领域、行业，强调专题探讨及产业应用实践。

机器学习的应用其实早已在不知不觉中融入我们的生活中 $\overline{{\jmath}}$ , 举例如下

社群软件大量运用 $\mathbf{A} \mathbf{I}$ ，预测用户的行为模式，过滤垃圾 (1)
信件

(3）还有各式的30产品，包括手机、智能音箱，以人脸识别取代登录、语音识别代替键盘输 $\lambda$ .

(5）制造机器人（Robot）提供智能制造、 、 $J \mathsf{L}$ 童陪
老人照护
伴，凡此种种，不及备载。

## 1-3 机器学习应用领域

(2）电商运用 $\mathrm{A I}$ ，依据每位消费者的喜好推荐合适的商品

(4）聊天机器人取代客服人 $\P$ ，提供营销服务

 $\mathbf{A} \mathbf{I}$ 相关的应用领域如图1.6所示

![](figures/28-8-FIGURE.jpg)

图1.6 AI应月领域

目前相对热门的研发领域如下。

(1）各种疾病的诊断（Medical Diagnosics）及新药的开发 (2）聊天机器人：包括营销、销售及售后服务的支持。

(3）目标检测（Object Detection）人脸辨识（Facial Recognition)

(4）自动驾驶（Self Driving）。

 $( 5 )$ 制造机器 $\mathcal{A}$ .
O

一般来说，机器学习开发流程（Machine Learning
Worklow），有许多种建议的模型，如数据挖掘流程，包括 CRISP-DM (Cross-industr $\mathbf{Y}$ Standard Process for Data
Mining) Google Cloud 建议的流程等，个人偏好的流程如图 1.7所示。

机器学习开发流程大概分为十个步骤，这里不含较高层次的企 $\gnu$ 需求了解（Business Understanding 只包括实际开发的步骤。

(2）数据清理（Data Cleaning 数据探索与分析
(Exploratory Data Analysis, EDA) EDA通常是指以描述统计量及统计图观察数据的分布，了解数据的特性、极端值（Outlier) 变量之间的关联性。

## 1-4 机器学习开发流程

![](figures/30-4-FIGURE.jpg)

图 $1. 7$ 机器学习开发流程

）搜集数据，汇整为数据集（Dataset O (1)

(3）特征工程（Feature Engineering) 原始搜集的数据未 $\grave{\varPsi}$ 是影响预测目标的关键因素，有时候需要进行数据转换，以找到关键的影响变量。

(4）数据切割（Data Split）：切割为训练数据（Training Data）及测试数据（Test Data），一份数据提供模型训练之用，另一份数据则用于衡量模型效果，如准确度。切割的主要目的是确保测试数据不会参与训练，以维持其公正性，即Out-of-Sample
Test.

(5）选择算法（Learning Algorihms 依据问题的类型选择适合的算法。

(6）模型训练（Model Training）：以算法及训练数据进行训练，产出模型

(7）模型计 $\mathrm{\^{-\sharp} ~ ( S c o r e ~ M o d e l )}$ ：计算准确度等效果指标，评估模型的准确性。

(8）模型评估（Evaluate Model) 比较多个参数组合、多个算法的准确度，找到最佳参数与算法

(9）部署 $( \mathrm{D e p l o y} )$ 复制最佳模型至正式环境（Production Environment) 制作使用界面或提供 $\mathrm{A P I}$ ，通常以网页服务
(Web Services）作为预测的 $\mathrm{A P I}_{\circ}$ 

(10）预测 $( \mathrm{P r e d i c t} )$ 客户端传入新数据或文件，系统以模型进行预测，传回预测结果。

机器学习开发流程与一般应用系统开发有何差异？最大的差别如图1.8所示

(1）一般应用系统利用输入数据与转换逻辑产生输出，如撰写报表，根据转换规则将输入字段转换为输出字段；但机器学习会先产生模型，再根据模型进行预测，重用性（Reuse）高。

(2）机器学习除了输入数据外，还会搜集大量历史数据或在 nternet中抓取一堆数据，作为塑模的「饲料」

）新产生的数据可回馈入模型中，重新训练，自我学习，
(3)
使模型更「聪明

![](figures/32-4-FIGURE.jpg)

1.8、机器学习与一般应用系统开发流程的差异

Python 是目前机器学习主流的程序语言，可以直接在本机安装开发环境，亦能使用云端环境，首先介绍本机安装的程序，建议依照以下顺序安装。

(1）安装Anaconda：建议安装此软件，它内含Python及上百个常用工具包。用户可先至

https://www.anaconda.com/products/individual下载安装文件，在 Windows操作系统安装时，建议执行到图1.9所示画面时，将两者都勾选，就可将安装路径加 $\lambda$ 至环境变量Path 内，这样就能保证在任何目录下均可执行Python程序。Mac/Linux系统则须自行修改登录文件（Profile），增加Anaconda 安装路径。

(2）安装TensorFlow 最新版：Windows操作系统安装时， 运行cmd命令，开启DOS 窗口，Mac/Linux则须开启终端机，输 $\lambda$ 

## 1-5 开发环境安装

## 1，开发环境安装

![](figures/33-6-FIGURE.jpg)

图1.9 Anaconda 安装注意事项

注意：此指令同时支持CPU及GPU，但要支持GPU 须另外安装驱动程序及SDK。

lmportError: DLL load failed: The specified module coulo not be found.则可能是 Windows操作系统较旧，须安装 $\mathsf{M S V C}$ 2019 runtime

②如果还是出现DLL canmnot be loaded.，也可使用Anaconda 提供的指令来安装。

(4）若要支持GPU，则还需安装 CUDA Toolkit、cuDNN SD $\mathbf{K}$ ，只能安装TensorFlow 支持的版本，请参考TensorFlow官网说明'，如图1.10所示

$$
\mathrm{p i p ~ i n s t a l l ~ T e n s o r F l o w}
$$

(3）测试：安装完成，在DOS窗口或终端机内，输入 python，进入交互式环境，再输入以下指令进行测试：

import tensorflow
$$
> \mathrm{e x i t ( )}
$$

$$
> \mathrm{i m p o r t \, t e n s o r f l o w}
$$

$$
> \mathrm{e x i t ( )}
$$

 $\copyleft$ 若出现下列错误：

pywrap tensorfow internal import

conda install tensorflow

③注意：若安装Python $\mathrm{V 3. 7}$ 或更旧版，则只能安装 TensorFlow 2.0.0 版或以下版本

注意：目前只支持NVIDIA独立显卡，若是较旧型的显卡，则必须查阅驱动程序搭配的版本信息请参考NVIDIA官网说明的，如图 1.11所示。

(5）提醒各位读者，低端的显卡，若不能安装TensorFlow支持的版本，就不用安装 CUDA Toolkit、cuDNN SDK $\overline{{\jmath}}$ ，因为显卡内存过 $\imath\rfloor\omicron$ ，执行TensorFlow 时常常会发生内存不足（OOM）的错误，徒增困扰

(6）CUDA Toolkit、cuDNN SDK 安装成功后，将安装路径下的 bin、libnvwp加入至环境变量Path 中，如图1.12所示。

![](figures/35-3-FIGURE.jpg)

图1.10 TensorFlow支持的 CUDA Toolkit版本

Table 2. CUDA Toolkit and Compatible Driver Versions

| CUDA Toolkit | Linux x86 64 Driver Version | Windows x86 64 Driver Version |
| --- | --- | --- |
| CUDA 11.1.1 Update 1 | >=455.32 | >=456.81 |
| CUDA 11.1 GA | >=455.23 | >=456.38 |
| CUDA 11.0.3 Update 1 | >= 450.51.06 | >= 451.82 |
| CUDA 11.0.2 GA | >= 450.51.05 | >= 451.48 |
| CUDA 11.0.1 RC | >= 450.36.06 | >= 451.22 |
| CUDA 10.2.89 | >= 440.33 | >= 441.22 |
| CUDA 10.1 (10.1.105 general release, and updates) | >= 418.39 | >= 418.96 |
| CUDA 10.0.130 | >= 410.48 | >= 411.31 |
| CUDA 9.2 (9.2.148 Update 1) | >= 396.37 | >= 398.26 |
| CUDA 9.2 (9.2.88) | >= 396.26 | >= 397.44 |
| CUDA 9.1 (9.1.85) | >= 390.46 | >= 391.29 |
| CUDA 9.0 (9.0.76) | >= 384.81 | >= 385.54 |
| CUDA 8.0 (8.0.61 GA2) | >= 375.26 | >= 376.51 |
| CUDA 8.0 (8.0.44) | >= 367.48 | >= 369.30 |
| CUDA 7.5 (7.5.16) | >= 352.31 | >= 353.66 |
| CUDA 7.0(7.0.28) | >= 346.46 | >= 347.62 |

图1.1 CUDA Tookit版本与驱动程序的搭配

 $( 7 )$ 安装若有其他问题，可参考笔者的博客文章「DayO1 轻 $\big<$ 掌握Keras」

我们再来谈谈云端环境的开通，Google、AWS、Azure都提供机器学习的开发环境，这里介绍免费的 Google 云端环境
Colaboratory，须具备Gmail账号才能使用，其具以下特点。

(2）免费的 GPU：NVIDIA Tesla K80 GPU 显卡，含12GB内存，设置合理。

(3）它在使用时实时开通Docker Container，限连续使用12 小时，超时的话虚拟环境会被口收，所有程序、数据一律会被删除。

![](figures/36-4-FIGURE.jpg)

图112将bin、lbnwp加入Path中

## 2.云端环境的开通

1）常用的框架均已预安装，包括TensorFlow

开通程序如下。 (1）使用Google Chrome 浏览器，进 $\lambda$ 云端硬盘（Google Drive）接口。

(2）建立一个目录，如「0」，并切换至该目录，如图1.13所示。

(3）在屏幕中间右击，选择「更多」 关联更多 $\overrightarrow{\Psi}$ 用」
选项，如图1.14所示。

(4）在搜索栏输入『Colaboratory」，找到后单击该App，单击「Connect」按钮即可开通，如图1.15和图1.16所示。

![](figures/37-4-FIGURE.jpg)

图 $1. 1 3$ 新建目录

![](figures/37-6-FIGURE.jpg)

图1.14连结 $\overrightarrow{\omega}$ 用程序

(5）开通后，即可开始使用，可新增一个『Colaboratory」的文件，如图117所示。

![](figures/38-1-FIGURE.jpg)

图1.15搜索「Calaboratory

![](figures/38-3-FIGURE.jpg)

图1.16开通「Colaboratory

![](figures/38-5-FIGURE.jpg)

1.17新增『Golaboratory」文件图

(6）Google Colaborato $\mathbf{y}$ 会自动开启虚拟环境，建立一个空白的 Jupyter Notebook文件，后缀为ioynb，几乎所有的云端环境 $\mathcal{R}$ 大数据平台Databricks 都l以 Notebook为主要使用接口，如图 1.18 所示。

(7）或者直接以鼠标双击Notebook文件，也可以自动开启虚拟环境，进行编辑与执行。本机的Notebook文件也可以上传至云端硬盘，选择使用完全不用转换，非常方便。

(8）若要支持GPU可设定运行环境使用GPU或TPU，TPU 为Google 发明的 NPU，如图119所示。

![](figures/39-3-FIGURE.jpg)

图1.18：自动建立空白Jupyter Notebook文件

![](figures/39-5-FIGURE.jpg)

图1.19 设定运行环境使用GPU或TPU
(

(9）「Colaboratory」相关操作，可参考官网说明“。

注意：本书所附的范例程序，一律为Notebok文件，因为 Notebook 可以使用 Markdown 语法撰写美观的说明,包括数学 $\angle\ >$ 式，另外，程序也可以单独执行，便于讲解，相关的用法可以参考 lupyter Notebook: An introduction ln

## 第2章神经网络原理

## 第2章

## 神经网络原理

现在我们每天 $\prod$ 乎都会通过各种渠道看到几则有关人工智能 (Al）的新闻，介绍AI的各式研发成果，很多人基于好奇也许会想一窥究竞，了解背后运用的技术与原理，就会发现一堆数学符号 $\mathcal{R}$ 统计公式，也许会产生疑问：要从事 $\mathbf{A} \mathbf{I}$ 系统开发，非要搞定数学、统计不可吗？答案是肯定的，我们都知道机器学习是从数据中学习到知识（Knowledge Discovery from Data, KDD），而算法就是从数据中萃取出知识的「榨汁机」，它必须以数学及统计为理论基础，才能证明其解法具有公信力与精准度，然而数学/统计理论都有局限，只有在假设成立的情况下，算法才是有效的，因此，如果不了解算法中的各个假设，随意套用 $\big< \g$ 式，就好像无视交通规则， 在马路上任意飚车一样危险。

因此，以深度学习而言，我们至少需要熟悉下列学科：①线性代数（Linear Algebra 2微积分（Galculus ③概率论与数

## 2-1必备的数学与统计知识

![](figures/42-3-FIGURE.jpg)

(图片来源：Decision makers need more mathl)

以神经网络优化求解的过程为例，上述四门学科就全部用上了，如图2.2所示

(2）反向传导：透过偏微分计算梯度，同时，利用线性规划优化技巧寻找最佳解。

理统计（Probability and Statistios ④线性规划（Linear Programming） 如图2.1所示。

![](figures/43-3-FIGURE.jpg)

图2.1 必备的数学与统计知识

![](figures/43-5-FIGURE.jpg)

图 $2. 2$ 神经网络权重求解过程

(1）正向传导：借由线性代数计算误差及损失函数。

(3）统计串联整个环节，如数据的探索与分析、损失函数定 $\mathcal{X}$ 、效果衡量指标等，都是基于统计的理论架构而成的。

四项学科相互为用，贯穿整个求解过程，因此，要通晓深度学习的运作原理，并且正确选用各种算法，甚至进而能够修改或创新算法，都必须对其背后的数学和统计有一定基础的认识，以免误用或滥用。

四门学科在大学至少都是两学期的课程，对已经离开大学殿堂很久的工程师而言，在上班之余，还要重修上述课程，相信大部分的人都会萌生退意了！那么我们是否有速成的快捷方式呢?

笔者在这里借用一个概念Statistical Programming，原意是 「以程序解决统计问题」，换个角度想，我们是不是也能以程序设计的方式学统计，以缩短学习历程呢?

通常我们按部就班地学习数学及统计，都是从「假设 →「定 $\mathrm{X}_{l}$ 定理」→「证明」→「应用」，一步一步学习。

由于「证明」都会有一堆的数学符号及公式推导，经常会让人头晕脑涨，降低学习的效率，因此，笔者大胆建议，工程师将心力着重在假设、定 $\mathcal{Y}$ 、定理的理解与应用，并利用程序进行大量个案 (4）深度学习的推论以概率为基础，预测目标值

(1）「假设」是「定义/定理」成立的前提。 (2）「证明」是「定理」的验证。
(3）「应用」是「定义/定理」的实践。

(1）「假设」是「定义/定理」成立的前提

(2）「证明」是「定理」的验证

(3）「应用」是「定义/定理」的实践的验证，虽然忽略了证明的做法，会让学习无法彻底地融会贯通， 但是对已进入职场的工程师会是一种较为可行的快捷方式

接下来我们就以上述做法，对四项学科进行重点介绍，除了说明深度学习需要理解的知识外，更强调如何以撰写程序实现相关理论及解题方法。

张量（Tensor）是描述向量空间（Vector Space）中物体的特征，包括零维的纯量、一维的向量（Veotor）、二维的矩阵
(Matrix）或更多维度的张量，线性代数则是说明张量如何进行各种运算，它被广泛应用于各种数值分析的领域。以下就以实例说明张量的概念与运算。

张量（Tensor）是描述向量空间（Vector Space）中物体的特征, 量、一维的向量二维的矩阵
包括零维的纯量、 (Vector

## 2-2 线性代数下面使用程序计算向量的长度与方向，请参阅02 01线性代数向量.pynb.

## 2-2-1 向量

向量是一维的张量，它与线段的差别是除了长度
）以外， （Direction），其数学表示法为 (Magnitude) 还有方向

Magnitude）以外，还有方向（Direction ，其数学表示法大

$$
\upsilon=\left[ \begin{matrix} {2} \\ {1} \\ \end{matrix} \right]
$$

图形表示如图2.3所示。

![](figures/47-6-FIGURE.jpg)

图 $2. 3$ 向量长度与方向

## 1.长度

计算公式为欧几里得距离（Eucolidean Distance ，即

$$
\left\| \boldsymbol{v} \right\|=\sqrt{\nu_{1}^{2}+\nu_{2}^{2}}=\sqrt{5}
$$

$$
\star\equiv\lfloor\pm\lfloor\exists\pm\rfloor\rceil\top\colon
$$

## 也可以使用np.linalg,norm()计算向量长度，程序代码如下：

1#使用 $n p. l i n a l g. n o r m ( )$ 计算向量长度 $( M a g n i t u d e )$ 2 import numpy as np
3
4 magnitude = np.linalg.norm(v)
5 print(magnitude)

## 2。方向

使用tan' $( )$ 函数计算

$$
\operatorname{t a n} ( \theta)=\frac{1} {2}
$$

$$
F^{3} I \pi> I J
$$

$$
\theta=\operatorname{t a n}^{-1} \biggl( \frac{1} {2} \biggr) \approx2 6. 5 7^{\circ}
$$

## 程序代码如下：

1 import math
2 import numpy as np
3
4 #向量(Vector）
5 v = np.array([2,1])
6
7 vTan = v[1] / v[0]
8 print ('tan(O) = 1/2')
9
10 theta = math.atan(vTan)
11 print("弧度(radian) =', round(theta,4))
12 print('角度(degree) -', round(theta*180/math.pi, 2)) 13
14#也可以使用 $m a t h. \, d e g r e e s ( \c)$ 转换角度
15 print('角度(degree) -', round(math.degrees(theta), 2))

## 3。向量四则运算规则

(2）加减乘除另一个向量：两个向量相同位置的元素作加减乘除，所以两个向量的元素个数必须相等

(1）加减乘除一个常数：常数直接对每个元素作加减乘除

## 4。向量加减法

向量加减一个常数，长度、方向均改变

$$
( 1 ) \textit{F} \equiv\textit{F} \pm\textit{F} \pm\textit{F} \pm\textit{F} \mp\textit{F} \mp\textit{F}
$$

![](figures/49-5-FIGURE.jpg)

(2）执行结果：如图2.4所示

![](figures/49-7-FIGURE.jpg)

图2.4、向量加减一个常数，长度、方向均改变

## 5.向量乘除法向量乘除一个常数，长度改变、方向不改变

$$
( 1 ) \textit{F F F} \textit{F} \pm[ \exists\ni\ss] \Gamma:
$$

![](figures/50-2-FIGURE.jpg)

(2）执行结果：如图2.5所示

![](figures/50-4-FIGURE.jpg)

图2.5 向量乘除一个常数，长度改变、方向不改变

## 6。向量加减乘除另一个向量

两个向量的相同位置的元素作加减乘除。

$$
( 1 ) \textit{F} \equiv\textit{F} \pm\textit{( l )} \ni\textit{( l )} \pm\textit{F} :
$$

![](figures/51-0-FIGURE.jpg)

(2）执行结果：如图2.6所示

![](figures/51-2-FIGURE.jpg)

图 $2. 6$ 向量加另一个向量

## 7。「内积」或称「点积」（Dot Product)

$$
\angle{\csc\ddag\Im}
$$

$$
\boldsymbol{v} \cdot\boldsymbol{s}=( \upsilon_{1} \cdot s_{1} )+( \upsilon_{2} \cdot s_{2} ) \cdots+( \upsilon_{n} \cdot s_{n} )
$$

numpy是以@作为内积的运算符号，而非*。程序代码如下：

| #载人库 |
| --- |
| 2 import numpy as np |
| 3 |
| #向量(Vector) |
| L 3 S =np.arrav([-3.21) |
| ← TPL3/ 7  |
| 3 #·内积或称点积乘法(Dot Product) |
| 0 d=v@S |
| 0 |
| print (d) |

## 8。计算两个向量的夹角

$$
\angle{\mod\ddag\Im}
$$

$$
\upsilon\cdot\boldsymbol{s}=\lVert\boldsymbol{v} \rVert\rVert\boldsymbol{s} \rVert\cos\theta
$$

$$
\mp\pm\mathrm{I m}, \enskip\enskip\bigg\{\pm
$$

$$
\operatorname{c o s} \theta=\frac{\upsilon\cdot\mathbf{s}} {\left\| \upsilon\left\| \right\| \mathbf{s} \right\|}
$$

再利用cos'(计算夹角 $\theta_{\circ}$ 程序代码如下

| 1 | 载人库 |
| 2 | mport math |
| 3 | mport numpy as np |
| 4 |  |
| 5 | 向量(Vector) |
| 6 | = np.array([2,1]) |
| 7 | = np.array([-3,2]) |
| 8 |  |
| 9 | 计算长度(Magnitudes) |
| 10 | Mag = np.linalg.norm(v) |
| 11 | Mag = np.linalg.norm(s) |
| 12 |  |
| 13 | 计算cosine(o) |
| 14 | os=(v@s)/ (vMag * sMag) |
| 15 |  |
| 16 | 计拿8 |
| 17 | heta - math.degrees(math.acos(cos)) |
| 18 |  |
| 19 | rint(theta) |

矩阵是二维的张量，拥有行（Row）与列（Column) 可用于表达一个平面N个点（N×2） 或一个3D空间N个点（N×3) 例如：

矩阵加法/减法与向量相似，相同位置的元素作运算即可，但乘法运算通常是指内积，使用@

## 2-2-2 矩阵

$$
A \!=\! \left[ \begin{matrix} {1} & {\, 2} & {\, 3} \\ {4} & {\, 5} & {\, 6} \\ \end{matrix} \right]
$$

> 02线性代数_矩阵.ipynb 以下程序请参考02

## 1。两个矩阵相加

$$
\left[ \begin{matrix} {1} & {2} & {3} \\ {4} & {5} & {6} \\ \end{matrix} \right]+\left[ \begin{matrix} {6} & {5} & {4} \\ {3} & {2} & {1} \\ \end{matrix} \right]=\left[ \begin{matrix} {7} & {7} & {7} \\ {7} & {7} & {7} \\ \end{matrix} \right]
$$

$$
\pi_{\pm}^{\Pi} \equiv1 \pm h \pm\pm h \mp F :
$$

| #载人库 |
| 2 import numpy as np |
| 3 |
| #矩阵 |
| A = np.array([[1,2,3]. |
|  5 [4,5,611 |
| L B= np.arrav([[6,5,41. |
| 3 [3,2,111) |
| 5 1  |
| #加法 |
| print(A+B) |

## 2.两个矩阵相乘

$$
\left[ \begin{matrix} {1} & {2} & {3} \\ {4} & {5} & {6} \\ \end{matrix} \right] \mathbf{\cdot} \left[ \begin{matrix} {9} & {8} \\ {7} & {6} \\ {5} & {4} \\ \end{matrix} \right]=?
$$

解题：左边矩阵的第二维须等于右边矩阵的第一维，即（m
$$
\bf k ) ~ \times~ ( \bf k, \bf n ) ~=~ ( \bf m, \bf n ) ~ ~,
$$
则有

其中左上角的计算过程为（1,2,3) $\times~ ( 9, 7, 5 ) ~=$ 
$$
( 1 \times9 ) ~+~ ( 2 \times7 ) ~+~ ( 3 \times5 ) ~=3 8,
$$
右上角的计算过程为 $( 1, 2, 3 )$  $\mathbf{x}$ 
$$
( 8, 6, 4 ) ~=~ ( 1 \times8 ) ~+~ ( 2 \times6 ) ~+~ ( 3 )
$$
Bx4）-32，以此类推，如图2.7所
示。

$$
\left[ \begin{matrix} {1} & {2} & {3} \\ {4} & {5} & {6} \\ \end{matrix} \right] \bullet\left[ \begin{matrix} {9} & {8} \\ {7} & {6} \\ {5} & {4} \\ \end{matrix} \right]=\left[ \begin{matrix} {3 8} & {3 2} \\ {1 0 1} & {8 6} \\ \end{matrix} \right]
$$

其中左上角的计算过程为 $( 1, 2, 3 ) ~ \times~ ( 9, 7, 5 ) ~=~$ 

$$
\begin{array} {c} {{-\left[ \begin{matrix} {1} & {3} & {3} \\ {4} & {5} & {6} \\ \end{matrix} \right] \cdot\left[ \begin{matrix} {\frac{4} {4}} & {6} \\ {1} & {4} \\ \end{matrix} \right] \;=\; \left[ \begin{matrix} {\left( \begin{matrix} {3 8} \\ {1 0 1} & {8 6} \\ {1 0 1} & {8 6} \\ \end{matrix} \right]} \\ {-\left[ \begin{matrix} {1} & {2} \\ {4} & {5} \\ \end{matrix} \right] \cdot\left[ \begin{matrix} {9} & {\frac{4} {4}} \\ {3} & {\frac{4} {4}} \\ \end{matrix} \right] \;=\; \left[ \begin{matrix} {3 8} & {\left( \widehat{3 2} \right)} \\ {1 0 1} & {8 6} \\ {1} & {3} \\ {1} & {4} \\ \end{matrix} \right] \;=\; \left[ \begin{matrix} {3 8} & {3 2} \\ {1 0 1} & {8 6} \\ {1} & {3} \\ {1} & {2} & {3 6} \\ \end{matrix} \right]} \\ {-\left[ \begin{matrix} {1} & {2} & {3} \\ {4} & {2} & {3 6} \\ {4} & {3} & {4} \\ \end{matrix} \right] \;=\; [ \begin{matrix} {3 8} & {3 2} \\ {3 8} & {3 2} \\ {3} & {3 6} \\ \end{matrix} \right]} \\ \end{array}
$$

图2.7矩阵相乘

## 程序代码如下：

| 1 | #载人库 |
| --- | --- |
| 2 | import numpy as np |
| 3 |  |
| 4 | #矩年 |
| 5 | A = np.array([[1,2,3], |
| 6 | [4,5,6]]) |
| 7 | B = np.array([[9,8], |
| 4 8 | SLLJJ 17.61. |
|  L 9 [5.41. |
|  上 10 1)  |
|  一 11  |
| 12 | #乘法 |
| 13 | print(A@B) |

## 3。矩阵（A、B）相乘矩阵在运算时，除 $7-$ 般的加减乘除外，还有一些特殊的矩阵，包括转置矩阵（Transpose) 反矩阵 $\mathrm{( l n v e r s e )}$ 对角矩阵 (Diagonal Matrix 单位矩阵（ldentity Matrix）等。

Ax5是否等于BxA？
程序代码如下

| 1 | #蒸法：AxB!=BxA |
| 2 |  |
| 3 | A= np.array([[1,2], |
| 4 | [4,5]]) |
| 5 | B = np.array([[9,8], |
| 6 | [7,6, 9  |
| 7 | ]) |
|  二 8  |
| 9 | print(A@B) |
| 10 | print() |
| 11 | print(B@A) |
| 12 | print() |
| 13 | print('AxB != B xA') |

执行结果：AxB不等于BxA

| [[23 20] |
| [71 | ]] |
| [[41 | ] |
| [31 44]] |
| AxB | EBxA |

## 4.特殊矩阵

转置矩阵：列与行互换。例如： (1)

$$
\left[ \begin{matrix} {1} & {2} & {3} \\ {4} & {5} & {6} \\ \end{matrix} \right]^{\mathrm{T}}=\left[ \begin{matrix} {1} & {4} \\ {2} & {5} \\ {3} & {6} \\ \end{matrix} \right]
$$

 $( \boldsymbol{A}^{\mathrm{T}} ) ~ ~^{\mathrm{T}}=\boldsymbol{A}$ ：进行两次转置，会恢复成原来的矩阵

对上述矩阵作转置。程序代码如下：

（2）反矩阵（A'）：A必须为方阵，即列数与行数须相等， 且必须是非奇异方阵（Non-singular) 即每一列或行之间不可以相异于其他列或行。程序代码如下：

（3）单位矩阵：若 $\boldsymbol{A}$ 为非奇异（Non-singular）矩阵，则A $\boldsymbol{A}^{-1}=$ 单位矩阵 $( \boldsymbol{I} )$ 所谓的非奇异矩阵是指任一行不能为其他行
o
的倍数或多行的组合，包括各种四则运算，矩阵的列也须符合相同的规则。

| 1 n | mport numpy as np | 2 |  |
| --- | --- | --- | --- |
| 3 | = np.array([[1,2,3], |
| 4 | [4,5,6]]) |
| 5 |  |
| 6 | 转置矩阵 |
| 7 | orint(A.T) |

也可以使用月np.iranspose（A) O

| 1 | p |
| --- | --- |
| 2 |  |
| 3 | 2,3], |
| 4 | 5,6], |
| 5 | 8,9], |
| 6 |  |
| 7 | nv(A)) |

## 执行结果如下：

[[ 3.15251974e+15 -6.30503948e+15 3.15251974e+15] [-6.30503948e+15 1.26100790e+16 -6.30503948e+15] [ 3.15251974e+15 -6.30503948e+15 3.15251974e+15]]

试对下列矩阵验算A@ $\boldsymbol{A}^{-1}$ 是否等于单位矩阵 $( I )$ 。

$$
A=\left[ \begin{matrix} {9} & {\; 8} \\ {7} & {\; 6} \\ \end{matrix} \right]
$$

$$
\pi\equiv\pi\pm\pi\pm\pi\pm\Gamma:
$$

| 1 | # A@A反矩阵一单位矩阵(I) |
| 2 | A= np.array([[9,8], |
| 3 | [7,6], |
| 4 | ]) |
| 5 |  |
| 6 | print(np.around(A @ np.linalg.inv(A))) |

## 执行结果如下：

$$
\lceil\begin{array} {c c} {[ \, 1 \,, \, \, 0 \,, \, ]} \\ {[ \, 0 \,, \, \, 1 \,, \, ] \, ]} \\ \end{array}
$$

结果为单位矩阵，表示A为非奇异矩阵

试对下列矩阵验算A@ $\boldsymbol{A}^{-1}$ 是否等于单位矩阵 $\left( I \right)$ 。

$$
A=\left[ \begin{matrix} {1} & {2} & {3} \\ {4} & {5} & {6} \\ {7} & {8} & {9} \\ \end{matrix} \right]
$$

## 程序代码如下：

| 1 | #A@A反矩阵！=单位矩阵(I) |
| 2 | #A为Singular矩阵 |
| 3 | # 第二行=第一行+1 |
| 4 | # 第三行=第一行+2 |
| 5 | A = np.array([[1,2,31, |
| 6 | 2LL2 14.5.61. |
| 7 | L 17.8.91. |
| 8 | 上 1)  |
| 9 | 一 |
| 10 | print(np.around(A @ np.linalg.inv(A))) |

## 执行结果如下：

$$
\begin{array} {r r r} {\left[ \, \left[ \, \, \emptyset\,. \, \, \, \, \, \, \, 1 \,, \, \, \,-\, 0 \,. \, \right]} \\ {\, \left[ \, \, \, \emptyset\,. \, \, \, \, \, \, \, \, 2 \,, \, \, \, \, \,-\, 1 \,. \, \, \right]} \\ {\, \left[ \, \, \, \emptyset\,. \, \, \, \, \, \, \, \, 3 \,, \, \, \, \, \, \, 2 \, \, \right] \, \right]} \end{array}
$$

A为奇异（Singular）矩阵，因为

第 $\sqsupset\Sigma\Sigma=$ 第一列+1

第 $\Xi F \i\rfloor=$ 第一列+2 故 $\boldsymbol{A}$ @ $\boldsymbol{A}^{-1}$ 不等于单位矩阵将第一个方程式两边乘 $\mathrm{l} \lambda-1 0$ ，加上第二个方程式，即可消去 $\boldsymbol{X}$ 。变成

(1）以矩阵表示： $\boldsymbol{A}$ 友 $( x, y )$ 的系数，B为
为方程式中天知数
等号右边的常数且A ${\bf X}={\bf B}$ .

## 2-2-3：联立方程式求解

在中学阶段，我们通常会以高斯消去法（Gaussian Elimination）解联立方程式。以下列方程式为例：

$$
\begin{matrix} x+y=1 6 \\ 1 0 x+2 5 y=2 5 0 \end{matrix}
$$

$$
- 1 0 ~ ~ ( x+y ) ~=-1 0 ~ ~ ( 1 6 )
$$
$$
1 0 x+2 5 y=2 5 0
$$
简化为
 $1 5 y=9 0$ 
得到 $\mathrm{I J} y=6$ ，再代入任一方程式，得到x-10。 以上过程，如果以线性代数求解就简单多 $\overline{{\jmath}}$ 。

$$
- 1 0 ~ ~ ( x+y ) ~=-1 0 ~ ~ ( 1 6 )
$$

$$
\tilde{\imath} \tilde{\Xi} \backslash\tilde{\chi} \gg
$$

$$
1 5 y=9 0
$$

得到 $y=6$ ，再代入任一方程式，得到 $\boldsymbol{x}=1 0$ .

以上过程，如果以线性代数求解就简单多 $\overline{{\jmath}}$ 

$$
( 2 ) \, \, \, \mathfrak{F e l} \, A=\! \left[ \! \! \begin{array} {c c c} {1} & {{}} & {1} \\ {1 0} & {{}} & {2 5} \\ \end{array} \! \! \right] \! ; \, \, \, \, X=\! \! \left[ \! \! \begin{array} {c} {x} \\ {y} \\ \end{array} \! \! \right] \! ; \, \, \, \, B=\! \! \left[ \! \! \begin{array} {c} {1 6} \\ {2 5 0} \\ \end{array} \! \! \right] \!,
$$

$$
\Pi{\bf\Pi} {\bf X}={\bf A}^{\mathrm{-1}} \mathrm{\boldmath~ B_{o} ~} \mathrm{\ i [ \bf E \bar{\Theta} \bar{\cal H} \bar{\cal X} ] ~} {\bf\Pi} {\bf\Pi} {\bf\Pi} :
$$

②A' $\boldsymbol{A}$ 等于单位矩阵，且任一矩阵乘以单位矩阵，还是等于原矩阵，故

①两边各乘 $\boldsymbol{A}$ '得

$$
\boldsymbol{A}^{\mathrm{-1}} \boldsymbol{A} \boldsymbol{X}=\boldsymbol{A}^{\mathrm{-1}} \boldsymbol{B}
$$

$$
\boldsymbol{X}=\boldsymbol{A}^{-1} \boldsymbol{B}
$$

③以上式求得 $( x, y )$ 。注意：前提是A须为非奇异矩阵

>以下程序均收录在02 03联立方程式求解.ipynb

(1）以 NumPy库求解上述联立方程式。程序代码如下：

1 #x+y=16
2 # 10x+25y=250
3
4#载人库
5 import numpy as np
6
7#定义方程式的 $A \cdot B$ 
8 A = np.array([[1 , 1], [10, 25]]) 9 B = np.array([16, 250])
10 print('A=')
11 print(A)
12 print('')
13 print('B=')
14 print(B.reshape(2, 1))
15
16 # np.linalg.solve:釜性代数求解 17 print("\n线性代数求解：'）
18 print(np.linalg.inv(A) @ B)

1 # x+y=16
2# 10x+25y=250
3
4#载人库
5 import numpy as np
6
7#定义方程式的A、B
8 = np.array([[1 , 1], [10, 25]]) 9 B = np.array([16, 250])
10 $\operatorname{p r i n t} (^{\prime} A=^{\prime} )$ 
11 $\operatorname{p r i n t} \left( A \right)$ 
12 print('')
13 $\mathrm{p r i n t} (^{\prime} B=^{\prime} )$ 
14 print(B.reshape(2, 1))
15
16 # $n p. l i n a l g. s o l v e$ ：线性代数求解 $1 7$ print("\n线性代数求解：'）
18 print(np.linalg.inv(A) @ B)

Oinv(A）： $\boldsymbol{A}$ 的反矩阵。
执行结果： $x=1 0 \,, \, \, \, y=6 \,.$ 
也可以直接使用 np.inalg.solve()函数求解。程序代码如下：

$$
\mathrm{\odot i n v} ~ \mathrm{~ ( A ) ~ : ~ A \hbar\Im\F~ k \Xi\beta\xi~_{\circ} ~}
$$

②执行结果： $x=1 0 \,, \, \, \, y=6 \,.$ 

③也可以直接使用np.linalg.solve()函数求解。程序代码如下：

1 print(np.linalg.solve(A, B))

$$
\fbox{1 \, \, \, \, \mathrm{p r i n t} \, ( \, \mathrm{n p. \, l i n a l g. \, s o l v e ( A, \ B ) \, )}}
$$

(2）画图，交叉点即联立方程式的解。程序代码如下

![](figures/61-0-FIGURE.jpg)

执行结果：如图2.8所示

![](figures/61-2-FIGURE.jpg)

图 $2. 8$ 交叉点即联立方程式的解

(3以NumF $\mathbf{y}$ 库求解下列联立方程式。

$$
- 1 x+3 y=-7 2
$$

$$
3 x+4 y-4 z=-4
$$

$$
- 2 0 x-1 2 y+5 z=-5 0
$$

$$
\star\equiv\lfloor\pm\lfloor\exists\pm\rfloor\rceil\rceil\subset\Gamma:
$$

2也可以使用 $\mathrm{S y m P y}$ 库求解，直接将联立方程式整理在等号左边,使用 so $\mathrm{v e ( )}$ 函数，参数内的多项式均假设等号 $(=)$ 右边为 $0_{\mathrm{c}}$ 

![](figures/62-1-FIGURE.jpg)

D执行结果：
$$
( x, \, y, \, z ) ~=~ ( 1 2, \,-2 0, \,-1 0 ) ~ ~ ~ \circ
$$

| 1 | irom sympy.solvers import solve |
| 2 | irom sympy import symbols |
| 3 A | 设完亦数 |
| 4 C | 汉上刻 561 |
| 5 6 | ,y, Z= symbols('xy z') |
| 7 | ：解题：设定联立方程式，等号石边预设为园 |
| 8 | olve([-1*x + 3*y + 72,3*x+4*y-4*7+ 4,-20*x + -12*y+ 5*7+ 501) |

微积分包括微分（Differentiation）与积分（Integration），微分是描述函数某一点的变化率，借白微分可以得到特定点的斜率 $\mathrm{( S l o p e )}$ 或梯度（Gradient），即变化的速度，二次微分可以得到加速度。积分则是微分的逆运算，积分可以计算长度、面积、体积，也可以用来计算累积的概率密度函数（Probability Density
Function）。

在机器学习中，微分在求解的过程中占有举足轻重的地 $\overrightarrow{\omicron}$ ，因此 TensorFlow框架就提供自动微分（Automatic Differentiation) 的功能，用以计算各特征变量的梯度，进而求得模型的权重
 $( \mathrm{W e i g h t} )$ 参数。

## 2-3微积分微分用于描述函数的变化率（Rate of Change），如y-2x+5, 表示x每增加一单 $\imath\vec{\Sigma}$ ，y会增加 $2$ 。因此，变化率就等于 $2$ ，也称达斜率； $5$ 为截距（Intercept) 或称偏差（Bias) 如图2.9所示。

这就是微分的定 $\mathcal{X}$ ，但上述极限值（limit） $\overline{{\varLambda}} \mathrm{\overline{{\Lambda}}}$ 一定存在，其存在的要素如 $\mathrm{F}$ :

(1）h为正值时的极限值等于1为负值时的极限值，亦即函数在该点时是连续的。

## 2-3-1 微分

![](figures/64-4-FIGURE.jpg)

图 $2. 9$ 斜率与截距

我们先不管截距，只看斜率，算法为：取非常相近的两个点 (距离 $\boldsymbol{h}$ 趋近于 $0 )$ ，y坐标值之差 $( \Delta y )$ 除以x坐标值之差
(Ax），有

$$
f^{\prime} ( x )=\frac{\Delta y} {\Delta x}=\operatorname* {l i m}_{h \to0} \Biggl[ \frac{f ( x+h )-f ( x )} {h} \Biggr]
$$

(2）上述极限值不等于无穷大（x）或负无穷大（-o O

如图2.10所示的函数在x-5 的地方是连续的，由上方（5.25) 逼近，或由下方 $( 4. 7 5 )$ 逼近是相等的，相关彩色图形可参考 02 04_微分.ipynb

相反地，图2.11所示函数在x0时是不连续的，逼近x-0时有两个解。

![](figures/65-2-FIGURE.jpg)

图 $2. 1 0$ 连续函数

![](figures/65-4-FIGURE.jpg)

图 $2$ .1不连续函数

接着来看看几个应用实例。

>以下程序请参考02.04微分.ipynb。

（1）试绘制一次方函数f $( \mathbf{x} ) ~=\! 2 x+5 \,$ 。程序代码如下： (2）试绘制二次方曲线f $( x ) ~=\!-1 0 \, x^{2}+1 0 0 \, x+5 \,,$ 求最大值。程序代码如 $\mathrm{F}$ :

![](figures/66-1-FIGURE.jpg)

执行结果：如图2.12所示

![](figures/66-3-FIGURE.jpg)

图2.12，一次方函数执行结果

由执行结果可以看出，一次方函数每一点的斜率均相同。 $\odot\ -$ 次方函数整条在线的每一个点的斜率都相同，但是二次方曲线上的每一个点的斜率就者 $\beta\pi< \eta$ 样 $\overline{{\j}}$ ，如图2.13所示，相关彩色图形可参考02 04 微分.ipynb。

![](figures/67-1-FIGURE.jpg)

执行结果：如图2.135所示。由执行结果可以得到以下结论

![](figures/67-3-FIGURE.jpg)

图2.13二次方曲线执行结果

绿线（细抛物线）：二次曲线，是一条对称的抛物线

 $\clubsuit$ 紫线（斜线）：抛物线的一阶导数。 ②每一个点的斜率即该点与二次曲线的切线（红线），均不相同，斜率值可通过微分求得 $- \ss\uparrow$ 导数(图中的斜线），随着x变大，斜率越来越小，二次曲线的最大值就发生在斜率等于 $\mathbf{0}$ 的地方，当x-5时， $f ~ ( \boldsymbol{x} ) ~=2 5 5 ~ \mathbf{_{c}}$ 

(3）试绘制二次方由线f $( \mathbf{x} ) \ =\mathbf{x}^{2} \!+\! 2 \mathbf{x}+7,$ 、求最小值。程序代码如下：

 $\bigoplus$ 红线（抛物线的切线）：三个点 $( 2, 5, 8 )$ 的斜率

![](figures/68-3-FIGURE.jpg)

执行结果：如图2.14所示。

由执行结果可得：斜率值可通过微分求得一阶导数（图中的斜线) 随着x变大，斜率越来越大，二次白线的最小值就发生在斜
、当x--1时，f $( x ) ~=6_{\mathrm{c}}$ .
率等于0的地方，

综合范例（2）（3），可以得知微分两次的二阶导数 $( \boldsymbol{f}^{\textbf{n}}$  $( \boldsymbol{x} )$ ）为常数，且为正值时，函数有最小值，反之，为负值时， 函数有最大值。但若f(x）为三次方（以 $\L)$ 的函数，一阶导数等于0的点，可能只是区域的最佳解（Local Minimum/Maximum) 而不是全局最佳解（Global Minimum/Maximum)

(4）试绘制三次方由线f $( \mathbf{x} ) ~=\mathbf{x}^{3} \!-\! 2 \mathbf{x} \!+\! 1 0 0 \,,$ 求最小值。程序代码如下：

![](figures/69-3-FIGURE.jpg)

图2.14二次方曲线执行结果

$$
\emptyset\equiv i \mathcal{R}
$$
方出线f $( \mathbf{x} ) \ =\mathbf{x}^{3} \!-\! 2 \mathbf{x} \!+\! 1 0 0$ 在斜率等于0的点 $\P$ 是区域
1
的最佳解

![](figures/70-1-FIGURE.jpg)

执行结果：如图2.15所示。由执行结果可以得到以下结论

![](figures/70-3-FIGURE.jpg)

图2.15三次方曲线执行结果

②三次方曲线一般为凸函数时才有全局最佳解

上述程序中的fd)函数为一阶导数，它们是如何求得的？只要
5
运用以下微分的定理，就可以轻易解出上述范例的一阶导数，相关定理整理如 $\mathrm{F}$ 。

 $( 1 ) ~ f ~ ( ~ x ) ~-\beta$ 个导数的表示法为？ $( x ) \Amalg\& \frac{\mathrm{d} y} {\mathrm{d} x} \circ$ 
 $( 2 ) ~ f ~ ( ~ x )$ 为常数 $( C ) \to f \quad( x ) \ =0 \circ$ 
(3)f(x)=Cg (x) -→f(x) =Og'(x
(4) f(x)=g (x) +h (x) >f (x) -g'(x) +H (x)。
(5）次方的规则
$$
f \mid\mid x \rangle=x^{n} \to f \mid( x ) \mid=n x^{n-1} \circ
$$
 $( 6 )$ 乘积的现： $\frac{\mathrm{d} [ f ( x ) g ( x ) ]} {\mathrm{d} x}=f^{\prime} ( x ) g ( x )+f ($ x)g'(x)o
(7）商的规则：若r（x）-s（x）/t（x），则
$$
' ( x )=\frac{s^{\prime} ( x ) t ( x )-s ( x ) t^{\prime} ( x )} {[ t ( x )^{2} ]} \circ
$$

以上一节范例（2）f（x）--10°+100x15 为例，针对多项式的每一项个别微分再相加，就得到f(x）的一阶导数为

## 2-3-2微分定理

（1）f（x）一阶导数的表示法为 $( x ) \mod\frac{\mathrm{d} y} {\mathrm{d} x} \circ$ 

(2）f（x）为常数 $( C ) \to f \quad( x ) \ =0 \circ$ 

(3)f
$$
( \textbf{x} )=C g \textbf{\textit{( x )}} \to\textit{\textbf{f}} ( \textbf{x )}=C g^{\prime} \textbf{\textit{( x )}}
$$
x) o

(4）f
$$
( \textbf{x} )=g \textbf{( x )}+h \textbf{( x )} \to f \textbf{( x )}=g^{\prime} \textbf{( x )}+h
$$
1(x O

$$
( 5 ) \; \; \succchi\brack\mathfrak{A} \mathfrak{A I I I} \mathfrak{I I I} : \; \; f \; \; ( \textup{A} \textup{A} \mathfrak{A} ) \; \;=x^{n} \to\mathfrak{A I I}
$$
(x)=nx"'.

$$
\left( 6 \right) \Amalg[ \Pi\iiint\Re\Re\Pi] \colon\ \frac{\mathrm{d} \left[ f ( x ) g ( x ) \right]} {\mathrm{d} x}=f^{\prime} ( x ),
$$
g(x)+/(x)g'(r).

(7）商的规则：若r（x)=s（x) /t（x 则
$$
r^{\prime} ( x )=\frac{s^{\prime} ( x ) t ( x )-s ( x ) t^{\prime} ( x )} {[ t ( x )^{2} ]} \circ
$$

(s）链式法则（Chain Rule

$$
\frac{\mathrm{d}} {\mathrm{d} x} [ o ( i ( x ) ) ]=o^{\prime} ( i ( x ) ) \cdot i^{\prime} ( x )
$$

$$
f ~ ( \mathbf{x} ) ~=-2 0 ~ \mathbf{x}+1 0 0
$$

SymPy库直接支持微积分函数的计算，可以验证定理，接下来我们就写一些程序来练 $\exists-\mathrm{F}_{c}$ 

(3）乘积的规则： $\frac{\mathrm{d} [ f ( x ) g ( x ) ]} {\mathrm{d} x}=f^{\prime} ( x ) g ( x )+$ f（(x)g"te)。程序代码如下:

>以下程序请参考02 04_微分.ipynb

(1)f(x）为常数 $( C ) \to f^{\prime} ( x )=0$ 。程序代码如下：

1#常数微分 $f \big( x \big) \;=\; C \implies\big> \; f^{\prime} \big( x \big) \;=\; 0$ 2 from sympy import *
3
4 $x \;=\; S y m b o l ( \,^{\prime} x^{\prime} \, )$ 
5 $\# f ( x ) \stackrel{\vee} {\lambda} \frac{H} {1 0} \frac{H} {X}$ 
6 $y=\ 0 \ast\times+5$ 
7 yprime = y.diff(x)
8 yprime

执行结果：

$$
( 2 ) \ \ f ( x ) \! \!=C g ( x ) \! \to\! f^{\prime} ( x )=C g^{\prime} ( x ) \circ
$$
程序代码如下

| 1 | #$f(x)=Cg(x)$==> f'(x)$= Cg'(x) |
| 2 |  |
| 3 | from sympy import * |
| 4 |  |
| 5 | x=Symbol('x') |
| 6 |  |
| 7 | #Cg(x) |
| 8 | y1=5*x **2 |
| 9 | yprimel = y1.diff(x) |
| 10 | 厂 print(vprime1)  |
| 一广 11  |
| 一 12 | #ax) |
| 13 | V2=x**2 |
| 14 | #Ca'x)  |
| 15 | vprime2=5 * v2.diff(x) |
| 16 print(yprime2)  |
| 17 |
| 18 | #比较 |
| 19 | vprime1 == yprime1 |

执行结果均为10x。

| 1 | 1 # (d[f(x)g(x)])/dx= f"(x)g(x)+f(x)g'(x |
| 2 | 2  |
| 3 | 3 from sympy import * |
| 4 5 - | 4 5 x = Symbol('x') 一  |
| 6 | 6  |
| 7 | 7 #d[f(x)g(x)])/dx |
| 8 | B f=x**2 |
| 9 | 9 g=x**3 |
| 10 | 0 v1=f*g |
| 11 | 1 yprime1 = y1.diff(x) |
| 12 print(yprime1) |
| 13 |
| 14 #fXIDg'X |
| 15 | 5 yprime2= f.diff(x)*g+ f*g.diff(x) |
| 16 print(yprime2) |
| 17 |
| 18 | 8 #比较 |
| 19 | 9 yprime1 == yprime1 |

$$
\pm\hbar\atop1 5 \pm\pm\mp\hbar: 5 \times4 \circ
$$

(4）链式法则： $\frac{\mathrm{d}} {\mathrm{d} x} [ o ( i ( x ) ) ]=o^{\prime} ( i ( x ) ) \cdot i^{\prime} ( x )_{\circ}$ 程序代码如下

| 1 | rom sympy import * |
| 2 |  |
| 3 | = Symbol('x') |
| 4 |  |
| 5 | d[f(g(x))])/dx |
| 6 | =x**3 |
| 7 | =g**2 |
| 8 | orimel = f.diff(x) |
| 9 print(yprime1) |
| 10 |
| 一 11 | f'0' |
| 12 | JCCN = Svmbol('g') |
| 13 | 2O 三口**2  |
| 14 | 口 L义*3 |
| 15 | 欢 orime2= f.diff(g) * g1.diff(x) |
| 16 | 口口将f'a)的口以零3取代 |
|  C 17 print(vprime2.subs({g:x ** 3})) |
| O 18  |
| 19 | 比较 |
| 20 | orime1 svprime1 |

执行结果：6x

$$
( 5 ) \; \; \exists\Delta\i\mathrm{I I} f \; \; ( x ) \;=-1 0 x^{2}+1 0 0 x+5_{\circ}
$$
程序代码如下

$$
\begin{array} {r l} {{\textstyle1} \quad{\mathrm{f r o m ~}} \quad\mathrm{5 y m p y ~ \quad{\mathrm{i m p o r t ~}} *}} \\ {{\textstyle2}} \\ {{\textstyle3}} & {{\times} \quad{\mathrm{5 y m b o 1}} ( {\textstyle^{\prime} x^{\prime}} )} \\ {{\textstyle4}} & {{\times} \quad{\textstyle f} ( x )=-{\textstyle1 0} \, {\times} 2 \ +{\textstyle1 0 0} \, {\bar{x}}+5} \\ {{\textstyle5}} & {{\times} \quad{\times} \quad{\times} * \neq2 \ +{\textstyle1 0 0} \ * \ \times\ +\ 5} \\ {{\textstyle5}} & {{\mathrm{y p r i m e}} \quad{\times} \quad{\mathrm{e r ~}} * \ * \ \mathbf{2} \times\ +{\ *} \ 1 0 0 \ * \ \times\ +\ 5} \\ {{\textstyle7} \quad\mathrm{y p r i m e} \quad{\mathrm{y p r i m e}} \quad{\mathrm{y. d i f f}} ( x )} \end{array}
$$

执行结果： $1 0 0-2 0 x_{\circ}$ ## 接着，利用一阶导数等于0，求最大值。程序代码如下：

| 1 n | from |  |  |
| --- | --- | --- | --- |
| 4 3 | #一 |  |  |
| 4 | dict1 |  |  |
| 福 5 | print |  |  |
| 6 | x1= |  |  |
| 7 | print | 100 本 |  |

执行结果：x-5时，最大值为255

$$
( 6 ) ~ ~ \exists\Delta\i\b F f ~ ( x ) ~=x^{2}+2 x+7_{\circ}
$$
程序代码如下

$$
\begin{array} {r l} {1} & {{} {\mathrm{f r o m ~}} {\mathrm{~ s y m p y ~ i m p o r t ~}} *} \\ {2} & {{}} \\ {3} & {{} x=S y {\mathrm{m b o l ~ ('~ x')}}} \\ {4} & {{} {\times} \ {f ( x )=z 2+2 z+7}} \\ {5} & {{} y=( x * * z ) \; \;+\; \; {\mathrm{( 2^{*} x )}} \;+\; 7} \\ {6} & {{} {\mathrm{y p r i m e ~}}=\; y \cdot{\mathrm{d i f ~ f ~ ( x )}}} \end{array}
$$

执行结果： $2 x+2_{\circ}$ 

接着，利用一阶导数等于0，求最小值。程序代码如下：

1 from sympy.solvers import solve
2
3#一阶导数=0
4 dict1 = solve([yprime])
5 print(dict1)
g nasdictish $= \{\left( \times1^{*} \times2 \right) \;+\; \left( 2^{*} \times1 \right) \;+\; 7 \}^{\prime} \left( \right)$ 6
 $\mathrm{p r i n t} ( f^{\prime} x=\{\times1 \}$ ，最小值

执行结果：X-1时，最小值为 $6$ . 在机器学习中，偏微分（Partial Difrentiation）在求解的过程中占有举足轻重的地位，常用来计算各特征变量的梯度，进而求得最佳权重。梯度与斜率相似，单一变量的变化率称为斜率，多变量的斜率称梯度。

在 $\mathrm{t-}$ 节中，（）只有单变量，如果x是多个变量、 $\boldsymbol{X}_{2}$ 、 $X_{3}...,$ 要如何找到最 $\rfloor\j\gpmb[ 1$ 直或最大值呢？这时，我们就可以使用偏微分求取每个变量的梯度，让函数沿着特定方向寻找最佳解，如图 2.16 所示即沿着等高线（Contour），逐步向圆心逼近，这就是所谓的梯度下降法（Gradient Descent)

## 2-3-3：偏微分

![](figures/75-3-FIGURE.jpg)

图 $2. 1 6$ 梯度下降法图解

>以下程序请参考02 05偏微分.ipynb

$$
( 1 ) \setminus\mathrm{\#} \mathrm{\#} \mathrm{i} \frac{\pi} {\chi} f \setminus( x, y ) \ =x^{2}+y^{2},
$$
请分别对 ${\boldsymbol X}$ 、y作偏微分

解题：先针对x作偏微分，有

$$
\frac{\partial f ( x, y )} {\partial x}=\frac{\partial( x^{2}+y^{2} )} {\partial x}
$$

其中第 $8$ 、12行的 $\mathbf{y 1}$ adi(X） yL.diff $\mathrm{( y )}$ 分别表示对x、作偏微分

将其他变量视为常数，对每一项个别微分，得

$$
\begin{matrix} \frac{\partial x^{2}} {\partial x}=2 x \\ \frac{\partial y^{2}} {\partial x}=0 \end{matrix}
$$

$$
\hbar\Pi, \cong, \ H, \ H
$$

$$
\frac{\partial f ( x, y )} {\partial x}=2 x+0=2 x
$$

再针对y作偏微分，同样将其他变量视为常数，有

$$
\frac{\partial f ( x, y )} {\partial y}=0+2 y=2 y
$$

$$
\bigsqcup\lfloor\pm\rfloor\leq\pm\leq\pm\bigstar1 \rfloor
$$

$$
\frac{\partial f ( x, y )} {\partial x}=2 x \, \frac{\partial f ( x, y )} {\partial y}=2 y
$$

$$
F^{\textsc{g}} \equiv1 \pm F \pm\textsc{g d T} :
$$

| 1 | = x^2 +y^2 |
| 2 |  |
| 3 | mpy import * |
| 4 | symbols("x y') |
| 5 |  |
| 6 | 俪微力 |
| 7 | *2 +y**2 |
| 8 | =y1.diff(x) |
| 9 print(yprime1) |
| 10 |
| 11 | 偏微分 |
| 12 | =y1.diff(y) |
| 13 | prime2) |
| 14 |  |

(2）假设f $( \mathbf{x} ) \ =\mathbf{x}^{2}$ ，请使用梯度下降法找最小值。

④重复步骤②和3，判断梯度是否接近于 $0$ ，若已很逼近于 0，即可找到最佳解

解题：程序逻辑如下

①任意设定一起始点（xstart O

2计算该点的梯度d（x O

3沿着梯度更新x，逐步逼近最佳解，幅度大小以学习率控制。新的x-0·学习率（Learning Rate *梯度。

$$
\star\equiv\pm\pm\pm\pm\pm\pm\pm\pm\pm
$$

![](figures/78-0-FIGURE.jpg)

执行结果：如图2.17所示

![](figures/78-2-FIGURE.jpg)

图 $2. 1 7$ 梯度下降法实践

得到x每一点的坐标为：
 $[ 5$ . 4. 3.2 $2$ .56 $2$ .05 1.64 1.31 1.05 0.84 0.67 0.54 $\diamond$ .43 0.34 $\emptyset. 2 7 \ 0. 2 2$ 0.18 0.14 0.11 0.09 0.07 0.06 0.05 0.04 $\emptyset. \O3 \emptyset. \O2 \ \emptyset. \O2 \rfloor$ 

$$
\frac{} {\pi}_{\circ}
$$
①改变起始点 $\mathrm{X \_~} \mathrm{s t a r t}=-5$ ，依然可以找到最小值，如图2.18所

2设定学习率 $\mathrm{I r}=0. 9$ ：如果函数较复杂，可能会跳过最小值如图2.19所示

得到x每一点的坐标为

我们可以改变第24-26列的参数，观察执行结果

![](figures/79-5-FIGURE.jpg)

图 $2. 1 8$  ${\bf x}$ start 参数
改变

![](figures/79-7-FIGURE.jpg)

 $2. 1 9$ 设定 $\mathrm{I r}=0. 9$ 图

3设定学习率 $\mathrm{I r} \,=\, 0. 0 1$ ：还末逼近最小值，就提早停止了，可以增加执行周期数来解决问题，如图2.20所示。

上述程序是神经网络优化器求解的简化版，在后续的章节会进行详细的探讨，目前只聚焦在说明偏微分在深度学习的应用。

![](figures/80-2-FIGURE.jpg)

 $2. 2 0$ 设定 $\mathrm{I r} \,=\, 0. 0 1$ 图

在优化求解中，也经常利用一阶导数等于0的特性，求取最佳解。例如， $\operatorname{l o g}$ 普通最小二乘法（Ordinary Least Square, OLS）对简羊线性回归 $y=$ wx+b求解。首先定义目标函数（Object
Function）或称损失函数（Loss Function）为均方误差（MSE)） 即预测值与实际值差距的平方和，MSE当然越小越好，所以它是一个最 $\imath\rfloor\jmath\g$ 化的问题，所以，我们可以利用偏微分推导出 $\operatorname{Z M} \overline{{\mathbf{c}}}$ ，过程如下。

式中： 即实际值 $( y )$ 与预测值 $( \hat{y} )$ 之差： $\boldsymbol{n}$ 为为样
s为误差，
本个数。

(3）分别对w及b作偏微分，并且令 $- \ss\uparrow$ 导数等于0，可以得到两个联立方程式，进而求得w及b的解。

## 2-3-4 简单线性口归求解

\mathrm{M S E}=\sum\varepsilon^{2} \left/ \right. n=\sum\left( y-\hat{y} \right)^{2} \left/ \eta\right.

(2) MSE-SSE/ $\boldsymbol{n}$ 。我们忽略常数 $\boldsymbol{n}$ ，可以只考虑SSE，即
/

$$
{\mathrm{S S E}}=\sum\varepsilon^{2}=\sum( y-{\hat{y}} )^{2}=\sum( y-w x-b )^{2}
$$

## (4）对b偏微分，又因

$$
\begin{array} {c} {{f^{\prime} ( x ) \!=\! g ( x ) g ( x ) \!=\! g^{\prime} ( x ) g ( x ) \!+\! g ( x ) g^{\prime} ( x ) \!=\! 2 g ( x ) g^{\prime} ( x )}} \\ {{\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad\quad\qquad\quad\quad\qquad\quad\quad\quad\qquad\quad\quad\quad\quad\qquad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\) \) \quad\) \quad\) \) \quad\) \) \quad\) \ -\ -\quad\) \quad\ -\ -\ -\) \ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -\ -
$$

两边同除 $\mathrm{U} \lambda-2$ ，得

$$
\sum_{i=1}^{n} ( y-w x-b )=0
$$

$$
\flat\neq\flat\bigstar\bigstar
$$

$$
\sum_{i=1}^{n} y-\sum_{i=1}^{n} w x-\sum_{i=1}^{n} b=0
$$

除以 $\boldsymbol{n}$ , $\overline{{x}} \,,$  $\bar{y}$ 为x、）的平均数。有

$$
\overline{{y}}-w \overline{{x}}-b=0
$$

$$
F^{\pm} I_{J}^{\mp} I_{\pm}^{\Xi}
$$

$$
b=\overline{{y}}-w \overline{{x}}
$$

(5）对W扁微分，得

$$
\frac{\partial\mathrm{S S E}} {\partial w}=-2 \sum_{i=1}^{n} \, ( y-w x-b ) x=0
$$

两边同除 $\mathrm{D} l-2$ ，得

$$
\sum_{i=1}^{n} ( y-w x-b ) x=0
$$

$$
\Box A=\mod\Xi
$$

$$
\sum_{i=1}^{n} y x-\sum_{i=1}^{n} w x-\sum_{i=1}^{n} b x=0
$$

代入（4）的计算结果 $b=\overline{{y}}-w \overline{{x}}$ ，有

$$
\sum_{i=1}^{n} y x-\sum_{i=1}^{n} w x-\sum_{i=1}^{n} ( \overline{y}-w \overline{x} \, ) x=0
$$

$$
\lfloor\mathsf{L i s \rfloor} \not\equiv\mathsf{I I} \Xi
$$

现有一个世界人口统计数据集，以年度（year）为x，人口数为 $\boldsymbol{y},$ 依上达 $\big< \lambda$  $\boldsymbol{W}.$ 、 $\boldsymbol{b}$ 。程序代码如 $\mathrm{F}$ :
式计算回归系数

$$
\begin{array} {c} {{\displaystyle\sum_{i=1}^{n} ( y-\overline{{y}} ) x-w {\sum_{i=1}^{n} ( x^{2}-\overline{{x}} x )}=0}} \\ {{w={\sum_{i=1}^{n} ( y-\overline{{y}} ) x / \sum_{i=1}^{n} ( x^{2}-\overline{{x}} x )}}} \\ {{w={\sum_{i=1}^{n} ( y-\overline{{y}} ) x / \sum_{i=1}^{n} ( x-\overline{{x}} )^{2}}}} \end{array}
$$

$$
( 6 ) \, \sharp\pm i \AA_{\circ}
$$

$$
\begin{array} {c} {w \!=\sum_{i=1}^{n} ( y-\overline{{y}} ) x \, / \sum_{i=1}^{n} ( x-\overline{{x}} )^{2}} \\ {b \!=\overline{{y}}-w \overline{{x}}} \\ \end{array}
$$

>以下程序请参考02 06线性日月ipynb

执行结果：w-0.061159358661557375,
$$
b=\!-\! 1 1 6. 3 5 6 3 1 0 5 6 1 1 7 6 8 7_{\circ}
$$

使用NunF $\mathbf{y}$ 的现成函数polyfit()验算。程序代码如下

## 执行结果： w=0.061159358661554586, b-- $1 1 6. 3 5 6 3 1 0 5 6 1 1 7 1 2 1$ ，答案相差不多

积分则是微分的相反运算，与微分互为逆运算，微分用于斜率或梯度的计算，积分则可以计算长度、面积、体积，也可以用来计算累积的概率密度函数。

## 2-3-5 积分

## （1）积分一般数学表示式为

$$
\int_{0}^{n} f ( x ) \mathrm{d} x
$$

$$
( 2 ) \; \; \sharp\sharp\gamma\; \; ( x ) \;=x, \; \; \sharp\downarrow\gamma\Rightarrow\sharp\neq\lambda\; \frac{1} {2} x^{2} \; \circ
$$

(3）限定范围的积分：先求积分， 、下限代入多项
再将上
式，相减可得结果。例如

$$
\begin{array} {c} {\int_{0}^{3} x \mathrm{d} x=\frac{1} {2} \, x^{2} \, \Big|_{0}^{3}} \\ {( ( 1 / 2 )^{*} \, 3^{2} )-( ( 1 / 2 )^{*} \, 0^{2} )=4. 5} \\ \end{array}
$$

接下来撰写程序，看看积分如何计算

>以下程序请参考 $0 2 \bsmile0 7$ 积分.ipynb

 ${\bf1}$ 。进行下列积分计算并作图

$$
\int_{0}^{3} x \ \mathrm{d} x
$$

）积分运算：SPy库支持积分运算。程序代码如下： (1)

②第 $9$ 行呼叫 integrate.cuadt)，作限定范围的积分，参数如下

| 1 | #载入库 |
| 2 | import numpy as np |
| 3 | import scipy.integrate as integrate |
| 4 | import tnumpy as np |
| 5 | import math |
| 6 |  |
| 7 |
| 8 f=lambdax:x |
| 9 i, e= integrate.quad(f,0,3) |
| 10 |  |
| 11 | print(·积分值：'+str(i)） |
| 12 | print('误差：'+str(e)） |

执行结果：积分值为 $4. 5$ ；误差为 $1 4. 9 9 6 0 0 3 6 1 0 8 1 3 2 0 4 4 4 \mathrm{e}^{-1 4} \circ$ 程序说明如下。

## 执行结果：积分值为4.5；误差为4.996036108132044e-14 2

$$
\star\Xi\equiv i \pm\mathrm{H H F_{o}}
$$

D第 $3$ 行载入SciPy库

函数f（x）:
范围下限：设为负无穷大（-oo）
 $\clubsuit$ 范围上限：设为正无穷大（0o）
输出:含积分结果及误差值。

函数f $( x ) \quad;$ 

(2）作图。程序代码如下：

1 import matplotlib.pyplot as plt
2
3#样本点
4 ${\bf x} \,=\, \mathrm{r a n g e} \, ( \emptyset)$ ，11）
5
6#作图
7 plt.plot(x, f(x), color='purple')
8
9 #积分面积
10 area = np.arange(o, 3, 1/20)
11 plt.fill between(area, f(area), color='green') 12
13#设定图形属性
14 plt.xlabel('x')
15 plt.ylabel('f(x)')
16 plt.grid()
17
18 plt.show()

(1）正态分布的概率密度函数（Probability Density Funcion PDF）即

执行结果：如图2.21所示

![](figures/87-2-FIGURE.jpg)

图2.21积分运算执行结果

程序说明：第11行机Lbetwen)会填满整个积分区域

## 2.。计算正态分布（Normal Distribution）的概率并作图

$$
f ( x ; \mu, \sigma) \!=\! \frac{1} {\sqrt{2 \pi^{*} \sigma^{2}}} \! * \! e^{-\frac{1} {2} \ast( \frac{x-\mu} {\sigma} )^{2}}
$$

(2）计算正态分布 $(-\infty, \, \infty)$ 的概率。程序代码如下

![](figures/87-8-FIGURE.jpg)

执行结果：累积概率为 1.0；误差为1.0178191437091558-08

①第 $7$ 、 $8$ k $( \mathrm{m e a n} )$ 为 $0$ 、标准差（std）为1,
行定义平均数
也就是「标准」正态分布，又称 $\mathsf{Z}$ 分布

3注意： J $1$ ，即所有事件发生的概
任何概率分布的总和必然为
率总和必然为100 $\not\nabla_{O}$ 。

包括 $\pm1, ~ \pm2$ 、±3倍标准差，我们可以计算其概率。有关置信区间的定义会在概率与统计的章节进行说明。

$$
\star\equiv\pm\pm\pm\pm\pm\pm\pm\mp\pm\pm\pm\pm\pm\pm\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\circ
$$

2第11列定义正态分布的概率密度函数（PDF） O

## 3：正态分布常见的置信区间

D=1倍标准差的置信区间概率为68.3 $\not\sim$ 。程序代码如下
O

执行结果：累积概率为0.683；误差为
$$
7. 5 7 9 3 7 5 9 2 8 4 0 2 4 7 6 \mathrm{e}^{-1 5} \circ
$$

integrate.cuad)参数设为-1及1

2:-2倍标准差的置信区间概率为95.4%。程序代码如下：

![](figures/88-10-FIGURE.jpg)

 $\bigoplus$ 执行结果：累积概率为0.954；误差为 1.8403560456416134e-11.

今我们常用±1.96倍标准差的置信区间，概率为95 $\not\supset$ ，概率刚好为整数。程序代码如 $\mathrm{F}$ :

⑤另外，可以使用随机数 $\nabla\omega$ 直方图，绘制标准正态分布图。程序代码如下:

integrate.uad（)诊数设为2及 $2$ 

3-3倍标准差的置信区间概率为99.7%。程序代码如下：

![](figures/89-4-FIGURE.jpg)

执行结果： 0.997；误差为
累积概率为
$$
1. 1 0 7 2 2 5 6 5 0 3 1 0 5 3 1 4 \mathrm{e}^{-1 4} \circ
$$

 $\bigoplus$ 执行结果：累积概率为0.997；误差为

integrate.ouadl修数设为-3及3

![](figures/89-8-FIGURE.jpg)

 $\clubsuit$ 执行结果：累积概率为0.95；误差为
1
$$
1. 0 4 7 4 0 9 6 4 9 2 7 0 1 3 2 5 \mathrm{e}^{-1 1} \circ
$$

 $\bullet$ integrate.quad()参数设为-1.96及1.96

 $\bigoplus$ 第 $9$ 行 sns.distplot (x, hist-False) 使用 Seaborn 画直方图，参数hist-False 表示不画阶梯直方图，只画平滑由线。

第17行axvine()：画垂直线，标 $\bar{\pi} \pm1, ~ \pm2$ 、=3倍标准差。

![](figures/90-2-FIGURE.jpg)

执行结果：如图2.22所示

![](figures/90-4-FIGURE.jpg)

图2.22、正态分布±1、-2、±3倍标准差的置信区间

$$
\star\Xi\equiv\pm\mathrm{H F}_{\circ}
$$

统计是从数据所推衍出来的信息，包括一些描述统计量、概率分布等，如平均数、标准差等。而我们搜集到的数据统称为数据集 (Dataset），它包括一些观察值（Observations）或案例
(Cases），即数据集的行，以及一些特征（Features）或称属性 (Attributes），即数据集的字段。例如，要预测某市下一任市长， 我们做一次问卷调查，相关定义如 $\mathrm{F}$ 。

## 2-4 概率与统计

(1）数据集：全部问卷调查数据
(2）观察值：每张问卷。
(3）特征或属性：每一个问题，通常以X表示
(4）属性值或特征值：每一个问题的回答。
(5）目标（Target）字段：市长候选人，通常以y表

(1）数据集：全部问卷调查数据

 $( 2 )$ 观察值：每张问卷

(3）特征或属性一个问题，通常以X表 $\overline{{\Pi}}$ . 每一 O

(4) ：每一个问题的回答属性值或特征值:

(5）目标（Target）字段：市长候选人，通常 $\mathcal{U}$ y表示

下面我们会依序介绍下列内容：

(1）拍样:
(2）描述统计量
(3）概率；
(4）概率分布函数
(5）假设检定。具体如图2.23所示

$$
( 1 ) \ \ \ddag\equiv\hbar\notin\psi;
$$

 $( 2 )$ 描述统计量

$$
( 3 ) \quad\sharp\neq\sharp\geq\sharp
$$

(5）假设检定。具体如图2.23 所示

![](figures/92-0-FIGURE.jpg)

图2.23、基础统计介绍的范围及其关联

依照特征的数据类型，分为定性（Qualitative）及定量
(Quantitative）的字段。定性字段为非数值型的字段，通常包含有限类别，又可以分为名义数据（Nominal Data）及有序数据
(Ordinal Data）；定量字段为数值型字段，又可以分为离散型数据（Discrete Data）及连续型数据（Continuous Data）。预测的二标字段如为离散型变量，则适用分类（Classification）的算法； 反 $\grave{\varUpsilon}$ ，目标字段为连续型变量，则适用回归（Regression）的算法。

(1）名义数据：域值并没有顺序或大小的隐含意义，如颜色，红、蓝、绿并没有谁大谁小的隐含意义。转换为代码时，应以 One-Hot Encoding 处理，将每一类别转为个别的哑变量（Dummy Variable），每个哑变数只有True/False或 $1 / 0$ 两种值，如图2.24 所示。

Color特征有三种类别，经过One-Hot Encoding 处理，会转换为三个哑变量。

## 2-4-1数据类型

|  | color r | is_blue | is_green | is red |
| --- | --- | --- | --- | --- |
| 0 | green | 0 | 1 | 0 |
| 1 | red | 0 | 0 | 1 |
| 2 | green | 0 | 1 | 0 |
| 3 | blue | 1 | 0 | 0 |

图2.24哑变量

>以下程序请参考02 08：特征转换.ipynb。程序代码如下：

(2）有序数据（Ordinal Data）：域值有顺序或大小的隐含意 $\mathcal{X}$ ，如衣服尺寸XL>L> M> S，如图2.25所示。

| df2 = pd.get dummies(df["color"], columns=["color"], |
| 2 prefix='is', prefix sep= |
| 3 |
| # 连结转换前后的字段，相互比较 |
| pd.concat((df["color"], df2), axis=1) |

数据框（Data Frame）包含要转换的字段、前置符号 (prefix) 分隔符（preix sep

|  | color | size | price | classlabe |
| --- | --- | --- | --- | --- |
| 0 | green | XL | 10.1 | class1 |
| 1 | red | M | 10.1 | class1 |
| 2 | green | L | 13.5 | class2 |
| 3 | blue | S | 15.3 | class1 |

图 $2. 2 5$ 有序数据示例

size 字段依尺寸大小，分别编码为XL（4 L(3) M (2) S(1) 转换后如图2.26所示。

|  | color 福 | size | price | classlabel |
| --- | --- | --- | --- | --- |
| 0 | green | 4 | 10.1 | class1 |
| 1 | red | 2 | 10.1 | class1 |
|  | green | 3 | 13.5 | class2 |
| 3 | blue | 1 | 15.3 | class1 |

图 $2. 2 6$ 编码转换结杲

$$
\pi_{\pm}^{\Pi} \equiv\Gamma_{\pm}^{\Pi} \pm\Gamma_{\pm} \pm\Gamma_{\pm}^{\Pi} \Gamma_{\pm}^{\Pi} \pm\Gamma_{\pm}^{\Pi} \Xi_{\pm}^{\Pi} \Xi_{\pm}^{\Pi} \Xi_{\pm}^{\Pi} \Xi_{\pm}^{\Pi} \Xi_{\pm}^{\Pi} \Xi_{\pm}^{\Pi} \Xi_{\pm}^{\Pi} \Xi_{\pm}^{\Pi} \Xi_{\pm}^{\Pi} \Xi_{\pm}^{\Pi} \Xi_{\mp}^{\Pi} \Xi_{\Xi}^{\Pi}
$$

| #以字典定义转换规则 |
| 2 size mapping= {'XL': 4 |
| 3 L 3 |
| M': 2 1  |
| S':1} |
| 6 |
| #使用map)转换 |
| 3 df['size'1 = df['size'1.map(size mapping) |
| 0 df |

$$
\star\xi\equiv i \pm\mathrm{H H F}_{\circ}
$$

①第 $2$ 行：以字典定义转换规则，key为原值，value为转换后的值。

②第 $8$ 行ma $\mathbf{p} ( \mathbf{)}$ ：以字典转换size字段的每一个值

再以预测某市 $\mathrm{F-}$ 任市长的选举为例，由于人力、时间及经费的限制，不太可能调查所有市民的投票倾向，通常我们只会随机抽样1000份或更多的样本进行调查，这种方式称之为抽样
(Sampling） 相关名词定义如 $\mathrm{T}$ 。

 $\bigoplus$ 分层拍样（Stratfied Sampling) 依母体某些属性的比例，进行相同比例的抽样，希望能充分代表母体的特性，如政党、 年龄、性别比例等。

(1）简单抽样， $\mathcal{M}-$ 个集合中随机抽出 $\boldsymbol{n}$ 个样本。程序代码如下:

## 2-4-2 抽样

 $\bigoplus$ 母体（Population）：全体有投票权的市民。

 $\bigoplus$ 样本 $\mathrm{( S a m p l e )}$ ：被抽中调查的市民。

照例我们看看程序如何撰写，请参02 09。抽样.Ipynb

1 import random
2 import numpy as np
3
4 #1~10 的集合
5 list1 = list(np.arange(1, 10 + 1)) 6
7 #随机抽 $\lfloor\# \; 5$ 个
8 print(random.sample(listl, 5))

执行结果： $\lfloor1$ 6 2 8,5].

$$
\star\Xi\equiv\pm\mathrm{H F}_{\circ}
$$

D第 $8$ 行random.samplet)：自集合中随机抽样。

②每次执行结果均不相同，且每个项目不重复，此种抽样方法称为不放回式拍样（Sampling Without Replacement

(2）放回式抽样（Sampling With Replacement 程序代码如下：

）每次执行结果均不相同，但项目会重复，如 $\b=$ ， $4$ 被重复抽出两次，此种抽样方法称为放回式抽样。

1 import random
2 import numpy as np
3
4 #1~10的集合
5 list1 = list(np.arange(1, 10 + 1)) 6
7#随机抽出 $5 \, \setminus$ 
8 print(random.choices(listl, k=5))

执行结果:[8, 4, 7, 4. 61.

$$
\star\star\equiv\mathrm{i d t h} \ni\pm\it F_{\circ}
$$

D第 $8$ 自集合中随机抽样。 行random.choices():

(3) 库进行拍样。程序代码如下： 以Pandas

| 1 | from sklearn import datasets |
| 2 | import pandas as pd |
| 3 |  |
| 4 | #戴人鸢屋(iris）资料美 |
| 5 | ds= datasets.load iris( |
| 6 |  |
| 7 | #x，V合成一个资料集 |
| 8 | df = pd.DataFrame(data-ds.data, columns=ds.feature names) |
| 9 df['y'= ds.target |
| 10 |
| 11 | #随机抽出5个 |
| 12 | df.sample(5) |

执行结果：如图2.27所示

| sepal le | ngth(cmj | sepal width (cm) | petal length (cm) | petal width (cm) | y |
| --- | --- | --- | --- | --- | --- |
| 102 | 7.1 | 3.0 | 5.9 | 2.1 | 2 |
| 40 | 5.0 | 3.5 | 1.3 | 0.3 | 0 |
| 32 | 5.2 | 4.1 | 1.5 | 0.1 | 0 |
| 48 | 5.3 | 3.7 | 1.5 | 0.2 | 0 |
| 77 | 6.7 | 3.0 | 5.0 | 1.7 | 1 |

图2.27以Pandas 库进行拍样执行结果

$$
\star\equiv\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\circ
$$

)第12行di.sample （5）：自集合中随机抽样 $5 \overbrace{\Gamma}$ 。

2此抽样为不放回式抽样

(4）以Pandas 库进行分层抽样。程序代码如下：

![](figures/98-6-FIGURE.jpg)

## 执行结果如下：

| [38 | 115 | 136 | 80 | 111 | 94 | 1 | 0 | 48 | 108 | 104 | 8 | 51 | 131 | 78 | 9 | 142 |
| 112 | 11 | 126 | 79 | 95 | 2 | 46 | 128 | 125 | 55 | 10 | 72 | 145 | 130 | 56 | 138 | 96 |
| 88 | 19 | 7 | 43 | 4 | 82 | 32 | 91 | 127 | 133 | 73 | 85 | 62 | 129 | 42 | 57 | 84 |
| 40 | 105 | 49 | 75 | 113 | 147 | 99 | 27 | 6 | 58 | 35 | 26 | 124 | 92 | 70 | 69 | 139 |
| 66 | 101 | 74 | 60 | 110 | 15 | 39 | 59 | 3 | 89 | 107 | 61 | 143 | 118 | 86 | 71 | 98 |
| 50 | 41 | 34 | 12 | 149 | 77 | 23 | 21 | 117 | 97 | 54 | 119 | 64 | 120 | 45 | 81 | 141 |
| 122 | 114 | 20 | 144 | 134 | 132 | 17 | 24 | 13 | 44 | 123 | 31 | 116 | 76 | 18 | 47 | 137 |
| 63 | 83 | 29 | 25 | 36 | 102 | 28 | 37 | 33 | 148 | 14 | 146 | 16 | 103 | 68 | 90 | 140] |
| 抽出的索引值： |
| [52 |  | 30 | 109 | 106 | 671 |  |  |  |  |  |  |  |  |  |  |  |

$$
\star\xi\equiv i \pm\mathrm{H H F}_{\circ}
$$

D第13行StratliecdShufleSplit (test size-b) 将集合重新洗牌，并从中随机抽取 $6$ 个数据。

第14staie.spi dif,df $\mathbf{y}^{\i} \brack)$ 以 $\mathbf{y}$ 字段为key，分层抽样，得到的结果是generator数据类型，需转为ist，才能次取 $\uplus$ .

3第 $\underline{{\quad}}$ 个输出是重新洗牌的全部数据，第二个是抽出的索引值。

④利用di.locx[O1]指令，取得数据如图2.28所示。观察 $\mathbf{y}$ 列，每个类别各有两个与母体比例相同

| sepal lengt | th (cm) | sepal width (cm) | petal length (cm) | petal width (cm) ) | y |
| --- | --- | --- | --- | --- | --- |
| 52 | 6.9 | 3.1 | 4.9 | 1.5 | 1 1 |
| 5 | 5.4 | 3.9 | 1.7 | 0.4 | 0 ( |
| 30 | 4.8 | 3.1 | 1.6 | 0.2 | ( 0 |
| 109 | 7.2 | 3.6 | 6.1 | 2.5 | 2 |
| 106 | 4.9 | 2.5 | 4.5 | 1.7 | 2 |
| 67 | 5.8 | 2.7 | 4.1 | 1.0 | 1 |

 $2. 2 8$ di.iloxICII作台E取得的数据冬利用

母体比例可以d[ 『y ]value counts()指令取得数据如下 0/12类别各50个

| 2 | 50 |
| 0 | 50 |

（5）以Pandas 库进行不分层抽样。程序代码如下：

D第13行train test solit (test size-6) 将数据集重新洗牌，并从中随机拍样 $6$ 个为测试数据，其余为训练数据

D取得数据如图2.29斤示。观察 $\textbf{y} \varXi\i\textbf{J}$ ，类别 $0$ 有1个，类别有 $3 \uparrow$ ，类别 $2$ 有 $2 \uparrow$ ，与母体比例不相同。

![](figures/100-2-FIGURE.jpg)

## 执行结果：如图2.29所示

| sepal lengt | th(cm) | sepal width (cm) | petal length (cm) | petal width (cm) | y |
| --- | --- | --- | --- | --- | --- |
| 141 | 6.9 | 3.1 | 5.1 | 2.3 | 2 |
| 147 | 6.5 | 3.0 | 5.2 | 2.0 | 2 |
| 97 | 6.2 | 2.9 | 4.3 | 1.3 | 1 |
| 80 | 5.5 | 2.4 | 3.8 | 1.1 | 1 |
| 22 | 4.6 | 3.6 | 1.0 | 0.2 | 0 |
| 61 | 5.9 | 3.0 | 4.2 | 1.5 | 1 |

图 $2. 2 9$ 不分层抽样执行结果

$$
\star\Xi\equiv\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\circ
$$

通过各种方式搜集到一组样本后，我们会先对样本进行探索通常会先衡量其集中趋势（Central Tendency）及数据离散的程度 (Measures of Variance），这些指标称之为描述统计量
(Descriptive Statistics 较为常见的描述统计量如 $\mathrm{F}$ 。

(2）中位数 $( \mathrm{M e d i a n} )$ ：将所有样本日小排到大，以中间的样本值为准。若样本数为偶数，则取中间样本值的平均数。中位数可以避免离群值（Outlier）的影响。例如，统计平均年收 $\lambda$ ，如果加 $\lambda-$ 位超级富豪，则平均数将大幅提高，但中位数不受影响。假设有一组样本：[100，200，300，400，500]，其平均数一中位数 -300。现在把500 改为50000，则样本为：[100，200，300,
400，50000]，平均数为100200，被50000影响，大幅提升，无法反映大多数数据的类型；而中位数为300，仍然不变。

(3）众数 $( \mathrm{M o d e} )$ ：频率发生最高的数值，以大多数的数据为主

## 2-4-3基础统计

## ，集中趋势

1）平均数（Mean）：母体以x表示，样本以x表示。有

$$
\mu=\frac{\sum_{i=1}^{n} x_{i}} {n}
$$

## 2.。数据离散的程度

（1）级距 $( \mathrm{R a n g e} )$ 级距=最大值-最小值

(2）百分位数（Percentiles）与四分位数（Quariles 例如100为 $7 5$ 百分位数表示有75%的样本 $\imath\mathrm{J} \i\mp1 0 0$ 

可以使用箱形图（Box Plot，或称盒须图）直接显示上述相关的统计量。

(1）以美国历届总统的身高数据，计算各式描述统计量。程序代码如 $\mathrm{T}$ :

(3）变异数（Variance）：母体以6表示，样本以s表示。则有

$$
\begin{array} {l} {\delta={\sqrt{\sum( x-\mu)^{2}}}} \\ {n} \\ {s={\sqrt{\sum( x-\mu)^{2}}}} \\ {n-1} \\ \end{array}
$$

## 3.箱形图

下面进行实践， 10_基础统计ipynb
请参阅02

| #集中趋势(Central Tendency) |
| 2 print(f"平均数={df['height"].mean()}") |
| print(f"中位数-{df['height'].median()}") |
| print(f"众数={df['height'].mode()[0]}") |
| print(O |
|  |
| #数据离敬的程度(Measures of Variance |
| from scipy import stats |
|  |
| print(f"级距(Range)={df['height'].max() - df['height'].min()}") |
| print(f"182cm 百分位数-{stats.percentileofscore(df['height'], 182, 'strict')}") |
| 2 print(f"变异数={df['height'].std():.2f}") |

执行结果如下。

$$
\textcircled{1} \neq\sharp5 \mathbb{J} \sharp\mathbb{J}=1 7 9. 7 3 8 0 9 5 2 3 8 0 9 5 2 4_{\circ}
$$

②中位数-182.0 (2）以美国历届总统的身高数据，计算四分位数。程序代码如下：

3众数 $\boxed{=} 1 8 3$ 。
级距=30。
⑤182cm的百分
$$
\frac{\#} {x}=4 7. 6 1 9 0 4 7 6 1 9 0 4 7 6 1_{\circ}
$$
变异数 $= 7. 0 2_{\circ}$ 

③众数-183.

®变异数 $= 7. 0 2_{\circ}$ 

$$
1 \quad\mathrm{p r i n t} ( f^{*} | \! \! \raise0. 1 5, \ldots\! \! \! \mathrm{f i t h e r a l p h e r t r e s t r e s t r e s t r e s t r e s t r e s t r e s t r e s t r e s t r e s t r e s t r e s t r e s t r e s e s t r e s e s t r e s e s t r e t r e s e s t r e t r e s e s e s t r e t r e s e s e s t r e t r e t r e s e s e s t r e t r e t r e s e s e s t r e t r e t r e s e t r e t r e s t r e t r e t r e t r e s t r e t r e t r e t r e s t r e t r e t r e t r e t r e t r e t r e t r
$$
0.5,0.75,1])}")

执行结果如下

①四分位数
$$
\textcircled{2} 0. 2 5 ~ ~ 1 7 4. 2 5
$$
30.50182.00 00.75 183.00
$$
\mathbb{\widehat{5}} 1. 0 0 ~ ~ 1 9 3. 0 0
$$

1 print(f"{df['height'].describe()}")

$$
1 \mathrm{~ p r i n t} ( f^{n} \{d f [ \mathrm{~ ` h e i g h t ~^{\prime} ] ~. d e s c r i b e ( ) ~} \}^{n} )
$$

执行结果如下。

Ccount 420000 Dmean 179.738095
$$
\begin{array} {c c} {\widehat{3}} \\ \end{array} \mathrm{s t d} ~ ~ \begin{array} {c c} {7. 0 1 5 8 6 9} \\ \end{array}
$$

$$
\mathbb{T} \mathrm{c o u n t} \4 2. 0 0 0 0 0
$$

3是否存在离群值。注意：离群值发生的可能原因包括记录或输入错误、传感器失常造成量测错误或是重要的影响样本
(niluential Sample) 应仔细探究为何发生，不要直接剔除。一 Omin 163:000000 625% 174250000 650% 182.000000
0759% 13000000
Bmax 193.000000

$$
\mathbb{P} 7 5 \% \quad1 8 3. 0 0 0 0 0 0
$$

$$
\mathbb{Q} \mathrm{m a x} ~ \mathrm{~ 1 9 3. 0 0 0 0 0 0 ~}
$$

(3）绘制箱形图，定义如图2.30所示。

![](figures/104-5-FIGURE.jpg)

图 $2$ .30箱形图定 $\mathcal{X}$ 

由箱形图可观察以下特性。

D中间的线：中 $\overrightarrow{\omicron}$ 数

②中间的箱子：Q1~03共50%的数据集中在箱子内般而言，会更进一步收集更多样本，来观察离群值是否再次出现 ，更多的样本也可以稀释离群值的影响力。
另一方面，

除了单变量的统计量，我们也会做多变量的分析，了解变量之间的关联度（Correlation） 在数据探索与分析（Exploratory Data Analysis,EDA）的阶段，我们还会依据数据字段的属性进行不同面向的观察，如图2.32所示。

$$
\pi_{\pm}^{\Pi} \equiv\Gamma_{\pm}^{\Chi} \pm\Pi\Gamma:
$$

![](figures/105-3-FIGURE.jpg)

执行结果：如图2.31所示。

![](figures/105-5-FIGURE.jpg)

图 $2. 3 1$ 执行结果

常用的统计图功能简要说明如 $\mathrm{F}$ ，同时也使用程序实践相关图表。

(1）直方图（Histogram）：观察数据集中趋势、离散的程度、概率分布及偏态（Skewness）/峰态（Kurtosis 如图2.33 所示。

![](figures/106-2-FIGURE.jpg)

图 $2. 3 2$ 数据探索与分析不同面向的观察

![](figures/106-4-FIGURE.jpg)

E $1 2. 3 3$ 直方图

$$
\left. F \right] \equiv\mp[ 5 ] \pm[ \mathrm{J T} : \medskip
$$

(2）饼图（Pie Char 显示单一变量的各类别所占比例， 如图2.34所示。

(3）折线图（Line Charth 观察趋势，尤其是时间的趋势如股价、气象温度、营收情况、运力分析等，如图2.35所示。

![](figures/107-2-FIGURE.jpg)

![](figures/107-3-FIGURE.jpg)

图2.34 饼图

$$
\left\{\Xi\right\} \equiv\left\{\Xi\right\} \pm\left\{\Xi\right\} \pm\Gamma:
$$

![](figures/107-6-FIGURE.jpg)

(4）散点图（Scatter Chart 显示两个变量的关联，亦用于观察是否有离群值，如图2.36所示。

![](figures/108-1-FIGURE.jpg)

图 $2. 3 5$ 折线图

$$
\pi_{\pm}^{\sqcup} \equiv( 5 \mathrm{I J} ) \natural\mathrm{I J T} :
$$

![](figures/108-4-FIGURE.jpg)

![](figures/108-5-FIGURE.jpg)

图2.36散点图

$$
\star\Xi\equiv\langle5 | \rfloor\rangle\downarrow\Gamma:
$$

(5）气泡图（Bubble Chart 散点图再加一个变量，以该变量值作为点的大小，并且做三个维度的分析，如图2.37所示。

![](figures/109-1-FIGURE.jpg)

![](figures/109-2-FIGURE.jpg)

$$
\begin{array} {c c} {\overline{{\S}} ~ 2. 3 7} & {\unit{\equiv i g} ~ \overline{{\S}}} \\ \end{array}
$$

## 程序示例如下：

![](figures/109-5-FIGURE.jpg)

(6）热图（Heatmap）：显示各变量之间的关联度，便于轻易辨识出较高的关联度，如图 2.38所示。输入数据须为各变量之间的关联系数（Correlation Coeficint) 可使用 Pandas的corr)函数计算。公式为

(7）另外，Seaborn 库还提供了许多指令，使用一个指令就可以产生多张的图表，如pairplot（见图2.39所示 facegrid 等，读者可参阅 Seabon官网团

$$
r_{x, y}={\frac{\displaystyle\sum_{i=1}^{n} ( x_{i}-{\overline{{x}}} ) ( y_{i}-{\overline{{y}}} )} {\displaystyle\sqrt{\sum_{i=1}^{n} ( x_{i}-{\overline{{x}}} )^{2} ( y_{i}-{\overline{{y}}} )^{2}}}}
$$

![](figures/110-3-FIGURE.jpg)

图2.38热图

$$
\pi_{\pm}^{\Pi} \equiv\pi_{\pm} ( \bar{y} ) \pm\Pi\Gamma:
$$

![](figures/110-6-FIGURE.jpg)

![](figures/111-0-FIGURE.jpg)

图2.39、指令生成图表

以某市市长选举为例，当我们收集到问卷调查后，接着就可以建立模型来预测谁会当选，预测结果通常是以概率表示各候选人当选的可能性。

(1）实验（Experiment）或称试验（Tial 针对计划目标进行一连串的行动，如掷硬币、掷骰子、抽扑克牌、买彩票等

(2）样本空间（Sample Space）：一个实验会出现的所有可能结果，如掷硬币一次的样本空间为{正面、反面}，掷硬币两次的样本空间为{正正、正反、反正、反反}。

(4）事件 $( \mathrm{E v e n t} )$ 某次实验发生的结果，一个事件可能含多个样本点。例如，抽一张扑克牌出现红色的事件，该事件就包含 26个样本点。

## 2-4-4 概率

## ，概率的定义与定理

接下来我们先了解概率的相关术语（Terminology）及定义

(3）样本点（Sample Point 样本空间内任一可能的结
人
果，如掷硬币两次会有四种样本点，抽一张扑克牌有52个样本点。

(5）概率：某次事件发生的可能性，因此

$$
\left\{\begin{matrix} \pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\\ \pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\right.
$$

例如，掷硬币两次出现两个正面为{正正} $1$ 个样本点，样本空间所有样本点为{正正、正反、反正、反皮}共 $4 \uparrow$ ，故概率等于 1/4。同样地，掷硬币两次出现 $- \mathtt{E}-$ 反的概率 $= \left\{\pm\pi\right.$ 、反正}/{正正、正 $\sqrt{\Im}$ 、反正、反 $[ \overline{{\chi}} ]=1 / 2_{\circ}$ 

日事件独立（Independent) $\boldsymbol{A}$ 事件发生，不会影响B事件出现的概率，则称 $\boldsymbol{A}$ 、B两事件独立。例如，掷硬币两次，第一次出现正面或反面，都不会影响第二次的掷硬币结果。

②事件相依（Dependent 抽一张扑克牌，不放回，再抽第二张，概率会被第一张抽出结果所影响。例如

 $\bigoplus$ 第一张抽到红色的牌，第二张再拍到红色的概率：第一张抽到红色→红色牌剩25张，黑色牌剩26张，则第二张抽到红色概 ※-25/（25+26

 $\bigoplus$ 第一张抽到红色的牌，第二张再抽到黑色的概率：第二张抽到黑色概率 $= 2 6$ /(25+26

事件互斥（Mutually Excusive）：A事件发生，就不会出现 B事件，两事件不可能同时发生。例如，次日天气出现晴天的概率为 $1 / 2$ ，阴天的概率为 $1 / 4$ ，雨天的概率为 $1 / 4$ ，次日出现晴天，就不可能出现阴天，故次口不是雨天的概率为 $1 / 2+1 / 4=3 / 4$ ，即互斥事件的概率等于个别事件的概率直接相加。

(6）条件概率（Condiional Probability）与相依性 (Dependence)

上述两个概率是不相等的，故两次抽牌的事件是相依的 DA、B事件独 $\overrightarrow{\Psi}$ ，则同时发生A及B事件的概率P（AnB -P
$$
( A ) \ x P \ ( B )
$$

：（Set Theory）表 $\overline{{\Pi}}$ ，如冬2.40 以上条件概率可以使用集合论
所示。

有些事件我们会关心发生的顺序，相反地，有些时候则不关心事件发生的顺序，计算的公式会有所不同，结果也不一样，关心事件发生的顺序称为排列（Permutation），反之，不关心事件发生的顺序称为组合（Combination）。例如，掷硬币两次的排列样本空间为{正正、正反、反正、反反}，但组合样本空间为{正正、一正一 $\sqrt{\nabla}$ 、反反}，因为「正反」和「反正」都是 $\Gamma\!-\! \Xi\!-\! \Xi\rfloor$ 。

依照上述定义，衍生的相关定理如下

②A、 ，则发生A或B事件的概率 $\boldsymbol{P}$  $( A \cup B ) \ =P$ 
B事件互斥,
(A) +P(B)

3A、B相依事件，则发生A或B事件的概率 $\boldsymbol{P}$  $( A \cup B ) \ =P$ (A) +P (B) -P (ANB)

0样本空间内所有互斥事件的概率总和等于1

![](figures/114-7-FIGURE.jpg)

图2.40交集（Intersection）及并集（Union

## 2.。排列与组合以下列举各种案例说明排列与组合的相关计算 $\big<$ 式及程序撰 $\varXi$ ，请参阅02 11_概.pynb

范例 ${\bf1}$ . 标号各为1、2、3，它们排列的事件共有
有三颗球，
几种？

$$
\pi_{\pm}^{\Pi} \equiv\Gamma_{\pm}^{\pm} \Xi_{\pm} \pi_{\mathrm{J T}} \Gamma:
$$

| 1 | from itertools import permutations |
| 2 |  |
| 3 | #测试数据 |
| 4 | list1=[1,2, 3] |
| 5 |  |
| 6 | #声明排列的类别 |
| 7 | perm = permutations(list1) |
| 8 |  |
| 9 | #列出所有事件 |
| 10 | print('所有事件：'） |
| 11 | 联 list output= list(perm) |
| 12 | 一 for i in list output:  |
| 13 print(i) |
| 14 |
| 15 | print() |
| 16 | print(f"排列数={len(list output)}') |

执行结果：所有事件如下

$$
( 1, 2, 3 )
$$
$$
( 1, 3, 2 )
$$
$$
( 2, 1, 3 )
$$
$$
( 2, 3, 1 )
$$
$$
( 3, 1, 2 )
$$
$$
( 3, 2, 1 )
$$

(2, 3.1

$$
\left. \mathrm{H e F I J} \right\} \pm X=6
$$

## 范例2。有三颗球，抽出两颗球排列的事件共有几种？

$$
\star\equiv1 \pm\mp\pm\pm\pm\pm\pm\pm
$$

| 1 | from itertools import permutations |
| 2 |  |
| 3 | #测试数据 |
| 4 | list1= [1,2,3] |
| 5 |  |
| 6 | #声明排列的类别，三个球抽出两个球 |
| 7 | perm = permutations(listl, 2) |
| 8 | U |
| 9 | #列出所有事件 |
| 10 | print("所有事件：'） |
| 11 | list output = list(perm) |
| 12 | 一 for i in list output:  |
| 13 print(i) |
| 14 |
| 15 | print() |
| 16 | print(f'排列数={len(list output)}') |

(1）执行结果：所有事件如下

$$
( 1, 2 )
$$
$$
( 1, 3 )
$$
$$
( 2, 1 )
$$
$$
( 2, 3 )
$$
$$
( 3, 1 )
$$
$$
( 3, 2 )
$$

$$
\pm\vert\mp\Sigma\vert\rfloor\pm\bar{X}=6
$$

（2）出现 $( 1, 2 )$ 的概率- $( 1, 2 )$ (2,1)]/6=2/6-1/3。

范例 $\textbf{3}$ .有三颗球，抽出两颗球组合的事件共有几种？

范例 $\boldsymbol{4}$ 、再进一步深化，袋子中有10颗不同颜色的球，抽出 $\textbf{3}$ 颗球共有几种排列方式？

## 程序代码如下：

| 1 | from itertools import combinations |
| 2 |  |
| 3 | #测试数据 |
| 4 | list1=[1,2,31 |
| 5 |  |
| 6 | #声明排列的类别，三个球抽出两个球 |
| 7 | comb = combinations(list1, 2) |
| 8 |  |
| 9 | #列出所有事件 |
| 10 | print('所有事件："） |
| 11 | list output = list(comb) |
| 12 | for i in list output: |
| 13 print(i) |
| 14 |  |
| 15 | print() |
| 16 | print(f'组合数={len(list output)}') |

）执行结果：所有事件如下。 (1)

(1,2) (1,

(1,3)

(2,3)

$$
\begin{matrix} \pm\pm\pm x=3 \\ \pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\cdot\end{matrix}
$$

(2）出现 $( 1, 2 )$ 的概率 $\stackrel{\circ} {=} [ \; \; ( 1, \, 2 ) \; \; ] / 3=1 / 3_{\circ}$ 

解题：排列计算 $\big<$ 式为

$$
{\sharp} \left[ \pm\pi\right]=\frac{n!} {( n-k )!}
$$

式中： $\boldsymbol{n}$ 为样本点数10:

$$
k \grave{\pi} \it3_{\circ}
$$

(2）公式白来：抽第一次有 $1 0$ 种选择，拍第二次有 $9$ 种选择，拍第三次有 $8$ 种选择，故

范例 ${\bf5}$ .掷硬 $\mathbf{n} \Xi i \pi$ ，出现 ${\bf0},$ 1、 $2 \ \_$ 3
3次正面的组合共有几种？

解题：因为只考虑出现正面的次数，不管出现的顺序，故属于组合的问题，因为出现的顺序不同，仍视为同一事件，故要除以 k! 公式为

## 程序代码如下：

| 1 | import math |
| 2 |  |
| 3 | n=10 |
| 4 | k=3 |
| 5 |  |
| 6 #math.factorial:阶乘 |
| 7 | perm = math.factorial(n) / math.factorial(n-k) |
| 8 | 厂 |
| 9 | print(f'排列方式共{perm:.0f}种’） |

(1）执行结果：排列方式共720种

$$
1 0 \times9 \times8=\frac{1 0!} {( 1 0-3 )!}
$$

$$
\b Z \dag\pi=\frac{n!} {k! ( n-k )!}
$$

式中n为实验次数， $4$ 次

k为出现正面次数，0、1、2、3次。

$$
\pi\equiv\Gamma\pm\pi\pm\pm\pm\Gamma\Gamma:
$$

掷硬币、掷骰子、拍扑克牌 $- \lambda\pi$ ，均假设每个单一样本点发生的概率都均等，如果不是均等，则上述的排列/组合的公式需要再考虑概率 $\boldsymbol{p}$ ，如郑硬币出现正面的概率 $= 0. 4$ ，p*代表出现k次正面的概率， $( 1 \!-\! p )$ （n的代表出现n-K次反面的概率。

范例 $\mathbf{6}$ .掷硬币出现正面的概率为0.4，掷硬币三次，出现 $\mathbf{0} \ 、$  $2 \ \_$ 3次正面的概率为何?
1、

| 1 | import math |
| 2 |  |
| 3 | n=3 |
| 4 |  |
| 5 | # math.factorial:阶乘 |
| 福 6 | fork in range(4): |
| 7 | #组合公式 |
| 8 | comb = math.factorial(n) / (math.factorial(k) 本 math.factorial(n-k)) |
| 9 | print(f'出现{k}次正面的组合共{comb:.0f}种'） |

## 执行结果：

出现0次正面的组合共 $1$ 种
出现 $1$ 次正面的组合共 $3$ 种
出现 $2$ 次正面的组合共 $3$ 种
出现 $3$ 次正面的组合共 $1$ 种

出现 $3$ 次正面的组合共1种

## 3.二项分布

## 程序代码如下：

| 1 | import math |
| 2 | 2 |
| 3 | 3 n=3 |
| 4 | 1 p=0.4 |
| 5 | 6 |
| 6 | #math.factorial:阶乘 |
| 7 | fork in range(4): |
| 8 | 2 #组台公式 |
| 9 | 1 comb = math.factorial(n) / (math.factorial(k) )* math.factorial(n-k)) |
| 10 | prob = comb * (p**k) * ((1-p)**(n-k)) |
| 11 | print(f·出现{k}次正面的概率={prob:.4f}'） |

(2）使用SoiPy comb 组合函数，执行结果同上。程序代码如下：

(3）直接使用SciPy的统计模块binom.pmf $( \mathrm{k, n, p} )$ 执行结果同 $\b L$ 。程序代码如下：

基本玩法：从39个号码中选择5个号码。各奖项的中奖方式 $\Pi$ 表 $2. 1$ 。

(1）执行结果如下
出现 $\mathbf{0}$ 次正面的概率-0.2160
出现 $1$ 次正面的概率-0.4320
出现 $2$ 次正面的概率-0.2880
出现 $3$ 次正面的概率-0.0640

(1）执行结果如 $\mathrm{F}$ 

$$
\mathrm{T :}
$$

| 1 | #使用SciPy comb 组台函数 |
| 2 | from scipy import special as sps |
| 3 |  |
| 4 | n=3 |
| 5 | p=0.4 |
| 6 |
| 7 | fork in range(4): |
| 8 | prob = sps.comb(n, k) * (p**k) * ((1-p)**(n-k)) |
| 9 | print(f'出现{k}次正面的概率={prob:.4f}'） |
| 10 |  |

| 1 | #使用SciPy binom 项分布函数 |
| 2 | # binom = comb * (p**k) *((1-p)**(n-k)) |
| 3 | from scipy.stats import binom |
| 4 |  |
| 7 5 | n=3 |
| - 6 | =0.4 |
| 7 | 广 |
| 8 | fork in range(4): |
| 9 | print(f'出现{k}次正面的概率={binom.pmf(k,n,p):.4f}'） |

## 范例7。试算今彩539平均回报率

(1）假定每个号码出现的概率是相等的 $( 1 / 3 9 )$ 固定选 $5 \uparrow$ 号码，故计算概率时不需考虑 $p_{\circ}$ 

## 表2.1，中奖方式

| 奖项 | 中奖方式 | 中奖方式图示 |
| --- | --- | --- |
| 头等奖 | 与当期五个中奖号码完全相同者 | 00006 |
| 贰等奖 | 对中当期奖号之其中任四码 | 9000 |
| 叁等奖 | 对中当期奖号之其中任三码 | 900 |
| 肆等奖 | 对中当期奖号之其中任二码 | ®● |

## 各奖项金额见表2.2

## 表2.2 各奖项金额

| 奖项 | 头等奖 | 贰等奖 | 叁等奖 | 肆等奖 |
| --- | --- | --- | --- | --- |
| 单注奖金 | $8000000 | $20000 | $300 | $50 |

## 解题：

(2）计算从39个号码选 $5 \uparrow\b\# \b\#$ ，总共有几种组台

## 程序代码如下：

注意：排列与组合的理论看似简单，然而实践上的应用千变万化，建议读者多找一些案例实践，才能运用自如

执行结果如下：

从39个号码选 $5 \uparrow\Xi\mp\Xi$ ，总共有575757种

头等奖号码的个数：1

贰等奖号码的个数：170

叁等奖号码的个数：5610

肆等奖号码的个数：59840

(3）计算平均中奖金额。程序代码如下

执行结果如下：

平均中奖金额： $2 7$ .92

今彩539平均回报率-44.16%

概率分布（Distribution）结合了描述统计量及概率的概念，对数据做进一步的分析，希望推测母体是呈现何种形状的分布，如正态分布、均匀分布、泊松（Poisson）分布或二项分布等，并且依据样本推估概率分布相关的母数，如平均数、变异数等。有了完整概率分布信息后，就可以进行预测、区间估计、假设检定等。

概率分布的种类非常多，如图2.41所示。这里我们仅介绍几种常见的概率分布。

## 2-4-5 概率分布

(3）概率质量函数（Probabilty Mass Function, PMF) 如果是离散型的概率分布，则PDF改称为PMF

![](figures/124-1-FIGURE.jpg)

图2.41，各种概率分布及其关联叫

首先介经 $\mathit{\Pi}$ 个专有名词。

）概率密度函数：发生各种事件的概率 (1)

(2）累积分布函数（Cumulative Distribution Function CDF）：等于或低于某一观察值的概率。

因为正态分布（Normal Distribution）是由 Carl Friedrich
Gauss 提出的，因此又称为高斯分布（Gauss Distribution），世上大部分的事件都属于正态分布，如考试的成绩，考低分 $\mathcal{R}$ 高分的学生人数会比较少，中等分数的人数会占大部分。再如，全年的温度、业务员的业绩、一堆红豆的重量等。正态分布的概率密度函数定 $\grave{\chi}$ 为

我们在「2-3-5积分」节已经介绍过相关内容，并撰写 $7-\Xi$ 程序，这里直接使用SciPy的统计模块做一些实操。

接下来看几个常见的概率分布。

## 1，正态分布

$$
f ( x ; \mu, \sigma) \!=\! \frac{1} {\sqrt{2 \pi\! \cdot\! \sigma^{2}}} \bullet e^{-\frac{1} {2} \cdot( \frac{x-\mu} {\sigma} )^{2}}
$$

两 $\uparrow$ 重要参数介绍如下。

(1） $\boldsymbol{\mu}$ ：平均数，是全体样本的平均数

(2）8：标准差，描述全体样本的离散的程度

概率密度函数可简写成N $( \mu, \delta)$ 。

>以下程序请参考02 12正态分布Jpynb

范例1，使用ScP $\mathbf{Y}$ 绘制概率密度函数（PDP O

$$
\pi\equiv\Gamma\pm\pi\pm\pm\pm\Gamma\Gamma:
$$

范例2。绘制正态分布的±1、±2、13倍标准差区间及其概率密度分布

![](figures/126-1-FIGURE.jpg)

：如图2.42所示执行结果：

![](figures/126-3-FIGURE.jpg)

图 $2$ .42 概率密度函数

$$
\pi\equiv\pi\pm\pi\pm\pi\pm\Gamma:
$$

(2）将第 $5$ 勺 $(-2, \, 2 ) \, \, \, \,, \, \, \, \, \, \, \, (-3, \, 3 )$ ，结果如图2.44所
行改为
示。

![](figures/127-1-FIGURE.jpg)

(1）执行结果：如图2.43所示

![](figures/127-3-FIGURE.jpg)

图2.43、正态分布±1倍标准差区间及概率密度分布

![](figures/127-5-FIGURE.jpg)

有一个Student's $\mathbf{t}$ 分布，很像正态分布，常被用来做假设检定 (Hypothesis Test SciPy 也有资源

图2.44正态分布±2、±3倍标准差区间及概率密度分布

范例 $\textbf{3}$ .使用SoPy绘制累积分布函数（CDF O

$$
\star\equiv1 \pm\mp\pm\pm\pm\pm\pm\pm
$$

![](figures/128-4-FIGURE.jpg)

第12行从PDF改为CDF.

：如图2.45所示执行结果：

![](figures/128-7-FIGURE.jpg)

图2.45、累积分布函数

范例4.使用 SciPy绘制 Student'st分布的概率密度函数 (PDF），并与正态分布比较。

均匀分布也是一种常见的分布，通常是属于离散型的数据，所有样本点发生的概率都相同，如掷硬币、掷骰子，出现每一面的概率都一样。均匀分布的概率密度函数为

![](figures/129-1-FIGURE.jpg)

## 第13行为t为 Student'st分布

：如图2.46所示执行结果：

![](figures/129-4-FIGURE.jpg)

图2.46 Sudent'st分布的概率密度函数

图2.46中，较粗的线（红色） $\mathbf{t}$ 为 Student's $\mathbf{t}$ 分布

## 2。均匀分布

$$
f ( x ) \!=\! \frac{1} {( b-a )}, a \! \leqslant\! x \! \leqslant\! b
$$

13其他分布.ipynb >以下程序请参考02

范例 ${\bf5}$ 。绘制掷骰子的概率密度函数

$$
\star\equiv\pm\pm\mp\pm\pm\pm\pm\pm\pm
$$

![](figures/130-3-FIGURE.jpg)

执行结呆：如图2.47所示 (1)

![](figures/130-5-FIGURE.jpg)

图2.47范例 $5$ 执行结果

(2）可以使用随机数模拟，程序代码为np.random.randint (1,6)

## 3.二项分布顾名思 $\grave{\chi}$ ，二项分布就是只有两种观察值，如成功/失败、 $\mathbb{T}$ 面/反面、有/没有等，类似的分布有以下三种。

(1）伯努利（Bernoulli）分布：做一次二分类的实验，概率密度函数为

(2）二项（Binomial）分布：做多次二分类的实验。概率密度函数为

(3）多项（Multinomial）分布：做多次多分类的实验。概率密度函数为

$$
f \left( \textbf{x} \right) \ =p^{x} \left( 1-p \right) \sp{1-x}, \textbf{x}=0 \implies\textbf{b x} 1
$$

式一：p为成功的概率

$$
f ( k, n, p ) \!=\! \! \left( \! \! \begin{array} {c} {n} \\ {k} \\ \end{array} \! \! \right) \! p^{k} \left( 1-p \right)^{n-k}
$$

式中： $\boldsymbol{n}$ 为实验次数：k为成功次数：p为成功的概率

$$
f ( x_{1}, x_{2}, \cdots, x_{k}, n, p_{1}, p_{2}, \cdots, p_{k} )={\frac{n!} {x_{1}! x_{2}! \cdots x_{k} \,!}} \, p_{1}^{\, x_{1}} \, p_{2}^{\, x_{2}} \cdots p_{k}^{\, x_{k}}, \sum x_{i}=n
$$

范例6。绘制掷硬币的概率密度函数

$$
\star\Xi\equiv\lfloor\pm\Xi\rfloor\not\supset\Gamma:
$$

![](figures/132-0-FIGURE.jpg)

执行结果：如图2.48所示

![](figures/132-2-FIGURE.jpg)

图2.48伯努利分布执行结果

## 范例7、绘制掷硬币 10次的概率密度函数

$$
\star\Xi\equiv\lfloor\pm\Xi\rfloor\not\supset\Gamma:
$$

![](figures/132-6-FIGURE.jpg)

(2）若掷郑骰子60次，1~6点各出现10次的概率，执行结果为 4.006789054168284e-06。概率会小很多，这是由于掷骰子60次比现的样本空间会有更多的组合。程序代码如下：

执行结果：如图2.49所示。

![](figures/133-2-FIGURE.jpg)

图2.49二项分布执行结果

## 范例8。绘制掷骰子10次的概率

$$
\star\equiv\lfloor\pm\bigstar\exists\pm\rfloor\rceil\subset\Gamma:
$$

![](figures/133-6-FIGURE.jpg)

$$
( 1 ) \, \left( \sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\sharp\ 4 \right) \ \frac{1} {2} \lineup\frac{1} {2} \line( 1 ) \Ad^{\prime} \sharp\sharp\sharp\sharp\sharp\sharp\line( 1 ) \line( 1 ) \line( 1 ) \line( 1 ) \line( 1 ) \line( 1 ) \line( 1 ) \line( 1 ) \line( 1 ) \line( 1 ) \line( 1 ) \line( 1 ) \line( 1 ) \line( 1 ) \line( 1 ) \f^{\prime} \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \2 \end\end\end\end\end\end\end\end\end\end\end\end\end\end\end\end\end\end\end\end\end\end\end\end\end\end\end\end\end\end\end\end\ 7 7 7 7 7 7
$$

(3）若掷骰子1000次，给定 $1 {\sim} 6$ 点各出现的次数。程序代码如下：

泊松分布经常运用在「给定的时间内发生K次事件」的概率分布函数，我们得以建立一套顾客服务的模型，用来估计一个服务柜台的等候人数，进而计算出需要安排几个人服务柜台，来达成特定的服务水平 S $\mathrm{L A} )$ ，其应用场景非常
(Service Level Agreement,
 $\varGamma$ ，如售票柜台、便利商店结账柜台、客服中心的电话线数、依个人发生车祸的次数决定车险定价等。

![](figures/134-2-FIGURE.jpg)

执行结果：如图2.50所示

![](figures/134-4-FIGURE.jpg)

图 $2. 5 0$ 执行结呆

## 4。泊松分布泊松分布的概率密度函数为

$$
f ( x, \lambda)=\frac{\lambda^{k_{\mathrm{e}}-\lambda}} {k!}
$$

式中：2为平均发生次数； $\mathbf{e}$ 为自然对数；k为发生次数

## 范例9，绘制各种X值的泊松分布概率密度函数

$$
\star\Xi\equiv\lfloor\pm\Xi\rfloor\not\supset\Gamma:
$$

![](figures/135-5-FIGURE.jpg)

执行结果：如图2.51所示

![](figures/135-7-FIGURE.jpg)

图 $2. 5 1$ 泊松分布执行结果

根据维基百科的定 $\mathcal{X}^{[ 5 ]}$ ，假设检定是推论统计中用于检定统计假设的一种方法。而统计假设可以通过观察一组随机变量的模型进行检定。通常判定一种新药是否有效，我们并不希望因为随机抽样的误差，而造成判定错误，所以，治愈人数的比例必须超过某一显著水平，才能够认定为具有医疗效果。在谈到假设检定之前，我们先来了解什么是置信区间（Confidence Interval)

之前我们谈到平均数、中位数都是以单一值表达样本的集中趋势，这称为点估计，不过，这种表达方式并不精确。以正态分布而 $\overrightarrow{\mathbf{\Xi}}$ ，如图2.52所示的概率密度函数，样本刚好等于平均数的概率也只有0.4，因此，我们改以区间估计会是比较稳健的做法。例如， 估计95%的样本会落在特定区间内，这个区间我们便称为置信区间 (Confidence Interval)

## 2-4-6假设检定

## 1。置信区间

![](figures/136-4-FIGURE.jpg)

图 $2. 5 2$ 置信区间示例

以正态分布而言，1倍的标准差置信水平约为68.3 $\not\supset$ ， $2$ 倍的标准差置信水平约为95.4%，3倍的标准差置信水平约为99. $7 \mathcal{Z}_{o}$ ，如 2.53所示。

以业界常用的95 $\not\supset$ 置信水平为例，就是「我们确信95%的样本会落在 $(-1. 9 6 \delta,$ 1.966）区间内」 一般医药有效性的检定也是以此概念表达的，如疫苗等。

范例1.以美国历届总统的身高数据，计算各式描述统计量及 95%的置信区间，

![](figures/137-3-FIGURE.jpg)

图2.53、正态分布的置信水平

>以下程序请参考0214置信区间.ipynb

$$
\star\equiv\lfloor\pm\lfloor\exists\pm\rfloor\rceil\rceil\Gamma:
$$

(1）执行结果：平均数 $= 1 7 9. 7 4$ ，标准差 $= 7. 0 2$ ，置信区间：
$$
( 1 6 5. 7 1, 1 9 3. 7 7 )
$$

(2）如上所述，如果单纯以平均数179.74说明美国总统的身高并不完整，因为其中有多位总统的身高低于170cm，因此若再加 $\pm$ 「95 $\not\supset$ 美国总统的身高介于（165.71,193.77）」，则会让信息更加完整。

范例 $2$ .利用随机数生成正态分布的样本，再使用 SciPy的统计模块（stats）计算置信区间

(1）利用随机数生成10000个样本，样本来自正态分布N $( 5 )$  $2 )$ 即平均数为 $5$ ，标准差为 $2$ ，使用norm.interval)计算置信区间，参数分别为置信水平、平均数、标准差。程序代码如下：

执行结果：置信区间-（1.0487, 8.9326），约为 $( \; \mu-1. 9 6 \delta,$ 
$$
\mu+1. 9 6 \delta)
$$

## 解题:

![](figures/138-6-FIGURE.jpg)

(2) 。程序代码如下： 绘图。

范例 $\textbf{3}$ .利用随机数生成二项分布的样本，再使用SciPy的统计模块（stats）计算置信区间

![](figures/139-1-FIGURE.jpg)

执行结果：如图2.54所示

![](figures/139-3-FIGURE.jpg)

图2.54 95 $\not\supset$ 置信区间执行结果

$$
\pi\equiv\Gamma\pm\pi\pm\pm\pm\Gamma\Gamma:
$$

(1）执行结果：平均数 $= 0. 5 0 3 9$ ，标准差
-0.4999847897686489， 置信区间- （4941.0,5137.0) 约为（u-
 $\mu+1. 9 6 \delta)$ 
1.960,

(2）二项分布标准差公司 $\dot{\mathrm{t}}=\sqrt{p ( 1-p )}$ 可以验算 $\( p^{\star} ~ \mid1-$  $o ) ~ ~ ) ~ ~^{\star\star}. 5=0. 5,$ 。随机数的样本与理论值相差不远。

中心极限定理（Central Linit Theorem, CLT）是指当样本数越大时，不管任何概率分布，每批样本的平均数之概率分布会近似于正态分布。因此，我们就可以使用正态分布估计任何样本平均数之概率分布的置信区间或做假设检定。

然而，因为每一个样本是一批的数据，假设个数为 $\boldsymbol{n}$ 。则每批样本的平均数所形成的标准差不是原样本标准差，而是

例如，我们有1000 ，每批的样本有 $1 0$ 个数据，则每
批的样本
批样本的平均数所形成的标准差为

![](figures/140-5-FIGURE.jpg)

## 2.中心极限定理

$$
\delta_{x}=\frac{\delta} {\sqrt{n}}
$$

又如，样本来自二项分布，标准差 $= \sqrt{p ( 1-p )}$ ，则每批样本的平均数所形成的标准差为

平均数的标准差称为标准误差（Standard Error 它有别于原样本的标准差

范例4。利用随机数生成二项分布的10000批样本，每批含 100个数据，请计算平均数、标准误差，并绘图验证是否近似于 $\mathtt{I E}$ 态分布。

(1）执行结果：平均数=0.5003，标准差 $= 0. 0 4 9 8 2,$ 、置信区间 (4906.0, 5102.0) 如图2.55所示。

$$
\delta_{x}=\frac{\delta} {\sqrt{1 0}}
$$

$$
\delta_{x}=\frac{\sqrt{p ( 1-p )}} {\sqrt{n}}=\frac{\sqrt{p ( 1-p )}} {n}
$$

> 15_中心极限定理.ipynb 以下程序请参考02 15

$$
\star\equiv\lfloor\pm\lfloor\exists\pm\rfloor\rceil\rceil\subset\Gamma:
$$

![](figures/141-8-FIGURE.jpg)

我们设定特定的显著水平（Significance Level），以α表 $\overline{{\Pi}}$ , 例如 $5 \mathcal{\%}$ ，或者相对的置信水平95%，如果样本的平均数落在置信区间之外，我们就说新药是显著有效，这样的判定只有5%概率是错误的，这种检定方法就称为假设检定（Hypothesis Testing）。当然，我们也可以用较严谨的显著水平 $( 0. 3 7_{o} )$ ，即三倍标准差，检定新药是否显著有效。

 假设检定依检定范围可分为单边（One-side）、双边（Two-side) 依样本的设计可分为单样本（Single-sample) 双样本 (Two-sample) 依检定的统计量，可分为平均数检定或标准差的检定。

![](figures/142-2-FIGURE.jpg)

图2.55中心极限定理执行结果

(2）平均数与原样本的平均数几乎相同（0.5003-0.5) O

(3）标准差与理论值也很接近
$$
( 0. 0 4 9 8 2 \!=\! 0. 0 5 )
$$
理论值=
$$
\mathrm{( p^{\star} ~ \left( 1-p \right) ~ / \ t r i a l s ) ~ \ T^{\star\star}. 5=0. 0 4 9}
$$
982

$$
( 4 ) ~ \boxplus\mp\fbox{g l g i g n} \pm\emptyset\pm\pm\emptyset\oplus\emptyset
$$

## 3.假设检定单边检定只关心概率分布的一边，例如， $\mathbf{A}$ 班的成绩是否优于 $\mathbf{B}$ 班 $( \mu_{\mathrm{A}} \! > \! \mu_{\mathrm{B}} )$ ，为右尾检定（Right-tail Test），病毒核酸检测 (PCR），ct值30以下即视为确诊 $( \mu\! < \! 3 0 )$ ，为左尾检定（Left-tail Test）；双边检定则关心两边，如 $\mathbf{A}$ 班的成绩是否与 $\mathbf{B}$ 班有显著差异 $( \mu_{\mathrm{A}} \! \ast\! \mu_{\mathrm{B}} )$ 

单样本检定只有一份样本，如拍样一群顾客，调查是否喜欢公司特定的产品；双样本则有两份样本互相作比较，如 $\mathbf{A}$ 班的成绩是否优于 $\mathbf{B}$ 班，或者新药有效性测试，通常会将实验对象分为两组， 一组为实验组（Treatment Group），让他们服用新药，另一组为对照组，又称控制组（Control Group），让他们服用安慰剂，这种设计又称为 $\mathrm{A} /$ B Tes.

检定时会有两个假设，原假设（NulHypohesis）及备择假设 (Alternative Hypothesis

备择假设是原假设反面的条件，通常备择假设是我们希望的结果，因此，检定后，如果原假设为真时，我们会使用不能拒绝（iai to reject）原假设，而不会说原假设成立。

特朗普的身高190cm，是否与历届的美国总统平均身
范例5.
高有显著的不同？假设历届的美国总统身高为正态分布，以显著水

$$
( 1 ) \textit{\#} \{\mathrm{\#} \} \exists\textit{\mathrm{i x}} \mathrm{H}_{0} \colon\textit{\mu}=0 \circ
$$

(2）备择假设 $\mathrm{H}_{1} \colon\mu\mathrm{=} 0$ ，H，也可以写成上
1

以下就以实例说明各式的检定

> 16_假设检定.ipynb 以下程序请参考02

解题：显著水平5%时，置信区间为
$$
( \mu-1. 9 6 \delta, \mu+1. 9 6 \delta)
$$
程
序代码如 $\mathrm{T}$ :

(1）执行结果：平均数 $= 1 7 9. 7 4$ ，标准差 $= 7. 0 2$ ，置信区间
$$
( 1 6 5. 9 9, 1 9 3. 4 9 )
$$
特朗普的身高190cm 在置信区间内，表示特朗
普身高并不显著比历届的美国总统高。

(2）从图2.56可以看 $\mathtt{H}$ ，特朗普的身高190cm 在左右两条虚线（置信区间〉之间

$$
\mp5 \% \pm\Delta\pm\Xi_{\circ}
$$

![](figures/144-4-FIGURE.jpg)

![](figures/144-5-FIGURE.jpg)

范例 $\mathbf{6}$ .要调查顾客是否喜欢 $\natural$ ， $\mathbf{z} \gRnewRGsuit$ 司进行问
司新上市的食品，
卷调查，取得客户的评价，范围介于0~10分，已知母体平均分数为 ${\bf5}$ 分，标准差为 $2 \sharp\! \! \! \Im$ ，请使用假设检定确认顾客是否喜欢 $\mathbf{Z} \grave{\aleph}$ 司新上市的食品。

，我们会使用检定，当样本大于30时，会近似于检验平均数，
正态分布。统计量为

图2.56特朗普身高的假设检验结果

$$
\mathfrak{H}_{+, \mathfrak{F}}^{\mathtt{H H}} :
$$

$$
\mathrm{\ --( F_{X}^{n} i \hbar_{X}^{\ n} \hbar_{0} \colon\ m u s 5}
$$

备择假设H：/>5

$$
t=\frac{x-\mu} {s / \sqrt{n}}
$$

1）使用随机数模拟问卷调查结果。程序代码如下

![](figures/145-8-FIGURE.jpg)

执行结果：最小值：1，最大值： $9$ ，平均数： $5. 4 6$ a

 $( 2 )$ 使开 stats.test samp()做假设检定，参数
alternative-'greater 『为右尾检定，SciPy需 $\mathrm{V 1}$ .6.0 以上才支持

①执行结果：统计量 $2. 0 2 5 0$ ，p值0.024。90%的置信区间约在 $( \mu\!-\! 1$ .6458, 1+1.6456）之间，因1统计量（2.0250）>1.645，故备择假设为真，确认顾客喜欢公司新上市的食品。使用t统计量比较，需考虑单/双尾检定，并比较不同的值，有点麻烦，专家会改用 $\boldsymbol{p}$ 值与显著水平比较,若 $\imath\rfloor\omicron$ 于显著水平，则备择假设为真，反 $\grave{\varUpsilon}$ , 不能拒绝原假设，因为 $\boldsymbol{p} \boldsymbol{1}$ 直的计算 $\parallel$ 式-1-CDF(累积分布函
数），如图2.57所示。

2图2.57实线为「母体平均数 $^\dagger$ t统计量x母体标准差」，旁边虚线为置信区间的右界，前者大于后者，表示效果显著，有明显差异。

alteraive参数。程序代码如下：

![](figures/146-3-FIGURE.jpg)

![](figures/146-4-FIGURE.jpg)

图2.57问卷调查结果的假设检验结果

马若使用双尾检定，也是叫 stas.tst samp()，删除参数 alternative 即可

范例 $\mathbf{7}$ 。要检定新药有效性，将实验对象分为两组，一组为实 、让他们服用新药，另一组为对照组，又称控制组，让他们服验组，
，检验两组疾病复原状况是否有明显差异。
用安慰剂,

(2）使用stats.itest ind()作假设检定，参数放入两组数据及 alternative-'greater 『表示右尾检定。程序代码如下：

3若使用左尾检定，也是叫 stats.ttest 1sam $\mathbf{p} ( \mathbf{)}$ ，参数 alternative-less $\Gamma$ 

$$
\mathrm{H+s=\overline{{n}} :}
$$

$$
\mathrm{F H \# \# i \hbar\hbar^{\ell} H_{0} \colon\mu_{1}=\mu_{2}}
$$

备择假设H：/+

1）使用随机数模拟复原状况。程序代码如下

| 1 | import numpy as np |
| 2 | import matplotlib.pyplot as plt |
| 3 | from scipy import stats |
| 4 |  |
| 5 5 | np.random.seed(123) |
| 6 | Control Group = np.random.normal(66.0, 1.5, 100) |
| 7 | 一 Treatment Group b=np.random.normal(66.55,1.5,200) |
| 8 | print(f"控制组平均数:{Control Group.mean():.2f}") |
| 9 | print(f"实验组平均数:{Treatment Group.mean():.2f}") |

执行结果：控制组平均数：66.04，实验组平均数：66.46

D执行结果：统计量2.2390，p值0.0129，p值若 $\imath\rfloor\omicron$ 于显著水平（0.05） 则备择假设为真，表示新药有显著疗效，如图2.58所示。

②图2.58实线为「母体平均数 $^\dagger$ 统计量x母体标准差」，旁边虚线为置信区间的右界，前者大于后者，表示效果显著，有明显差异。

范例 ${\bf8}$ 。另外有一种配对检定（Paired Tests），如学生同时参加期中及期末考试，我们希望检验期末时学生是否有显著进步， 因两次考试都是以同一组学生做实验，故称为配对检定。

![](figures/148-3-FIGURE.jpg)

![](figures/148-4-FIGURE.jpg)

图2.58新药有效性的假设检验结果

$$
\mathrm{H+s=\overline{{n}} :}
$$

(2）使用tstats.itest rel()做假设检定，参数放入两组数据及 alternative-'greater『表右尾检定。程序代码如下：

①执行结果： 品 $1. 0 1 5 9$ , $\boldsymbol{p} \boldsymbol{1}$ 直0.1561，p值大于显著水平
统计量
(0.05) 备择假设为假，表示学生成绩没有显著进步，如图2.59 所示。

$$
\mathrm{\ --( F_{X}^{n} i \pi~ H_{0} ~ \colon~ \mu_{1}=\mu_{2} ~}
$$

备择假设H： 

1）使用随机数模拟学生两次考试成绩。程序代码如下

| 1 | import numpy as np |
| 2 | import matplotlib.pyplot as plt |
| 3 | from scipy import stats |
| 4 |  |
| 5 | np.random.seed(123) |
| 6 | midTerm = np.random.normal(6o, 5, 100) |
| 7 | endTerm = np.random.normal(61,5, 100) |
| 8 | print(f"期中考:{midTerm.mean():.2f}") |
| 9 | print(f"期末考:{endTerm.mean():.2f}") |

执行结果：期中考：60.14，期末考：60.90

![](figures/149-7-FIGURE.jpg)

②图2.59实线为「母体平均数 $\dagger$ t统计量x母体标准差右侧虚线为置信区间的右界，前者小于后者，表示效果不显著。

以上基础统计的介绍，环环相扣，以某市市长选举为例介绍如下，详见图 $2 \!-\! 6 0$ 。

(1）抽样：使用抽样方法，从有投票权的市民中随机抽出一批市民意见作为样本。

 $( 2 )$ 计算描述统计量：推估母体的平均数、标准差、变异数等描述统计量，描绘出大部分市民的想法与差异

(3）概率：推测候选 $\mathcal{A}$ 当选的概率，再进而推估概率分布函数。

(4）假设检定：依据概率分布函数，进行假设检定，推论候选 $\mathcal{A}$ 当选的可信度或确定性，这就是一般古典统计的基础流程，与机器学习以数据为主的预测相辅相成，可彼此验证对方。

![](figures/150-6-FIGURE.jpg)

图2.59，学生成绩的配对检验结果

![](figures/151-0-FIGURE.jpg)

图 $2. 6 0$ 上述介绍的基础统计范围及其关联

线性规划是运筹学（Operations Research, OR）中非常重要的领域，它是给定一些线性的限制条件，求取目标函数的最大值或最小值。

## 2-5 线性规划

17线性规划.ipynb >以下程序请参考02

范例1。最大化目标函数 $z=3 x+2 y$ 

限制条件： $2 x \!+\! y \! \leq\! 1 0 0$ 

$$
x+y \leq8 0
$$

X<40 x>0,y>0

$$
\mathrm{H+s=\overline{{n}} :}
$$

(1）先画图，涂色区域（黄色）为可行解（Feasible Solutions) 即符合限制条件的区域， $2. 6 1$ 所示
如图

![](figures/152-8-FIGURE.jpg)

(2）上述线性规划求解可使用单形法（Simpex Method) 上述问题比较简单，凸集合的最佳解发生在可行解的顶点
$$
( \mathrm{V e r t e x} )
$$
，所以依上图，我们只要求每一个顶点对应的目标函数
值，比较并找到最大的数值即可。不过，深度学习的变数动辄几百 $\uparrow$ ，且神经层及神经元又很多，无法使用单形法求解，而是采取优化的方式，逐渐逼近找到近似解。因此，我们只运用程序来解题， 不介绍单形法的原理。首先安装pulp 包。

(2）上述线性规划求解可使用单形法（Simplex Method 上述问题比较简单，凸集合的最佳解发生在可行解的顶点

$$
\pi:
$$
保，定义目标函数及限制条件。程序代码如

图 $2$ .61、确定可行解

$$
\star\equiv1 \pm\mp\pm\pm\pm\pm\pm\pm
$$

![](figures/153-5-FIGURE.jpg)

![](figures/154-0-FIGURE.jpg)

## 执行结果如下，显示定义内容： 7

| 范例1.最大化目标函数： |  |
| PAALMLZL 小A |  |
| 3*X+2y+0 |  |
| SUBJECT TO 7网牛热：566 |
| 限制式1:2x+y<=100 |  |
| 限制式2:x+y<=80 |
| 限制式3:x<=40 |
| VARIABLES |
| x Continuous |
| y Continuous |  |

## (4）调用modal.solve(求解。程序代码如下：

| 1 | status = model.solve() |
| 2 | status = 'yes' if status == 1 else 'no" |
| 3 | print(f'有解吗?{status}') |
| 4 |  |
| 5 | print(f"目标函数：{model.objective.value()}") |
| 6 | for var in model.variables(): |
| 7 | print(f"{var.name}: {var.value(O}") |
| 8 |
| 9 | print(f'\n限制式的值(不太重要)') |
| 10 | for name, constraint in model.constraints.items(): |
| 11 | print(f"{name}: {constraint.value(O}") |

，当x=20、y-60时，目标函数最大值 $= 1 8 0$ 3 执行结果如下，

范例 $\mathbf{2}$ 。实例应用，运用线性规划来安排客服中心各时段的人力配置，请参考文献°

| 有解吗?yes 目标函数:180.0 |  |
| x:20.0 |  |
| y:60.0 |  |
| 限制式的值(不太重要) |
| 限制式1:0.0 |
| 限制式2:0.0 |
| 限制式3:-20.0 |  |

以上是简单的线性规划，还有其他的模型，如整数规划 (Integer Programming 二次规划（Quadratic
Programming) 非线性规划（Non-linear Programming

普通最小二乘法（Ordinary Least Squares, OLS）、最大似然估计法（Maximum Likelihood Estimation, MLE）是常见的优化和估算参数值的方法，如回归系数、正态分布的平均数/变异数。笔者戏称两者是优化器求解的倚天剑与屠龙 $\supsetg$ ，许多算法问题凭借这两把利器便可迎刃而解。

## 2-6 普通最小二乘法与最大似然估计法在 $\Gamma_{2-3-4}$ 简单线性回归求解」章节，利用一阶导数等于0的特性，对简单线性回归 $( y=w x+b )$ 求取最佳解，使用的就是普通最 $\imath\rfloor\omicron$ 二乘法，这里再强 $\mathrm{[ t-T ~}$ ，使用矩阵运算，让解法可应用到多元回归、深度学习。

首先定义线性回归的目标函数（Objective Function）或称损失函数（Loss Function）为均方误差（MSE) 公式为误差平方和 (SSE) 除以样本数 $( n )$ ，其中

2以矩阵表示线性回归 $y=w X_{c}$ 。其中 $b=b^{\star} x^{0}$ ，故将b也可视为 w的一个项目。

## 2-6-1普通最小二乘法

$$
y \!=\! W_{1} \ X_{1} \!+\! W_{2} \ X_{2} \!+\! W_{3} \ X_{3} \!+\! \L\!+\! W_{n} \ X_{n}+\! \b b
$$

$$
{\frac{\i\i} {\i\hbar}} {\frac{\buildrel\mit\pm} {\mit\i}} ( \begin{array} {c} {\varepsilon} \end{array} )={\frac{\i\i} {\i}} {\vec{\jmath}} \vec{\jmath} \vec{\jmath} \vec{\imath} \vec{\imath} \left( \begin{array} {c} {y} \\ \end{array} \right)-{\vec{\jmath}} \vec{\jmath} \vec{\jmath} \vec{\jmath} \vec{\jmath} \vec{\imath} \left( \begin{array} {c} {\hat{y}} \\ \end{array} \right)
$$

MSE当然越小越好，所以它是一个最小化的问题

$$
\begin{array} {r l} {\left( 1 \right)} & {{} \prod\hbar\hbar\hbar\arctan\hbar\hbar\hbar\operatorname{M S E}=\sum\varepsilon^{2} \, / \, n-\sum\left( y-\hat{y} \right)^{2} / \, n \, \circ} \end{array}
$$

OMSE- SSE $/ \boldsymbol{n}$ ，我们忽略常数 $\boldsymbol{n}$ ，可以只考虑SSE，有

$$
{\mathrm{S S E}}=\sum\varepsilon^{2}=\sum( y-\hat{y} )^{2}=\sum( y-w x-b )^{2}
$$

③以矩阵表示 $\mathrm{S S E} \! :$ 

$$
\begin{matrix} ( y-w x )^{\mathtt{T}} \ast( y-w x ) \\ y^{\mathtt{T}} y-2 w^{\mathtt{T}} x^{\mathtt{T}} y+w^{\mathtt{T}} x^{\mathtt{T}} w x \end{matrix}
$$

$$
\mathbf{\theta} ( y-w x )^{\textsf{T i x}} ( y-w x )
$$

结合矩阵、微 $\acute{\jmath}$ ，我们就能够轻松求出线性回归的系数，比原理是神经网络优化求解的基石

执行结果：W-1.08011358e-(
$$
0 1 \, 4. 6 4 2 0 4 5 8 4 \mathrm{e-} 0 2 \, 2. 0 5 5 8 6 2 6 4 \mathrm{e-}
$$
02 2.68673382e+00

$$
( 2 ) \mod w \lfloor\frac{\unitlength} {1 6} \rangle\langle\frac{\omega} {\pi} \rangle\rangle\neq\frac{\Xi} {\Xi}
$$

$$
\begin{array} {c} {{\displaystyle\frac{\mathrm{d S S E}} {\mathrm{d} w}}=-2 {\boldsymbol{x}^{\mathrm{T}}} y+2 {\boldsymbol{x}^{\mathrm{T}}} x {\boldsymbol{w}}=0} \\ {{\boldsymbol{w}}=( {\boldsymbol{x}^{T}} {\boldsymbol{x}} )^{-1} {\boldsymbol{x}}^{T} {\boldsymbol{y}}} \\ \end{array}
$$

以下程序请参考02 18_普通最小二乘法ipynb

以普通最 $\imath\rfloor$ 二乘法建立线性回归模型，预测波士顿范例1.
 $( \mathbf{B o s t o n} )$ 房价

$$
\mathrm{A F+s \Xi} :
$$

1）依上述公式计算回归系数。程序代码如下

![](figures/158-8-FIGURE.jpg)

3.06049479e-01 -1.23345939e-02 -9.52747232e-01 9.311683270-03

范例 $2$ 。使用 SciPy以普通最 $\imath\rfloor\mathbf{\Lambda}$ 二乘法计算函数X+5的最小值。

$$
- 5. 2 4 7 5 8 3 7 8 \mathrm{e} \!-\! 0 1 \; 3. 6 4 5 9 4 8 8 4 \mathrm{e} \!+\! 0 1 ]
$$

(2）计算相关效果衡量指标。程序代码如下：

![](figures/159-4-FIGURE.jpg)

(3）以 Skearn 库验证。程序代码如下：

![](figures/159-6-FIGURE.jpg)

执行结果：与公式计算一致，验证无误。

(1）先对函数绘图。程序代码如下： $( 2 )$ 调用 scjpy.optimizea模块的 eastsq()国数进行优化求解。

D在leastsq()函数中，第一个参数是求解的函数；第二个参数是起始点；eastsα 是采逼近法，而非纯数学公式求解，nfev显示它经过22次执行周期，才找到最小值5（fvec），当时
$$
\mathbf{X=} 1. 7 2 8 9 2 3 7 9 \mathrm{e} \mathrm{-} 0 5 \approx0_{\circ}
$$

![](figures/160-2-FIGURE.jpg)

执行结果：如图2.62所示。

![](figures/160-4-FIGURE.jpg)

图 $2. 6 2$ 函数绘图

![](figures/160-6-FIGURE.jpg)

②执行结果如下：

③起始点可设为任意值，通常采用随机数或是直接给 $0$ 。指定值设定不佳的话，仍然可以找到最佳解，不过，需要较多次的执行周期，也就是所谓的较慢收敛（Convergence)

今当面对较复杂的函数或较多的变量时，我们很难单纯运用数学去求解，因比，逼近法会是一个比较实用的方法，深度学习的梯度下降法就是一个典型的例子，后面章节我们将使用TensorFlow 程序来进行说明。

![](figures/161-2-FIGURE.jpg)

最大似然估计法（MLE）也是估算参数值的方法，二标是找到一个参数值，使出现目前事件的概率最大。如图2.63所示，圆点是样本点，由线是四个可能的正态概率分布（平均数/变异数不同） 我们希望利用最大似然估计法找到最适配（Fit）的一个正态概率分布。

(2）假设来自正态分布的多个样本互相独立，联合概率就等于个别的概率相乘。即

## 2-6-2 最大似然估计法

![](figures/162-3-FIGURE.jpg)

图2.6、最大似然估计法示意图

下面使用最大似然估计法求算正态分布的参数 $\mu\Im\delta_{\epsilon}$ 

## （1）正态分布

$$
P ( x ; \mu, \sigma) \!=\! \frac{1} {\sigma\sqrt{2 \pi}} \operatorname{e x p} \! \left(-\frac{( x-\mu)^{2}} {2 \sigma^{2}} \right)
$$

$$
{\mathcal{L}} ( {\mathcal{D}} \, | \, \mu, \sigma^{2} )=\prod_{i=1}^{N} \frac{1} {\sqrt{2 \pi\sigma^{2}}} e^{-\frac{\left( x_{i}-\mu\right)^{2}} {2 \sigma^{2}}}
$$

(3）目标是求取参数值 $\mu\Im\delta$ 、最大化概率值，即最大化目标函数。即

(4）N个样本概率全部相乘，不易计算，通常我们会取对数变成一次方，所有的概率值取对数 $\sqrt{\Xi}$ ，大小顺序并不会改变。即

(7）一阶导数为0时，有最大值，得到目标函数最大值下的 $\nabla\delta$ 。所以，使用最大似然估计法算出的 $\mu\nabla$ 心就如同我们常见的 $\big<$ 式。有

$$
\operatorname{a r g} \operatorname* {m a x} {\cal L} ( {\cal D} \, | \, \mu, \sigma^{2} ) \, \mu, \sigma^{2}
$$

$$
\mathrm{\operatorname{a r g} \operatorname* {m a x} \operatorname{l o g} {\cal L} ( {\cal D} | \, \mu, \sigma^{2} )} \, \mu, \sigma^{2}
$$

 $( 5 )$ 取对数 $\sqrt{\Pi}$ ，化简为

$$
\log{\mathcal{L}} ( {\mathcal{D}} \, | \, \mu, \sigma^{2} ) \!=\!-\! \frac{N} {2} \log( 2 \pi\sigma^{2} ) \!-\! \frac{1} {2 \sigma^{2}} \! \sum_{i=1}^{N} ( x_{i}-\mu)^{2}
$$

(6）对及8分别偏微分，得

$$
\begin{array} {c} {{\displaystyle{\frac{\partial\operatorname{l o g} {\cal L}} {\partial\mu}}=\frac{1} {\sigma^{2}} \sum_{i=1}^{N} ( x_{i}-\mu)}} \\ {{\displaystyle{\frac{\partial\operatorname{l o g} {\cal L}} {\partial\sigma}}=-\frac{N} {\sigma}+\frac{1} {\sigma^{3}} \sum_{i=1}^{N} ( x_{i}-\mu)^{2}}} \end{array}
$$

$$
\frac{\partial\operatorname{l o g} \mathcal{L}} {\partial\mu} \!=\! \frac{1} {\sigma^{2}} \sum_{i=1}^{N} ( x_{i}-\mu)
$$

$$
\frac{\partial\operatorname{l o g} \mathcal{L}} {\partial\sigma}=-\frac{N} {\sigma}+\frac{1} {\sigma^{3}} \sum_{i=1}^{N} ( x_{i}-\mu)^{2}
$$

$$
\begin{array} {l} {\mu=\frac{1} {N} \sum_{i=1}^{N} x_{i}} \\ {\sigma^{2}=\frac{1} {N} \sum( x_{i}-\mu)^{2}} \\ \end{array}
$$

>以下程序请参考02 19最大似然估计法ipynb (1） NumPy计算概率密度函数 $\mathrm{( P D F )}$ 程序代码如使用
下:

## 范例1。如果样本点x-1，计算来自正态分布N （0,1）的概率

$$
\mathrm{T :}
$$

| 1 | #载人库 |
| 2 | import numpy as np |
| 3 | import math |
| 4 |  |
| 5 | #正态分布的概率密度函数 |
| 6 | def f(x, mean, std): |
| 7 | return (1/((2*np.pi*std**2) **.5)) * np.exp(-0.5*((x-mean)/std)**2) |
| 8 |  |
| 9 | F(1,0,1 |

执行结果：0.24

(2）也可以使用soipy,stats 模块计算。程序代码如下：

| 1 2 | trom scipy.stats import norm |
| 2 | #亚约数(mean）、标准美(std） |
| 4 | 欢— mean=0  |
| 5 | std=1 |
| 6 |  |
| 7 | #计算来自正态分布N(O,1)的概率 |
| 8 | norm.pdf(1, mean, std) |

执行结果：0.24

）绘制概率密度函数。程序代码如下： (3)

![](figures/164-9-FIGURE.jpg)

执行结果：如图2.64所示范例 $2$ ，如果有两个样本点 $\mathbf{x}=1, \ 3$ ，来自正态分布N（11）及 N（2,3）的可能性，哪一个比较大？

假设两个样本是独立的，故联合概率为两个样本概率相乘，使用scip $\mathbf{Y}$ .stats 模块计算概率。程序代码如下：

范例 $\textbf{3}$ 。如果有一组样本，计算来自哪一个正态分布N $( \mu, \, \delta)$ 的概率最大，请依上面证明计算 $\boldsymbol{\mu}$ 、 $\delta_{\circ}$ 

![](figures/165-3-FIGURE.jpg)

图 $2$ 64 绘制概率密度函数

![](figures/165-5-FIGURE.jpg)

执行结果如 $\mathrm{T}$ ，表明来自N $( 1, 1 )$ 可能性比较大。

来自 ${\mathsf{N}} ( 1, 1 )$ 的概率： $0. 0 2 1 5 3 9 2 7 9 3 0 1 8 4 8 6 3 4$ 来自 ${\mathsf{N}} ( 2, 3 )$ 的概率：0.01582423339377573

(1）对正态分布的PDF取对数（og 。程序代码如下： (3）上述函数使用 if()各对平均数、变异数偏微分。程序代码如下：

![](figures/166-1-FIGURE.jpg)

. Jupyter Notebook 可显示Lalex数学式。有执行结果如下，

$$
\log\left( {\frac{0. 7 0 7 1 0 6 7 8 1 1 8 6 5 4 7 {\mathrm{e}}^{-{\frac{0. 5 (-m+x^{2} )} {s^{2}}}}} {\pi^{0. 5} ( s^{2} )^{0. 5}}} \right)
$$

(2）带入样本数据。程序代码如下

1#带入样本数据
2 $\texttt{l o g P}=\ \emptyset$ 
3 for xi in data:
4
$$
\texttt{l o g} \texttt{P}+=\texttt{l o g} \texttt{p. s u b s} \left( \left\{\texttt{x} ; \right. \times\texttt{i} \right\} )
$$
5
6 logP

## 码如下：

| 1 | from sympy import diff |
| 2 |  |
| 3 | logp diff m= diff(logP,m） #对平均数(m）愿微分 |
| T 4 | 中囗一一G |
| 5 | logp diff s= diff(logP,s)#对变异数(s）福微分 |
| 6 | 畦铖O一E—Q |
| 7 | print('m 偏导数：,logp diffm) |
| 8 | 一— print('s 偏导数：',logp diffs) |

(4）使用simpliy()简化偏导数。程序代码如下： (5）令一阶导数为0，有最大值，可得到联立方程式。程序代码如 $\mathrm{T}$ :

| 1 | from sympy import simplify |
| 2 |  |
| 3 | #简化m偏导数 |
| 4 | logp diff m = simplify(logp diff m) |
| 5 | print(logp diff m) |
| print) 6  |
| 7 |
| 8 | #简化s偏导数 |
| 9 | logp diff s = simplify(logp diff s) |
| 10 | logp diff s |

$$
\pi\exists\flat\brack\mathrm{F} :
$$

| 1 ^ | from sympy import solve |
| 3 | funcs = Tlogpc diff s, logpc diff m] |
| 4 | solve(funcs, ,[m,s])  |

执行结果：
$$
( 3. 6 2,-1. 5 7 ) \ \, \ \ \ ( 3. 6 2, 1. 5 7 ) \ \ ]
$$

(6）使用NumPy验证。程序代码如下：

 $1$ 
$$
\mathrm{n p. m e a n ( d a t a ) \,, \; \; n p. s t d ( d a t a ) \,.}
$$

执行结果：（3.62, 1.57) ，与MLE求解的结果相符有了以上的基础后，我们将综合微积分、矩阵、概率等数学知 $\grave{\i} \mathbb{R}$ ，对神经网络求解，这是进入深度学习领域非常重要的概念。

## 2-7 神经网络求解神经网络是深度学习最重要的算法，它主要是仿真生物神经网络的传导系统，希望通过层层解析，归纳出预测的结果，如图2.65 所示。

生物神经网络中表层的神经元接收到外界信号，归纳分析后， 再通过神经未梢，将分析结果传给下一层的每个神经元，下一层神经元进行相同的动作，再往后传导，最后传至大脑，大脑做出最后的判断与反应，如图2.66所示。

## 2-7-1 神经网络

![](figures/169-3-FIGURE.jpg)

图2.6.5生物神经网络的传导系统

![](figures/169-5-FIGURE.jpg)

图 $2. 6 6$ 单一神经元结构

于是，AI科学家将上述生物神经网络简化成如图 $2. 6 7$ 所示的网络结构：

AI神经网络最简单的连接方式称为完全连接（Full Connected, $\mathrm{F C} )$ ，即每一神经元均连接至下一层的每个神经元，因此，我们可以把第二层以后的神经元均视为一条回归线的 $\boldsymbol{y}$ ，它的特征变量x就是前一层的每一个神经元。如图2.68所示， $\boldsymbol{y}_{1}$ 、之两条回归线表示为

所以，简单地讲，一个神经网络可视为多条回归线组台而成的模型

![](figures/170-3-FIGURE.jpg)

图2.67 Al神经网络

$$
\begin{array} {c} {y_{1}=W_{1} X_{1}+W_{2} X_{2}+W_{3} X_{3}+b} \\ {Z_{1}=W_{1} y_{1}+W_{2} y_{2}+W_{3} y_{3}+b} \\ \end{array}
$$

$$
\boldsymbol{y}_{1} \!=\, \boldsymbol{W}_{1} \boldsymbol{X}_{1}+\boldsymbol{W}_{2} \boldsymbol{X}_{2}+\boldsymbol{W}_{3} \boldsymbol{X}_{3}+\boldsymbol{b}
$$

$$
Z_{1} \!=\, W_{1} y_{1}+\, W_{2} y_{2}+\, W_{3} y_{3}+\b b
$$

![](figures/170-8-FIGURE.jpg)

图2.68一个神经网络模型

以上的回归线是线性的，为了支持更通用性的解决方案
(Generic Solution），模型还会乘上一个非线性的数，称为激励函数（Activation Function），期望也能解决非线性的问题，如图 2.69 所示。因激励函数并不能明确表达原意，因此下百直接以
Activation Function 表示。

如果不考虑激励函数，每一条线性回归线的权重及偏差可以通过普通最 $\rfloor\i\lrcorner$ 乘法求解，请参阅2-6-1说明，但如果乘上非线性的 Activation Function，就比较难用单纯的数学求解 $\overline{{\jmath}}$ ，因此，学者就利用优化（Optimization）理论，针对权重、偏差各参数分别偏微 $\acute{\jmath}$ ，沿着切线（即梯度）逐步逼近，找到最佳解，这种算法就称为梯度下降法（Gradient Descent)

有人用 $7-$ 个很好的比喻，「当我们在山顶时，不知道下山的路，于是，就沿路往下走，遇到叉路时，就选择坡度最大的叉路定，直到抵达平地为上」，所以梯度下降法利用偏微分（Parial Differential）求算斜率，依斜率的方向，一步步地往下走，逼近最佳解，直到损失函数没有显著改善为 $\lfloor\b L$ ，这时我们就认为已经找到最佳解 $\overline{{\jmath}}$ ，如图2.70所示。

![](figures/171-3-FIGURE.jpg)

$$
\mathrm{g \! : ~ \ A c t i v a t i o n ~ F u n c t i o n}
$$

图 $2. 6 9$ 激励函数

上图及以下程序请参考02 20，梯度下降法Ipynb

![](figures/172-1-FIGURE.jpg)

图 $2. 7 0$ 梯度下降法示意图

梯度其实就是斜率，单变量口归线的权重称为斜率，多变数口归线时，需分别作偏微分求取权重值，此时就称为梯度。下百我们先针对单变量求解，示范如何使用梯度下降法（Gradient
Descent）求取最 $\rfloor\rfloor\jmath\g$ 直。

范例 ${\bf1}$ 、假定损失函数f $( \mathbf{x} ) \ =\mathbf{x}^{2},$ 、而非MSE，请使用梯度下降法求取最小值。

注意：损失函数又称为目标函数或成本函数，在神经网络相关文献中大多称为损失函数，本书 $\boldsymbol{\mathcal{M}}$ 善如流,以下将统一以损失函数取代目标函数。

(2）定义梯度下降法函数，反复更新 $\boldsymbol{X}$ ，更新的公式如下，后面章节我们会推算 $\big< \lambda$ 式的由来。

新的x-目前的×学习率（learning rate）*梯度（gradient） 程序代码如下：

## 2-7-2梯度下降法

(1）定义函数（fune）及其导数（dfunc） 。程序代码如下：

![](figures/173-7-FIGURE.jpg)

(3）设定起始点、学习率（In { $( \mathrm{e p o c h s} )$ 等参
执行周期数
数后，呼叫梯度下降法求解。程序代码如下：

![](figures/174-1-FIGURE.jpg)

![](figures/174-2-FIGURE.jpg)

D执行结果：如图2.71所示

![](figures/174-4-FIGURE.jpg)

图 $2. 7 1$ 梯度下降法执行结果

2每一执行周期的损失函数如图2.72所示。随着x变化，损失函数逐渐收敛，即前后周期的损失函数差异逐渐缩 $\imath\rfloor\omicron$ ，最后当x-0 时，损失函数 $\boldsymbol{f}$ (（x）等于 $0$ 、为函数的最小值，与普通最 $\prime\rfloor\varsigma=$ 乘法 (OLS）的计算结果相同。

® $\mathrm{( x \_s t a r t )}$ 为其他值，如-5，依然可以找到如果改变起始点
相同的最小值。

范例 $2$ 。假定损失函数f $( \mathbf{x} ) \ =2 \mathbf{x}^{4} \mathbf{-3 x-2 0}$ ，请使用梯度下降法求取最小值。

5.2,0.
$$
8, 0. 3 2, 0. 1 3, 0. 0 5, 0. 0 2, 0. 0 1, 0, 0, 0, 0, 0, 0
$$
., 0.0

![](figures/175-4-FIGURE.jpg)

图 $2. 7 2$ 每一执行周期的损失函数

1）定义函数及其微分。程序代码如下

| 1 | #3 损失函数 |
| 2 | def func(x): return 2 2*x**4-3*x+2*x-20 |
| 3 |  |
| 4 | #大损失函数一阶导数 |
| 5 | def dfunc(x): return 8*x**3-6*x+2 |

(2）绘制损失函数。程序代码如下

(3）梯度下降法函数（GD）不变，执行程序，如果学习率不变 $( \mathrm{I r}=0. 3 )$ 「Resuittoo large」，原因是学习
，会出现错误信息
率过大，梯度下降过程错过最小值，往函数左方逼近，造成损失函数值越来越大，最后导致溢 $\imath\overrightarrow{\Omega}$ 。程序代码如下：

![](figures/176-1-FIGURE.jpg)

执行结呆：如图 $2$ 735所示，

![](figures/176-3-FIGURE.jpg)

图 $2. 7 3$ 梯度下降法损失函数

![](figures/176-5-FIGURE.jpg)

(4）修改学习率 $( \mathrm{I r}=0. 0 0 1 )$ 同时增加执行周期数
$$
( \mathbf{e p o c h s}=1 5 0 0 0 )
$$
避免还未逼近到最 $\rfloor\j\ghfil$ 直程序就先提早结束。
程序代码如下：

观察上述范例，不管函数为何，我们以相同的梯度下降法
(GD 欧数）都能够找到函数最小值，最重要的关键是x的更新公式

接着我们会说明此公式的由来，也就是神经网络求解的精华所在。

![](figures/177-3-FIGURE.jpg)

执行结果： $\exists x=0. 5 1$ 时，函数有最小值，如图2.74所示

![](figures/177-5-FIGURE.jpg)

图 $2. 7 4$ 修改学习率后执行结果

新的x-目前的×学习率（learning rate） $\times$ 梯度（radient 神经网络求解是一个正向传导与反向传导反复执行的过程，如图2.75 所示。

(1）由于神经网络是多条回归线的组合，建立模型的主要任务就是计算出每条回归线的权重 $( w )$ 与偏差（b

(2）依上述范例的逻辑，一开始我们指定 $\boldsymbol{W}$ 、b为任意值，建立回归方程式 $y_{=W X} \!+\! b_{\mathbf{,}}$ ，将特征值（x）带入方程式，可以求得预测值 $( \, \hat{y} \, )$ ，进而计算出损失函数，例如 $M S E$ ，这个过程称为正向传导（Forward Propagation)

(3）透过最小化MSE的目标和偏微分，可以找到更好的w $\boldsymbol{b}$ ，并依学习率来更新每一层神经网络的 $\boldsymbol{W}.$  $\boldsymbol{b}$ ，此过程称之为反向传导（Backpropagation）。这部分可以借由微分的链法则（Chain Rule），依次逆算出每一层神经元对应的 $\boldsymbol{W}$ 、 $\boldsymbol{b}$ ，公式为

## 2-7-3 神经网络求解

![](figures/178-5-FIGURE.jpg)

图 $2. 7 5$ 神经网络权重求解过程

式中： E $= \!-2 \, * \times\, * ( y-\, \, \hat{y}$ )[后续证明]；学习率为优化器事
梯度
先设定的固定值或动能函数。

(4）重复（2 (3）步骤，一直到损失函数不再有明显改善为止

(1）损失函数 $\mathrm{M S E}={\frac{\sum( y-\hat{y} )^{2}} {n}} \,,$ 因7为常数，故仅考虑分子即SSE.

 $W_{\mathrm{t}+1}=W_{\mathrm{t}}$ 学习率 $\mathbf{x}$ 梯度

梯度 $\big< \lambda$ 式证明如下。

(2)$SE-
$$
\sum( y-{\hat{y}} )^{2}=\sum( y-w x )^{2}=\sum( y^{2}-2 y w x+w^{2} x^{2} )
$$

(3）以矩阵表示， x

$$
\left( \mathbf{4} \right) \quad{\frac{\partial\mathrm{S S E}} {\partial w}}=-2 y x+2 w x^{2}=-2 x ( y-w x )
$$
;)=-2x(y-)

(5）同理， ${\frac{\partial\mathrm{S S E}} {\partial b}} \!=\!-2 x^{0} ( y-\hat{y} ) \!=\!-2 ( y-\hat{y} )$ 

(6）为了简化公式，常把系数 $2$ 拿掉

$$
( 7 ) \quad\frac{\Xi} {\Xi} \equiv\angle\pm\pm\Sigma
$$

调整后权重←原权重＋（学习率*梯度）

(8）有些文章将梯度负号拿掉， $\big< \lambda$ 式就修正为调整后权重一原权重-（学习率*梯度）

(8）有些文章将梯度负号拿掉，公式就修正为

以上是以MSE为损失函数时的梯度计算公式，若是其他损失函数，梯度计算结果也会有所不同，如果再加上Activation
Function，梯度 $\big< \lambda$ 式计算就更加复杂 $\overline{{\jmath}}$ ，好在深度学习框架均提供自动微分、计算梯度的功能，我们就不用烦恼 $\overline{{\j}}$ 。后续有些算法会自定义损失函数，会因此产生意想不到的功能，如风格转换（Style Transter）可以合成两张图像，生成对抗网络（Generative
Adversarial Network, GAN），可以产生几可乱真的图像。也因为如此关键，我们才花费了这么多的篇幅介绍梯度下降法。

基础的数学与统计就介绍到此，告一段落，下一章起，我们将开始以TensorFlow ，并介绍相关的应用。
实现各种算法，

ensorFlow 是 Google Brain 于 2015年发布的深度学习相架它是日前深度学习框架占有率最高的框架，又于1.4版纳 $\lambda$ 了 Keras，兼顾简易性与效能，是深度学习最佳的入门套件。

(1）从张量运算、自动微分、神经层，最后构建完整的神经网络。

(2）说明神经网络的各项函数，如Activation Function、损失函数、优化器（Ootimizer) (Metrics）等，并介
效果衡量指标
绍运用梯度下降法找到最佳解的原理与过程。

(3）示范TensorFlow各项工具的使用，包含回调函数 (Callback) TensorBoard 可视化工具、TensorFlow Dataset TensorFlow Serving部署等。

## 第二篇TensorFlow基础篇

本篇将介绍TensorfFlow 的整体架构，包含以下内容。

(4）神经网络完整流程的实践。
(5）卷积神经网络（Convolutional Neural Network, CNN) (6）预先训练的模型（Pre-trained Model)
(7）转移学习（Transfer Learning）

t4）神经网络完整流程的实践。

(5）卷积神经网络（Convolutional Neural Network, CNN)

(6）预先训练的模型（Pre-trained Model）。

(7）转移学习（Transter Learning) O ## TensorFlow架构与主要功能 ## 第3章

维基百科列举了近20种的深度学习框架，有许多已经逐渐被淘汰 $\overline{{\jmath}}$ ，以Python/C++ 语言的框架而言，目前比较流行的只剩以
具体见表3.1。
下四和，

TensorFlow、PyTorch 分居占有率的前两名，其他框架均望尘莫 $\mathcal{R}$ ，如图3.1所示。因此推荐读者若是使用 Python 语言，熟悉 TensorFlow、PyTorch 就绰绰有余了；若偏好 $\mathrm{C / C++}$ ，则可以使用 Caffe 框架。

## 3-1常用的深度学习框架

$$
\mp3. 1 \quad\mathrm{P y t h o n} / \mathrm{C++i \pm\Xi\pm\pm\pm\pm\Psi}
$$

| 框架名称 | 程序语言 | 优点 | 缺点 |
| --- | --- | --- | --- |
| TensorFlow/Keras | Python、 Ct+ JS、Java、Go | 1.Keras简单、易入门，支持多种程序语言； 2.效果佳  | 与Python未完全整合,如模型结构不能直接使用if  |
| PyTorch | Python、 C++、 Java  | 1.与Python整合较好，有弹性： 2.学界采用占比有增加的趋势 | 执行速度较慢 |
| Caffe | C++ | 1.执行速度非常快： 2.专长于影像应用，NVIDIA有一改良版  | 1.复杂； 2.无NLP模块 |
| Apache MXNet | Python、 Scala、 Julia  | AWS/Azure 云端支持 | 复杂 |

![](figures/183-5-FIGURE.jpg)

图3.1深度学习框架在Arxiv网站的论文采用比率

本书的范例以TensorFlow/Keras为主，不会涉及其他深度学习框架，以期对TensorFlow/Keras 进行全面而深入的探讨

(图片来源： Mhic de eaning tamo k ksth bst? R)

梯度下降法是神经网络主要求解的方法，计算过程需要使用大量的张量运算，另外，在反向传导的过程中，要进行偏微分，并需解决多层结构的神经网络问题。因此，大多数的深度学习框架至少都会具备以下功能。

学习的路径可以从简单的张量运算开始，再逐渐熟悉高阶的神经层函数，以奠定扎实的基础。

接着再来了解TensorFlow 执行环境（Runtime) 如图 $3. 2$ 所 $\overline{{\Pi}}$ 。它支持·GPU、分布式计算、低阶CAP 等，也支持多种网络协议。

## 3-2 TensorFlow 架构

(1）张量运算：包括各种向量、矩阵运算。

$$
( 2 ) ~ \boxed{\Xi} \hbar\line\# \linebreak\Sigma_{\circ}
$$

神经网络及各种神经层（Layers O (3)

![](figures/185-7-FIGURE.jpg)

(2）TensorFlowjs：网页版本，适合边缘运算的装置及虚拟机装置（Docker/Kubernetes)

$$
( 3 ) \, \, \mathrm{T e n s o r F l o w ~ L i t e :}
$$
轻量版，适合指令周期及内存有限的
移动设备和物联网装置

程序设计堆栈（Progranming Stack）如图3.3所示，在2.x版后以Keras为主轴，TensorFlow 团队依照 Keras的规格重新开发，逐步整合其他模块，比独立开发的 Keras 框架功能更加强大， 因而Keras「大神」Frangois Chollet 也宣告 Keras 独立框架不再升级，甚至将Keras官网也改为介绍TensorFlow的 Keras模块， 而非自家开发的 Keras独立框架。所以，现在要撰写 Keras 程序， 应以TensorFlow为主，避免再使用 Keras 独立框架。

图 $3. 2$ TensorFlow执行环境 $( \mathrm{R u n t i m e} )$ 

(图片来源：TensorFlow GitHub l)

TensorFlow执行环境包括以下三个版本

$$
( 1 ) \textit{\textsf{T e n s o r F l o w :}}-\hbar\pi\tau\+ \dag\) \hbar\b, \hbar\pi\) \circ
$$

![](figures/186-7-FIGURE.jpg)

图3.3 TensorFlow程序设计堆栈 Stack)

TensorFlow Keras模块逐步整合其他模块及工具，如果读者之前使用的是 Keras独立框架，则可以补强以下项目。

除了 Keras的引进，特别要注意的是，TensorFlow 2.x版之后，默认模式一修改为Eager Execution Mode，舍弃了 Session 语法，若未调回原先的 Graph Execution Mode，则1.x版的程序执行时将会产生错误。由于日前网络上许多范例的程序新旧杂陈，还有很多1.x版的程序，读者应特别注意。

(图片来源：Exiaied: eep Leanin in Tensorflowl

(1）低阶梯度下降的训练（GradientTape) (2）数据集载入（Dataset and Loader)
(3）口调函数
$$
( \mathrm{C a l l b a c k} )
$$
(4）估计器（Estimator)
 $( 5 )$ 预训模型（Keras Application)
(6）TensorFlow Hub：进阶的预训模型 (7）TensorBoard可视化工具。
(8）TensorFlow Serving 部署工具。

(1）低阶梯度下降的训练（GradientTape O

(2）数据集载入（Dataset and Loader) O

$$
( 3 ) ~ ~ \exists i \sharp\b~ \exists\b~ \exists\sharp\b~ \Sigma~ \rangle\linebreak\Sigma~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~allowbreak
$$

$$
( 4 ) \, \setminus\mathrm{i+\frac{n g g} {n!} ~ ( E s t i m a t o r ) ~}_{\circ}
$$

(5）预训模型（Keras Application O
S

(6）TensorFlow Hub：进阶的预训模型。

(7）TensorBoard可视化工具

$$
\mathrm{( 8 ) ~ \ T e n s o r F l o w ~ S e r v i n g ~ \hbar\beta~ \) ~ I ~}
$$
具

我们会在后面的章节陆续探讨各项功能。 TensorFlow 顾名思义就是提供张量的定义、计算与变量值的传递功能。
因比，它最底层的功能即支持各项张量的数据类型与其运算函数，基本上都遵循NunPy库的设计理念与语法，包括传播 (Broadcasting）机制的支持。

以下我们直接以实践代替长篇大论，读者请参阅03 3张量运算.ipynb.

(3）如果要知道GPU的详细规格， $\mathrm{P y C u d a}$ 模块。请
可安装
注意在 Windows 环境 $\mathrm{F}$ ，无法以pip install ycuda 安装，可到 https://www.fd.uci.edu/-gohlke/pythonibs/?
cm mo uid-08085305845514542921829&cm me sid 50200000-145

em me uid-080853058455145429218
$$
3 2 9 \& \mathrm{c m \_~ s i d \_} 5 0 2 0 0 0 0 0=1 4 5
$$

## 3-3 张量运算

(1）显示TensorFlow版本。程序代码如下：

| 1 | #载人库 |
| 2 | import tensorflow as tf |
| 3 |  |
| 4 | #显示版本 |
| 5 | print(tf. version |

(2）检查 GPU是否存在。程序代码如下：

1 # check cuda available
2 tf.config.list physical devices('GPU')

，可显示 GPU 简略信息执行结果如下，

[PhysicalDevice (name-'/physical device: GPU: 0',device type-'GPU)]

6395916#pycuda 下载对应 Python、Cuda Toolkt版本的二进制文件

举例来说，Python v3.8三安装 Cuda Tookit v10.1，则须下载 pycuda-2020.1+cuda101-cp38-cp38-win amd64.wh!，并执行pip install pycuda-2020.1+cuda101-cp38-cp38-win amd64.whl。接着就可以执行本书所附的范例：python GpuQuery.py。

执行结果如下，即可显示GPU的详细规格，重要信息排列在前面

(4）声明常数（constant) 参数可以是常数、ist、NumPy array。程序代码如下：

装置 0: NVIDIA GeForce GTX 1050 Ti
计算能力：6.1
GPU记忆体:4096MB
 $6$ 个处理器，各有128个CUDA核心数，共768个CUDA核心数 ASYNC ENGINE COUNT: 5
CAN MAP HOST MEMORY: 1
CLOCK RATE: 1392000
COMPUTE CAPABILITY MAJOR: 6
COMPUTE CAPABILITY MINOR: 1
COMPUTE MODE: DEFAULT
CONCURRENT KERNELS: 1
ECC ENABLED:O
GLOBAL L1 CACHE SUPPORTED:1
GLOBAL MEMORY BUS WIDTH: 128

1 $^\sharp$ 声明常数(constant），参数可以是常数 $l i s t \; \cdot N u m p y \; A r r a y$ 2 x = tf.constant([[1, 2]])

(5）支持四则运算。程序示例如下：

1 print(x+10)
 $2 ~$ |
$$
\operatorname{p r i n t} ( x-1 \textcircled{0} )
$$
3
$$
\operatorname{p r i n t} ( x^{*} 2 )
$$
 $4$ print(x/2)

执行结果如下： reduce surn 是沿着特定轴加总，输出会少一维，故1， $2$ .3] 套用函数运算后等于 $6$ 。

(7）TensorFlow 会自动决定在 CPU 或GPU运算，可白下列指令侦测。一般而言，常数（tf.constant）放在 CPU，其他变量则放在GPU $\b L$ ，两者加总时，会将常数搬到 GPU上再加总，过程不需要人为操作。但请注意，PyTorch框架稍显麻烦，必须手动将变量搬移至 CPU或GPU 运算，不允许一个变量在CPU，另一个变量在 GPU。侦测程序代码如下：

tf.Tens
$$
\mathrm{s o r ~ \left( [ [ 1 1 ~ 1 2 ] ], \mathrm{s h a p e=~ ( 1, 2 ) ~ \, \mathrm{d t y p e}} \right) ~}
$$
e=int32
tf.Tensor(
$$
[-9-8 ] ], \, \mathrm{s h a p e=~ ( 1, 2 ) ~} \,, \mathrm{d t y p e=i n t 3 2 ~ )}
$$
tf.Ten $\mathrm{s o r ~ \langle[ [ 2 ~ 4 ] ], \ s h a p e=~ ( 1, 2 ) ~}$ , dtype=int32
tf. Tensor $\i[ [ 0. 5 ~ 1. ~ ] ],$ sh：
$$
\mathrm{a p e=~ ( 1, 2 ) ~}, \mathrm{d t y p e=f l o a t 6 4 ~ )}
$$
注意：如果要显示数值，须转换为NumPy array。例如： (x+10).numpy()

tfTensor (ID.
$$
5 1. \, ] \mathrm{, \, s h a p e=~ ( 1, 2 ) \, \, \,, \, d t y p e=f l o a t 6 4 )}
$$

注意：如果要显示数值，须转换为NumPy aray。例如

$$
( \mathbf{x}+1 0 ) \mathrm{\_{. \ n u m p y ( )}}
$$

(6）四则运算也可以使用TensorFlow 函数。程序代码如下

| 1 | #转为负数 |
| --- | --- |
| 2 | print(tf.negative(x)) |
| 3 |  |
| 4 | #常数、List 、Num Py array 均可运算 |
| 5 | print(tf.add(1, 2)) |
| 6 | print(tf.add([1, 2], [3, 4])) |
| 7 |  |
| 8 | print(tf.square(5)) print(tf.reduce sum([1, 2, 3])) |
| 9 |
| 10 |
| 11 | #混用四则运算持号及TensorFlow 函数 |
| 12 | print(tf.square(2) + tf.square(3)) |

(8）用户也能够使用 with f.deice（「CPU：（」）或with tf.device(「GPU: O」 强制指定在 CPU 或GPU运算。程序代码如 $\mathrm{T}$ :

| 1 | tf.constant([[1, 2,3]], dtype=float) |
| 2 | ("x1 是否在 0 GPU #O 上: ,x1.device.endswith("GPU:0")) |
| 3 |  |
| 4 | 居x为均匀分配乱数3x3 |
| 5 | tf.random.uniform([3,31) |
| 6 | ("x2是否在 GPU#O上: x2.device.endswith('GPU:O')) |
| 7 |  |
| 8 | +X2 |
| 9 | ("x3是否在 GPU #O 上 x3.device.endswith('GPU:0')) |

$$
\pm\pm\pm\pm\pm\pm\pm
$$

x1是否在 GPU#O $\bot$ ：False $\times2$ 是否在GPU #0 $\pm$ ： True ×3是否在GPU #0上：True

 $\times1$ 是否在GPU#0 $\pm$ ：False

 $\times2$ 是否在GPU#0 上： Tue

x3是否在GPU#0上：True

![](figures/191-7-FIGURE.jpg)

D第 $\underline{{\quad}}$ 次执行结果如下，CPU运算比GPU快：

$$
\mathrm{O n \, C P U \colon\, 6 4. 0 0 m s}
$$

$$
\mathrm{O n \ G P U \colon\ 3 1 1. 4 9 m s}
$$

(9）稀疏矩阵（Sparse Matrix）运算：稀疏矩阵是指矩阵内只有很少数的非零元素，如果依一般的矩阵存储会非常浪费内存， 运算也是如此，因为大部分项目为零，不需浪费时间计算，所以， 科学家针对稀疏矩阵设计出了特殊的数据存储结构及运算算法， TensorFlow 也支持此类数据类型，如图3.4所示。

(10）TensorFlow 稀疏矩阵只需设定有值的位置和数值，并设定维度如下:

0多次执行后，GPU反而比OPU运算快了很多

$$
\mathrm{O n \, C P U \colon\, \, 5 8. 0 0 m s}
$$

$$
\mathrm{O n} \ \mathrm{G P U \! : \ 1. 0 0 m s}
$$

![](figures/192-5-FIGURE.jpg)

图 $3. 4$ 稀疏矩阵

![](figures/192-7-FIGURE.jpg)

## 执行结果如下：

）转为正常的矩阵格式。程序代码如下： (11)

 $( 1 2 )$ 如果要执行TensorFlow 1.x 版 Graph Execution Moce 的程序，则需禁用（Disable）2.x版的功能，并改变加载框架的命名空间（Namespace) 程序代码如 $\mathrm{F}$ :

TensorFlow改良反而带来了后向兼容性差的困扰，1.x版的程序在2.x版的默认模式下均无法执行，虽然可以把 $2. {\bf x}$ 版的默认模式切换回1.x版的模式，但是要自行修改的地方较多，而且也缺乏未来性，因 $\operatorname{l k b}$ ，在这里建议大家以下几点。

DEager Execution Mode 已是 TensorFlow的主流，不要再使用1.x版的 Session 或TFLearn等旧的架构，套用一句电视剧对白，已经回不去了。

2手上有许多1.×版的程序，如果很重要，非用不可，可利用官网移转（Migration）指南时进行修改，单一文件比较可行，但若是复杂的框架，可能就要花费大量时间了。

| 1 | #转为正常的矩阵格式 |
| 2 | x = tf.sparse.to dense(sparse tensor) |
| 3 | print(type(x)) |
| 4 |  |
| 5 | #2.31 以前版本会出错 |
| 6 | x.numpyO |

## 执行结果如下:

| kclass te | sorflow.python. framework.ops.EagerTensor > |
| array([[1, o, 0,0], |
| [O | 0,2,0], |
| [0, | 0,0, 0]]) |

| 1 | if | tf. version [0] != 1': | # | 是否为 TensorFLow 1.x饭  |
| --- | --- | --- | --- | --- |
| 3 |  | tf.disable v2 behavior() | # | 使2.x 版功能关效(Disable) |

3官网也有提供指令，能够一次升级整个目录的所有程序，如下:

$$
\mathbf{t f}_{-} \mathbf{u p g r a d e}_{-} \mathbf{v} 2 \mathrm{-i n t r e e}.
$$
1.x版程序目录：--outree输出目

(14）GPU内存管理：由于TensorFlow对GPU内存的垃圾回收（Garbage Collection）机制并不完美，因此，常会出现下列 GEMM 错误：

此信息表示GPU内存不足，尤其是使用Jupyter Notebook 时，因为Jupyter Notebook是一个网页程序，关掉某一个
Notebook文件，网站仍然在执行中，所以不代表该文件的资源会被回收，通常要选择Kernel> Restart选项才会回收资源。另一个方法，就是限制GPU的使用配额。以下是TensorFlow 2.x版的方式，1.x版并不适用。程序代码如 $\mathrm{F}$ :

详细使用方法可参阅TensorFlow官网升级指南。

(13）禁用2.x版的功能后，测试1.×版程序，Graph Execution Mode 程序须使用tf.Session。程序代码如下：

| 1 | #测试1.x版程序 |
| 2 | x = tf.constant([[1, 2]]) |
| 3 | neg x= tf.negative(x) |
| 4 |  |
| 5 | with tf.Session() as sess: #使用Session |
| 6 | result = sess.run(neg x |
| 7 | print(result) |

InternalError: Blas GEMM launch failed : a.shape-(32, 784), b.shape=(784, 256), m=32, n-256, k-784
[[node sequential/dense/MatMul (defined at <ipython-input-3-9d42ad511782>:5) ]] [Op: inference train function 581]

| 1 2 | #限/ TensorfLow 只能使用 GPU 2GB 内存 gnus-tf config exnerimental.lict nhvsical devices('GPU'Y |
| 3 | if gpus: |
| 4 | try: |
| 5 | #限制第一个GPU 只能使用26B 内存 |
| 6 | tf.config.experimental.set virtual device configuration(gpus[o], |
| 7 | [tf.config.experimental.VirtualDeviceConfiguration(memory limit=1024*2)]) |
| 8 | 一—让衣衣一 |
| 9 | #显示GPU个数 |
| 10 | logical gpus = tf.config.experimental.list logical devices('GPU') |
| 11 | print(len(gpus), "Physical GPUs,", len(logical gpus), "Logical GPUs") |
| 12 | except RuntimeError as e: |
| 13 | #显示错误信息 |
| 14 | print(e) |

## （15）用户也可以不使用 GPU。程序代码如下：

| import OS |
| os.environ["CUDA VISIBLE DEVICES"] = "-1 |

反向传导时，会更新每一层的权重，这时就会用到偏微分运算，如图3.5所示。所以，深度学习框架的第二项主要功能就是自动微分。

同样地，我们直接以实践代替长篇大论，请参阅03 $2$ 自动微分.ipynb.

## 3-4、自动微分

![](figures/196-3-FIGURE.jpg)

图3.5 神经网络权重求解过程

(1）使用tf.GradientTape()函数可自动微分，再使用 g.gracdient $( \mathbf{y}, \mathbf{x} )$ 可取得y对x作偏微分的梯度。程序代码如下

| 1 | import numpy as np |  |
| 2 | import tensorflow as tf |  |
| 3 |  |  |
| 4 | x= tf.Variable(3.0) | 信明TensorFLow 变量(Variable) |
| 5 |  |  |
| 6 | with tf.GradientTape() a | 自动微分 |
| 7 | 兖 VEx*X  | =x^2 |
| 8  |
| 9 dy dx=g.gradient(y,x) #取得梯度，f'(x)=2x, x=3==>6 |
| 10 |
| 11 | print(dy dx.numpyO) | 专换为NumPy array格式 |

## 执行结果：

（2）声明为变量（fVariable）时，该变量会自动参与自动微 $\acute{\jmath}$ ，但声明为常数（tf.constant）时，如欲参与自动微 $\acute{\jmath}$ ，则需额 $\mathcal{Z}$ 卜设定 $\mathrm{g. w a t c h ( ),}$ 。程序代码如下：

(3）计算二阶导数：使用tf.GradientTape()、g.gradient (y, ${\bf x )}$ 函数两 $\gRspadesuit$ ，即能取得二阶导数。程序代码如下：

$$
\begin{array} {l} {f \ ( \textbf{x} ) \ =\textbf{x}^{2}} \\ {f \ \ ( \textbf{x} ) \ =2 \textbf{x}} \\ {f \ \ ( \textbf{x} ) \ =2 \star3=6 \circ} \\ \end{array}
$$

| 1 | import numpy as np |  |  |
| 2 | import tensorflow as tf |  |  |
| 3 |  |  |  |
| 4 | x= tf.constant(3.0) | # | 吉明 Tensorflow 常数 |
| 5 |  |  |  |
| 6 | with tf.GradientTape() )as | # | 自动微分 |
| 7 | g.watch(x)  | # | ， 设定常数参与自动微分 |
| 8 | 中 V=x*X | # | =x^2 |
| 一 9  |
| 10 dy dx=g.gradient(y,x) #取得梯度,f'(x)=2x,x3==>6 |
| 11 |  |  |  |
| 12 | print(dy dx.numpyO) | # | 转换为 NumPy array格式 |

执行结果：与上面（1）中相同

| 1 | x= tf.constant(3.0) | # | 声明 FTensorFLow 常数 |
| 2 | with tf.GradientTape() as g: | # | 自动微分 |
| 3 | g.watch(x) |  |  |
| 4 | with tf.GradientTape() asg | # | 自动微分 |
| 5 | gg.watch(x) | # | 设定常数参与自动微分 |
| 6 | 一 y=x*X | # | y=x^2 |
| 7  |
| 8 | dy dx=gg.gradient(y,x) | # | 一阶导数 |
| 9 | d2y dx2= g.gradient(dy dx, x) | # | 二阶导数 |
| 10 |
| 11 | print(f'一阶导数={dy dx.numpy()} | 二 | 阶导数={d2y dx2.numpyO}') |

执行结果：一阶导数-6.0，二阶导数 $= 2. 0$ 

$$
f ~ ( 0 ) ~=x^{2}
$$

(4）多变量计算导数：各自使用g.gracdient $( \mathbf{y}, \mathbf{x} )$ 函数， $\overline{{\mathrm{p J}}}$ 取得每一个变量的梯度。若使用g.gradient(两次或以 $\b L$ ，则
tf.Gradientlape()须加参数persistent-True，使t.GradientTape)不会被自动回收，用完之后，可使用「de g」 删除GradientTape对象。程序代码如下：

(5）借此机会我们认识一下PyToroh自动微分的语法，它与 TensorFlow稍有差异。程序代码如下：

$$
\begin{array} {l} {f \ \ ( x ) \ =2 x} \\ {\} \\ {f^{\prime} \ \ ( x ) \ =2} \\ {\} \\ {f^{\prime} \ \ ( 3 ) \ =2 \circ} \\ \end{array}
$$

| 1 | x= tf.Variable(3.0) | #声明 Tensorflow 常数 |
| --- | --- | --- |
| 2 with tf.GradientTape(persistent=True) as g: # 自动微分 |
| 3 | y=x*x  | #y=x^2 |
| 4 | Z=y * y | #z=y^2 |
| 5 |
| 6 | dz dx = g.gradient(z,x) | #4*x^3 |
| 7 | 一口口 dv dx= g.gradient(y.x) | #2*X |
| 一 8  |
| 9 delg #不用时可册除GradientTape 对象 |
| 10  |
| 11 | print(f'dy/dx={dy dx.numpy | , dz/dx={dz dx.numpy(O}') |

执行结果： dy/dx-6, dzix-108

$$
\begin{array} {l} {Z=f \ \ ( \textbf{x} ) \ =y^{2}=x^{4}} \\ {f \ \ ( \textbf{x} ) \ =4 x^{3}} \\ {f \ \ ( 3 ) \ =1 0 8 \,.} \\ \end{array}
$$

$$
z=f ~ \left( x \right) ~=y^{2}=x^{4}
$$

范例，利用TensorFlow 自动微分求解简单线性回归的参数 w、b)

| 1 | import torch | #载人库 |  |
| 2 |  |  |  |
| 3 | x= torch.tensor | (3.0, requires grad=True) ) | ad=True) )#设定x参与自动微分 |
| 4 | y=x*x | #y=x^2 |  |
| 5 |  |  |  |
| 6 | v.backward() | #反向传导 |  |
| 7 |  |  |  |
| 8 | print(x.grad | #取得梯度 |  |

Drecuires grad-True参数声明x参与自动微分

2 $\mathbf{y}$ .backward()，要求做反向传导。 调用

③调月x.grad 取得梯度

程序：请参阅03 3简单线性回归.ipynb。流程如图3.6所示。

![](figures/199-6-FIGURE.jpg)

图 $3. 6$ 程序设计流程

(1）载 $\lambda$ 库。程序代码如下

定义损失函数 $M S E={\frac{\sum( y-\hat{y} )^{2}} {n}} \circ$ 程序代码如下： (2)

(4）定义训练函数：在自动微分中需重新计算损失函数值， assign sub函数相当于 $\Gamma_{-}=\rfloor$ 。程序代码如下：

## (3）定义预测值函数y= wx+b。程序代码如下：

![](figures/200-2-FIGURE.jpg)

(5）产生随机数作为数据集，进行测试。程序代码如下：

| 1 | #产生线性随机数据100龙，介于0-50 |
| 2 | n=100 |
| 3 | X= np.linspace(o, 50,n) |
| 4 | y = np.linspace(o, 50,n) |
| 5 |  |
| 6 | #数据里加一点噪声(noise) |
| 7 | X t= np.random.uniform(-10, 10,n) |
| 8 | y t= np.random.uniform(-10, 10,n) |

## ）执行训练。程序代码如下： (6)

$$
\fbox{\begin{array} {r l} {{1}} & {{w}} & {{b \quad\frac{\sqrt{2} / 2} {2} \left( 2 \right) / 2 \sqrt{2} )} \theta} \\ {{2}} & {{w}} & {{=\mathrm{t f. V a r 1} \left( 2 \right) / 2 \left( 2 \right)} \theta} \\ {{3}} & {{w}} & {{=\mathrm{t f. V a r 1} \left( 2 \right) \left( 0 \right)}} \\ {{4}} & {{}} & {{}} \\ {{4}} & {{\# \left( 2 \pi\right) / 2 \pi}} \\ {{5}} & {{\left( r \left( 2 \pi\right) / 2 \pi}} & {{}} \\ {{6}} & {{\tan\left( X, \right)}} \\ {{8}} & {{\# \left( 2 \right) \left( \left( \frac{\phi} {2 \pi} \right) \left( x \right) \right)}} \\ {{8}} & {{\mathrm{t r} \left( x \right) \left( x \right) \right) \left( x \right) \left( x \right) \left( \frac{x \left( \phi\right) \left( x \right) \left( x \right) \right)} {2}}} \end{array}.
$$

②损失函数值随着训练周期越来越小，如下

$$
\exists: ( 7 ) \exists\exists\pm4 \pm\exists\exists\exists\exists\exists\uparrow\downarrow
$$
确实居于样本点中线。程序代码如

图 $3. 7$ 自动微分求解简单线性回归执行结果

D执行结果：w-0.9464，b-0.0326.

![](figures/201-4-FIGURE.jpg)

$$
\mathrm{T} \! :
$$

1 import matplotlib.pyplot as plt
2
3 plt.scatter(X, $\mathtt{y}$ ,label='data'
4 plt.plot(X, predict(X), label='predicted') 5 plt.legend()

执行结果：如图3.7所示

![](figures/201-8-FIGURE.jpg)

有 $\overline{{\jmath}}$ TensorFlow自动微分的功能，正向与反向传导变得非常简单，只要熟悉了运作的架构，后续复杂的模型就可以运用自如。

上一节运用自动微分实现 $7-$ 条简单线性回归线的求解，然而神经网络是多条回归线的组合，并且每一条回归线可能再乘上非线性的 Activation Function，假如使用自动微分函数逐一定义每条公式，层层串连，程序可能要很多个循环才能完成。所以为了简化程序开发的复杂度，TensorFlow/Keras直接建构了各式各样的神经层函数，可以使用神经层组合神经网络的结构，用户只需要专注算法的设计即可，轻松不少。

如图3.8所示，神经网络是多个神经层组合而成的，包括输入 $\underleftarrow{\Xi}$ (InoutLa $\mathrm{y e r} )$ 隐藏层（Hidden La $\mathrm{y e r} )$ 及输出层（Output La $\mathrm{y e r} )$ ，其中隐藏/层可以有任意多层。一般而言，隐藏层大于或等于两 $\beta$ ，即称为深度学习。

TensorFlow/Keras提供了数十种神经层，分成以下类别，用户
可参阅 Keras 官网说明
$$
\mathrm{( h t t p s : / k e r a s. i o / a p i / l a y e r s / )}
$$

## 3-5神经网络层

![](figures/203-4-FIGURE.jpg)

 $3. 8$ 神经网络示意图冬

(5）前置处理层（Preprocessing layern 提供 One-Hot Encoding、影像前置处理、 、（Data Augmentation）等。
数据增补

(2）建立模型：神经网络仅使用一个完全连接层，而且输入 $\P$ 有一个神经 $\overline{{\Pi}}$ ，即X，输出也只有一个神经元，即 $y_{\circ}$ Dense本身有一个参数use bias，即是否有偏差项，默认值为True，除 $7-$ 个神经元输出外，还会有一个偏差项。以上设定其实就等于

(1)核 $\grave{\imath} \grave{\mathrm{L}} \imath$ 类别（CoreLayer 包括完全连接层（Ful
Connected La $\mathrm{y e r} )$ 激励神线
$$
\b\# \b\# \mathbf{\Sigma} \mathrm{\ ( A c t i v a t i o n \ l a y e r )}
$$
嵌 $\lambda\equiv$ 
 $\mathrm{l a y e r} )$ 等。
(Embedding

$$
( 2 ) \textit{\# x h} \equiv\mathrm{( C o n v o l u t i o n a l \, L a y e r )} \; \;_{\circ}
$$

$$
( 3 ) \, \, \, \, \not\Pi\ll\mathbb{E} \, \, \, ( \mathrm{P o o l i n g \, L a y e r} ) \, \, \, \, \circ
$$

$$
( 4 ) \, \setminus\equiv\mp\mp\mp\mp\mp\mathrm{( R e c u r r e n t \, L a y e r )} \; \; \circ
$$

我们先来看看两个最简单的完全连接层范例。

范例1。使用完全连接层估算简单线性回归的参数（w、b) O

程序：请参阅03 $\boldsymbol{4}$ 简单的完全连接层.pynb

(1）产生随机数据，与上一节范例相同。程序代码如下

1#载人库
2 import numpy as np
3 import tensorflow as tf
4
5 #产生线性随机数据1 $e \theta$ 批·介于0-50
6
$$
\begin{array} {l} {{n \;=\; 1 0 0}} \\ {{\times\;=\; n p. 1 1 n s p a c e \, ( 0 \,, \; \; 5 0 \,, \; \; n )}} \\ {{y \;=\; n p. 1 1 n s p a c e \, ( 0 \,, \; \; 5 0 \,, \; \; n )}} \end{array}
$$
7
8
9
10#数据中加一点噪声
11 X += np.random.uniform(-10, 10, n) 12 y += np.random.uniform(-10, 10, n) $y_{=} w x+b_{c}$ 、为聚焦概念的说明，暂时不解释其他参数，在后面章节会有详尽说明。程序代码如下

(4）模型训练：只需一个指令model.fit $( \mathrm{X, y} )$ 即可，训练过程的损失函数变化都会存在history变量中。程序代码如下：

执行结呆：损失函数值随着训练周期越来越 $\imath\rfloor$ ，如图3.9所示。

![](figures/205-3-FIGURE.jpg)

## (3）定义模型的损失函数及优化器。程序代码如下

1#定义模型的损失函数为MSE，优化器为Adam
2 model.compile(loss='mean squared error
3 optimizer=tf.keras.optimizers.Adam())

 history = model.fit(X, y, epochs=500, verbose=False)

(5）训练过程绘图。程序代码如下：

1 import matplotlib.pyplot as plt
2 plt.rcParams['font.sans-serif' $]=[$ 'Microsoft JhengHei'] 3 plt.rcParams['axes.unicode minus'] = False
5 plt.xlabel('训练周期'，fontsize=20)
6 plt.ylabel("损失函数"，fontsize=20)
7 plt.plot(history.history['loss'])

![](figures/205-9-FIGURE.jpg)

(6）取得模型参数w为第一层的第一个参数，b为输出层的第一个参数。程序代码如下：

执行结果： $\mathbf{W}$ ： 0.8798， b： 3.5052，因输入数据为随机随机数。

与自动微分比较，这种方法程序更简单，只要设定模型结构、 损失函数、优化器 $\sqrt{\Xi}$ ，呼叫训练（fit）函数即可。

下面我们再看一个有趣的例子，利用神经网络自动求出华氏与摄氏温度的换算 $\big< \searrow$ 式。

 $3. 9$ 训练过程执行结果图

![](figures/206-5-FIGURE.jpg)

(7）绘图显示回归线。程序代码如下：

1 import matplotlib.pyplot as plt
2
3 plt.scatter(X, $\mathtt{y}$ ,label='data'
4 plt.plot $( X, \; X \;^{*} \; w \;+\; b \,, \;^{\prime} \; r \;^{\prime}$ , label='predicted') 5 plt.legend()

执行结果：如图3.10所示

![](figures/206-9-FIGURE.jpg)

图 $3. 1 0$ !显示回归线执行结果

(2）建立模型：神经网络只有一个完全连接层，而且输入 $\P$ 有一个神经元，即摄氏温度，输出只有一个神经元，即华氏温度。 程序代码如下：

执行结果：如图3.11所示。由此可见：损失函数值随着训练周期越来越小。

## 范例 2。使用完全连接层推算华氏与摄氏温度的换算公式

华氏（F 摄氏（0)*（9/5）+32

）利用换算公式，随机产生151个数据。程序代码如下： (1)

![](figures/207-5-FIGURE.jpg)

![](figures/207-6-FIGURE.jpg)

## (3）模型训练。程序代码如下

![](figures/207-8-FIGURE.jpg)

(4）训练过程绘图。程序代码如下：

![](figures/207-10-FIGURE.jpg)

(5）测试：输入摄氏100度及0度转换为华氏温度，答案完全正确。程序代码如 $\mathrm{F}$ :

①执行结果： $\mathbf{W}$ ： 1.8000，b： 31.9999，近似于华氏与摄氏温度的换算 $\big< \searrow$ 式。

![](figures/208-2-FIGURE.jpg)

图3.11训练过程执行结果

1 ypred = model.predict([100.0])[0][0]
2 print(f"华氏(F）：{y_pred:.2f}，摄氏(C）：100"） 3
4 y_pred = model.predict $( [ 0, 0 ] ) [ 0 ] [ 0 ]$  $( C ) : \emptyset^{n} )$ 5 print(f"华氏(F）：{y_pred:.2f}，摄氏

$$
\pm\pm\pm\pm\pm\pm\pm
$$

华氏（P 212.00，摄氏（O) 100

华氏（F) 32.00，摄氏（0) 0

(6）取得模型参数 $\boldsymbol{w}.$  $b_{\circ}$ 

| W= layer1.get t weights()[0][0][0] |
| b= layer1.get weights()[1][0] |
| print(f"w:{w:.4f} , b:{b:.4f}") |

D其实换算公式也是一条回归线读到这里，读者应该会好奇如何使用更多的神经元和神经层， 甚至更复杂的神经网络结构，下一章我们将正式迈入深度学习的殿堂，学习如何用TensorFlow解决各种实际的案例，并且详细剖析各个函数的用法及参数说明。

接下来，我们将开始以神经网络实践各种应用，可以暂时与数学/统计说再见，我们会着重于概念的澄清与程序的撰写。笔者会尽可能地运用大量图解，帮助读者迅速掌握各种算法的原理。

同时笔者也会借中「手写阿拉伯数字辨识」的案例，实践机器学习流程的十大步骤，并详细解说构建神经网络的函数用法及各项参数代表的意 $\AA$ ，最后我们会撰写一个完整的窗口接口程序及网页程序，让终端用户（End User）亲身体验 $\mathbf{A} \mathbf{I}$ 应用程序，期望激发用户对企业导入 $\mathbf{A} \mathbf{I}$ 有更多的体验。

## 第4章

## 神经网络实践

(1）读取手写阿拉伯数字的影像，将影像中的每个像素当成一个特征。数据源为 MNIST机构所收集的60000个训练数据，另含10000个测试数据，每个数据是一个阿拉伯数字、宽高为（28 28）的位图形。

(2）建立神经网络模型，利用梯度下降法，求解模型的参数值，一般称为权重。

 $( 3 )$ 依照模型推算每一个影像是0~9的概率，再以最大概率者为预测结果。

## 4-1 -个神经网络程序撰写第

手写阿拉伯数字辨识，如图4.1所示，问题定义如下。

![](figures/211-5-FIGURE.jpg)

图4.1手写阿拉伯数字辨识

(ComputationalGraph）的概念来编写，光是要将两个张量相加就要撰写一大段程序，被PyTorch 比了下去，于是TensorFlow 2.x 为了回击对手，官网直接在文件首页展 $\pi\pi-$ 个超短程序，示范如何撰写手写阿拉伯数字的辨识，要证明改版后的TensorFlow确实更好用，现在我们就来看看这支程序。

## 4-1-1最简短的程序

## TensorFlow 1.x 版使用会话（Session）及运算图

范例.TensorFlow官网的手写阿拉伯数字辨识

程序： 手写阿拉伯数字辨识.ipynb。程序代码如下：
04_01

![](figures/212-5-FIGURE.jpg)

## 执行结果如下：

| Epoch 1/5 |
| 1500/1500 | ======== ======] | -5s | 3ms/step | -loss: | 0.5336 - | -accuracy: | 0.8432 | val loss: | 0.1558 | - val accuracy: |
| 0.9555 |  |  |  |  |  |  |  |  |  |  |
| Epoch 2/5 |
| 1500/1500 | ==============================] | -5s | 3ms/step | -loss: | 0.1676 - | -accuracy: | 0.9505 | val loss: | 0.1134 | - val accuracy: |
| 0.9657 |  |  |  |  |  |  |  |  |  |  |
| Epoch 3/5 |  |  |  |  |  |  |  |  |  |  |
| 1500/1500 | =ssssssssssssssssssssssssss=s=] | -5s | 3ms/step | - loss: | 0.1203 - | -accuracy: | 0.9646 | val loss: | 0.0975 | - val accuracy: |
| 一L 0.9704  |  |  |  |  |  |  |  |  |  |  |
| Epoch 4/5 |  |  |  |  |  |  |  |  |  |  |
| 1500/1500 | ====== ====] | - 5s | 3ms/step | - loss: | 0.0981 | -accuracy: | 0.9703 | val loss: | 0.0968 | - val accuracy: |
| 0.9717 |  |  |  |  |  |  |  |  |  |  |
| Epoch 5/5 |  |  |  |  |  |  |  |  |  |  |
| 1500/1500 | ==========: ===========] | - 5s | 3ms/step | -loss: | 0.0786 - | -accuracy: | 0.9758 | val loss: | 0.0958 | - val accuracy: |
| 0.9713 |  |  |  |  |  |  |  |  |  |  |
| 313/313[== | =========: :===]$- | 1s 3 | ms/step- | loss:0 | 0807 - 2 | accuracy:o | .9752 |  |  |  |
| T0.08072374 | 016046524,0.97519999742507931 |  |  |  |  |  |  |  |  |  |

上述的程序除去批注，仅 $1 0$ 金辨识的准确率高达
多行，
 $9 7 \% \sim9 8 \%$ ，TensorFlow 成功实现了超越!

上一节的范例「手写阿拉伯数字辨识」是官网为了炫技刻意缩短了程序，本节将会按照机器学习流程的十大步骤（见图4.2)
撰写完整程序，并对每个步骤仔细解析，读者务必理解每一行程序背后代表的内涵。

(1）步骤 $1$ ：加载MNIST手写阿拉伯数字数据集。程序代码如下：

## 4-1-2程序强化

![](figures/214-3-FIGURE.jpg)

图4.2、机器学习流程十大步骤

范例依据上图十大步骤撰写手写阿拉伯数字辨识。

程序： 02三写阿拉伯数字辨识_完整版ipynb 04 5

执行结果：取得60000个训练数据 1000个测试数据，每个数据是一个阿拉伯数字，宽高各为（28,28）的位图形，要注意数据的维度及其大小必须与模型的输入规格契合。执行结果如 $\mathrm{F}$ :

(2）步骤 $2$ ：EDA，对数据集进行探索与分析，首先观察训练数据的目标值（ 即影像的真实结果。程序代码如下：

执行结果如下，每个像素的值在 $( 0, 2 5 5 )$ 之间，为灰阶影像，0为白色，255为最深的黑色。注意：这与RGB 色码刚好相 $\varpi$ ，RGB 中黑色为 $\mathbf{O}$ ，白色为255。

$$
\fbox{( 6 0 0 0 0, \; \; 2 8 \,, \; \; 2 8 ) \; \; ( 6 0 0 0 0 \,, \, ) \; \; ( 1 0 0 0 0 \,, \; \; 2 8 \,, \; \; 2 8 ) \; \; ( 1 0 0 0 0 \,, \, )}
$$

 $1$ #训练数据集前10张图片的数字
 $2$ 
$$
y_{-} \mathrm{t r a i n} [ : 1 0 ]
$$

，每个数据是一个阿拉伯数字。 执行结果如下，

array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)

） 个训练数据的像素。程序代码如下： (3）打印第一

1#显示第1张图片像素数据 $2$ | $x \_{\tan[ 0 ]}$ 

| [ | 0, | 0, | 0, | 0, | 0, | 0, | 0, | 0, | 0, | 0, | 0, | 0, | 0 | 0, 0 | 0, 01 | 0, | 0, | 0, | 0, | 0, | 0, | 0, | 0, | 0, | 0, | 0, |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  | 3 0. | 5JJ 0. | 0. | 0 | 0. | 0. | 0. | 0 | 0. | 0. | 0. | 0. | 3 |
|  | J 18 | J 18 | J 19 |  ） 126 |  J 136 |  ） 175. | ） 26 |  J 166 |  ， 255 |  ， 247 |  ， 127 | ） 0 | J 0 |
|  | +） 0. | +J 01. | 一J | 一） | 一3 | ） | ←5 | +） | 一） | 53 | ） | ） | J |
|  | , 0. | JJ 0. | 0 | 0. | 0. | 0 | 0 | 0. | 20 | 36. | QA | 154 | ^ 170 |
| 20 | J 53 |  -， 253 1 |  J 253 |  ） 253. |  ， 253 |  J 225 |  ） 172 |  ） 253 |  3 242 |  J 195 | 5 64. | 一 0. | - +3 0 |
| 一 | 0. | 一 01. | 一 | 一 | 一 | 一 | 一 | 一 | 一 | 一 |  |  | 3 |
|  | 0, | 0, | 0, | 0, | 0, | 0, | 0, | 49， | 238， | 253， | 253， | 253， | 253, |
| 2 | 53, | 253， | 253， | 253, | 251, | 93, | 82， | 82, | 56， | 39， | 0, | 0, | 0, |
|  | 0, | 0], |  |  |  |  |  |  |  |  |  |  |  |
|  | 0, | 0, | 0, | 0, | 0, | 0, | 0, | 18， | 219， | 253, | 253， | 253， | 253 |
| 2! | 53, | 198， | 182, | 247, | 241, | 0, | 0, | 0, | 0, | 0, | 0, | 0, | 0, |
|  | 0, | 0], |  |  |  |  |  |  |  |  |  |  |  |
|  | 0, | 0, | 0, | 0, | 0, | 0, | 0, | 0, | 80， | 156， | 107， | 253， | 253, |
| 20 | 05, | 11, | 0, | 43, | 154, | 0, | 0, | 0, | 0, | 0, | 0, | 0, | 0, |
|  | 0, | 0], |  |  |  |  |  |  |  |  |  |  |  |

(4）为了看清楚图片的手写的数字，将非 $\mathbf{0}$ 的数值转为 $1$ ，变为黑白两色的图片。程序代码如下

执行结果如下，笔者以笔描绘为 $1$ 的范围，隐约可以看出是 $5 \ {}_{\circ}$ 

| 1 | #将非0的数字转为1，显示第1张图片 |
| 2 | data =x train[o].copy() |
| 3 | data[data>o]=1 |
| 4 |  |
| 5 | #将转换后二维内容显示出来，隐约可以看出数字为5 |
| 6 | text image=rT |
| 7 | for i in range(data.shapefol): |
| 8 | text image.append(''.join(str(data[il))) |
| 9 | text image |

$$
5_{\circ}
$$

|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  | 2 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  | 1 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
|  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |
| 0 |  |  | 0 | 0 | 0 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | V | C | ) | 00 | 0 |

(5）显示第一个训练数据图像，确认是 $5$ 。程序代码如下
O

1#显示第1张图片图像
2 import matplotlib.pyplot as plt
3
4 #第一批数据
5
$$
\times2 \,=\, x_{-} {\mathrm{t r a i n}} [ \, 0, :, : \, ]
$$
6
7#绘制点阵图 $\prime\ c m a p {=} \^{\prime} g r a y$ ：灰阶
8 plt.imshow(X2.reshape(28,28), cmap='gray') 9
10 #隐藏刻度
11 plt.axis('off')
12
13#显示图形
14 plt.show()

(6）步骤 $3$ ：进行特征二程，将特征缩放至（0，1）区间，特征缩放可提高模型准确度，并且可以加快收敛速度。特征缩放采用正态化（Normalization）公式

(7）步骤 $4$ ：数据分割为训练及测试数据，此步骤无须进行， 因为加载 MNIST数据时，数据已经切割好了

执行结果如下：

![](figures/217-3-FIGURE.jpg)

## (x样本最小值）/（样本最大值-样本最小值）

2#颜色范国：0~255所以，公司商作有 $x ~ / ~ 2 5 5$  $z a t i o n ) \; \cdot\; \angle\exists\exists\exists\exists\exists\exists\exists\exists\exists\ Z \span( x \ -\ m i n ) \; \; / \; \; \left( m a x \ -\ m i n \right)$ 1#特征缩放，使用正态化(Normai
3#注意，颜色0为白色，与RGB颜色不局， $( \theta, \theta, \theta)$ 为黑色·
4 x train norm, x test norm = x train $\slash$ 255.0, $\mathsf{x}$ test $\slash$ 255.0
5 x_train norm[0]

## 执行结果如下：

| [0. | 0. | 0 | 0 | 0 |  |
| --- | --- | --- | --- | --- | --- |
| 0. | 0. | 0. | 0 | 3 0 |  |
| 0. | 0. | 0.0039 | 157, 0.0039 | 7, 0.00392157 |
| 0.003921 | 0.0039215 | 0.0039 | .57， 0.0039 | 7, 0.00392157 |
| 0.003921 | 0.0039215 | C 0.0039 | 157,0.0039 | 7,0. |
| 0. | 0. | 0. | ], |  |  |
| [0. | 0. | 0. | y 0 |  |  |
| 0. | 0. | 0. | 0.0039 | 7, 0.00392157 |
| 0.003921 | 0.0039215 | 0.0039 | 157, 0.0039 | 7, 0.00392157 |
| 0.003921 | 0.003921 | 0.0039 | 157， 0.0039 | 7,0.00392157 |
| 0.003921 | C 0.0039215 | 0.0039 | 157,0.0039 | 7,0. |
| 0. | 0. | . |  |  |

(8）步骤 $5$ ：建立模型结构如图4.3所示

Keras提供两类模型，包括顺序型模型（Seouential Model) $\mathcal{R}$ Functional API模型。顺序型模型函数为
tf.keras.mocdels.Sequential，适用于简单的结构，神经层一层接一层地顺序执行；使用Functional API可以设计较复杂的模型结构， 包括多个输入层或多个输出层，也允许分叉，后续用到时再详细说明。这里使用简单的顺序型模型，内含各种神经层。程序代码如下:

Keras 提供两类模型，包括顺序型模型（Sequential Model) 及Functional AP 模型。顺序型模型函数为

扁平层（Flatten $\mathrm{L a y e r} )$ ：将宽高各28像素的图压扁成一维数组 $\mathrm{( 2 8 \times2 8=7 8 4}$ 个特征

②完全连接层（Dense Layer）：输入为上一层的输出，输出为128个神经 $\overline{{\pi}}$ ，即构成128条回归线，每一条回归线有784个特征。输出通常定为 $4$ 的倍数，并无建议值，可经由实验调校取得较佳的参数值。

![](figures/218-4-FIGURE.jpg)

图4.3手写阿拉伯数字辨识的模型结构

![](figures/218-6-FIGURE.jpg)

SDropoutLa $\mathbf{y} \mathbf{e r}$ ：类似于正则化（Regularization），希望避免过度拟 $\overbrace{\Pi}$ ，在训练周期随机丢弃一定比例（0.2）的神经 $\overline{{\Pi}}$ ，一方面可以估计较少的参数，另一方面能够取得多个模型的均值，避免受极端值影响，借以矫正过度拟合的现象。通常会在每 $- \pi$ Dense 后面加一个Dropout，比例也无建议值，如图4.4所示。

④第二个完全连接层：为输出层，因为要辨识0~9这十个数字，故输出要设成 $1 0$ ，透过Softmax Activation Function，可以将输出转为概率形式，即预测0~9的个别概率，再从中选择最大概率者为预测值。

(9）编译指令（model.compile）需设定参数，优化器为 Adam，损失函数为 sparse categorical crossentropy（交叉
熵) 而非 $\mathit{M S E}$ ，相关参数后面章节会详细说明。程序代码如下：

(10）步骤 $6$ ：结合训练数据及模型结构，进行模型训练。程序代码如下:

![](figures/219-4-FIGURE.jpg)

 $4$ 4神经网络图

## a）标准神经网络；（b）丢弃一定比例的神经元后

![](figures/219-7-FIGURE.jpg)

Dvalidation split：将训练数据切割一部分为验证数据，目前设为 $0. 2$ ，即验证数据占 $2 0 \mathcal{H}_{o}$ ，在训练过程中，会用验证数据计算准确度及损失函数值，确认训练过程有无异常。

2epochs：设定训练要执行的周期数，所有训练数据经过一次正向和反向传导，称为一个执行周期。

③执行结果如下，每一个执行周期都包含训练的损失、准确率及验证数据的损失（valloss) 准确率（val acuracy 这些信息都会存储在history变量内，为一字典（dict）数据类型。

执行结果：随着执行周期次数的增加，准确率越来越高，且验证数据与训练数据的准确率应趋于一致，若 $\pi-$ 致或准确率过低, 就要检查每个环节是否出错，如图4.5所示。

## 11）对训练过程的准确率绘图。程序代码如下

1#对训练过程的准确率绘图
2 plt.rcParams['font.sans-serif' $]=[$ 'Microsoft JhengHei']
3 plt.rcParams['axes.unicode minus $]=F a I s e$ 
4
5 plt.figure(figsize=(8, 6))
6 plt.plot(history.history['accuracy $], ~^{1} r^{1}$ ，label="训练准确率）
7 plt.plot(history.history['val accuracy'], $g^{\prime}$ ，label='验证准确率'） 8 plt.legend()

![](figures/221-0-FIGURE.jpg)

执行结果：随着执行周期次数的增加，损失越来越低，验证数据与训练数据的损失应趋于一致，如图4.6所示。

 $( 1 3 )$ 步骤 $7$ ：评分（Score Model 使用evaluate()函数输入测试数据，会计算出损失及准确率。程序代码如下：

![](figures/221-3-FIGURE.jpg)

图4.5对训练过程绘图执行结果

12）对训练过程的损失绘图。程序代码如下

![](figures/221-6-FIGURE.jpg)

 $4. 6$ 训练过程损失绘图执行结果冬

(14) 面 $2 0 \uparrow$ ，使用predict classes()函
实际比对测试数据的前
数，可以得到预测类别。程序代码如 $\mathrm{F}$ :

(15）显示第 $9$ 个的概率：使用predict()函数，可以得到0~9 预测概率各自的值。

执行结果：发现 $5$ 及 $6$ 的概率很相近，表示模型并不很肯定。 所以，实际上，我们可以提高门槛，规定概率须超过规定的下限如 $0. 8$ ，才算是辨识成功，以提高可信度，避免模棱两可的预测执行结果：loss 为0.0833 0.9743.

![](figures/222-4-FIGURE.jpg)

执行结果如下，执行结果全部正确

![](figures/222-6-FIGURE.jpg)

![](figures/222-7-FIGURE.jpg)

o~9预测机率：[o. 0. 0. 0. 0. 0.59 0.41 0. 0. ]

第 $9$ E 像 $5$ 又像 $6_{\circ}$ 张图像如下

(16）步骤 $8$ ：效果评估，暂不进行，之后可调校相关超参数 (Hyperparameter）及模型结构，寻找最佳模型和参数。超参数是指在模型训练前可以调整的参数，如学习率、执行周期、权重初始值、训练批量等，但不含模型求算的参数如权重或偏差。

 $( 1 7 )$ 步骤 $9$ ：模型部署，将最佳模型存盘，再开发用户接口或提供 $\mathrm{A P I}$ ，连同模型文件一并部署到上线环境（Production Environment 程序代码如下：

(18）步骤10：接收新数据预测，之前都是使用 MNIST内建数据测试，严格说并不可靠，因为这些都是出自同一机构所收集的数据，因此，建议读者自己利用绘图软件亲自撰写测试。我们准备一些图文件，放在myDigits目录内，读者可自行修改，再利用下列程序代码测试，注意，从图文件读入影像后要反转颜色，颜色 $\mathbf{0}$ 为白色，与RGB色码不同，RGB色码中 $0$ 为黑色。程序代码如下：

![](figures/223-3-FIGURE.jpg)

![](figures/223-4-FIGURE.jpg)

（19）使用下列指令显示模型汇总作 $\pm\sharp\mathrm{\# ~ ( s u m m a r y )}$ 程序代码如下：

个执行结果：执行结果如下，包括每一神经层的名称及输出参数的个数。

②计算参数个数：举例来说dense 5，输出参数为1290，意思是共有 $1 0$ 条回归线，每一条回归线都有128个特征对应的权重 （w）与一个偏差项 $( b )$ ，所以总共有10x $( 1 2 8+1 ) ~=1 2 9 0 \uparrow$ 参数。

![](figures/224-3-FIGURE.jpg)

| 1 | 示模型的汇总信息 |
| 2 | l. summaryO |

| Model: "sequential2" |  |  |
| Layer (type) | Output Shape | Paramt |
| flatten 2 (Flatten) | (None,784) | 0 |
| dense 4 (Dense) | (None,128) | 100480 |
| Total params: 101,770 Trainable params: 101,770 Non-trainable params: o |  |  |

(20）绘制图形，显示模型结构：要绘制图形显示模型结构需先完成以下步骤，才能顺利绘制图形

D安装graphviz软件，网址为
https:/www.graphviz.org/download，再把安装目录下的 bin 路径加到环境变量Path .

以上我们按机器学习流程的十大步骤撰写 $\Im-$ 支完整的程序， 虽然篇幅很长，读者应该还是有些疑问，许多针对细节的描述，将 ②安装两个库: $\mathrm{\ p i p}$ installgrahviz ydotolus.

1 tf.keras.utils.plot model(model, to file='model.png')

③执行plot_model指令，可以同时显示图形和存盘。 执行结果：如图 $4. 7$ 所示

3执行plot odel指令，可以同时显示图形和存盘

执行结果： 4.7所示。
如图

![](figures/225-8-FIGURE.jpg)

图 $4. 7$ 绘制图形显示模型结构

于 $\mathrm{F-}$ 节登场，我们会做些实验来说明建构模型的考虑，同时解答教学现场同学们常提出的问题。

前一节我们完成了第一个深度学习的程序，也见识到了它的作用，扣除说明，短短十几行的程序就能够辨识手写阿拉伯数字，且准确率达到 $9 7 \%$ 。然而，仔细思考后我们会产生许多疑问。

(1）模型结构为什么要设计成两层Dense？更多层准确率会提高吗？

(2）第一层Dense输出为什么要设为128？设为其他值会有何影响？

(3）目前第 $- \pi$ 的Activation Function 设为relu，代
Dense
表什么意义？设为其他值又会有何不同?

(4）优化器、损失函数、效果衡量指标有哪些选择？设为其他值会有何影响？

## 4-1-3实验

(5） Dropout比例为 $0. 2$ ，设为其他值会更好吗？

(6）影像为单色灰阶，若是彩色可以辨识吗？怎么修改？

(7）目前执行周期设为 $5$ ，设为其他值会更好吗？

(8）准确率可以达到 $1 0 0 \%$ ，以便企业安心导入吗？

(9）如果要辨识其他对象，程序要修改哪些地方？

10）如果要辨识多个数字，如输入4位数，要如何辨识？ 以上问题是这几年授课时学员常提出的疑惑，我们就来逐一实验，试着导找答案

问题1：模型结构为什么要设计成两层Dense？更多层准确率会提高吗？

(1）前面曾经说过，神经网络是多条回归线的组合，而且每一条回归线可能还会包含在Activation Function内，变成非线性的函数，因比，要单纯以数学方法求解几乎不可能，只能以优化方法求得近似解，但是，只有凸集合的数据集，才保证有全局最佳解 $\mathsf{K}$ Global Minimization），以MNIST为例，总共有784维特征，即 784度空间，根本无法知道它是否为凸集合，因此严格来讲，到目前为 $\lfloor\b L$ ，神经网络依然是一个黑箱（Black Box）科学，我们只知道它威力强大，但如何实现较佳的准确率，依旧需要经验与实验， 因此，模型结构并没有明确规定要设计成几层，会随着不同的问题及数据来进行测试，case by case 进行效果调校，寻找较佳的参数值。

（2）理论上，越多层架构，回归线就越多，预测应当越准确，如ResNet模型就高达150)层，但是，经过实验证实，超过某一界限后，准确率可能会不升反降，这与训练数据量有关，如果 $\P$ 有少量的数据，要估算过多的参数 $( w, ~ b )$ 自然准确率不高。
，

(3）我们就来实验一下，多一层Dense，准确率是否会提高 $\d>$ 请参阅程序04 03_手写阿拉伯数字辨 $\grave{\imath} \mathbb{R}$ 实验L.ipynb (1希望了解更详细的相关信息，有哪些资源可以参阁？

## 解笞

(4）修改模型结构如下，加一对Dense/Dropout，其余程序代码不变。程序代码如下：

问题 $2$ .第 $- \pi$ Dense输出为什么要设为128？设为其他值会有何影响？

(1）输出的神经元个数可以任意设定，一般来讲，会使用 $4$ 的倍数，以下我们修改为256，请参阅程序04-04_手写阿拉伯数字辨识_实验2.ipynb。程序代码如下：

| 1 | #建文模型 |
| --- | --- |
| 2 | model = tf.keras.models.Sequential([ |
| 3 | tf.keras.layers.Flatten(input shape=(28, 28)）, |
| 4 | tf.keras.layers.Dense(128, activation='relu'). |
| 5 | tf.keras.layers.Dropout(0.2), |
| 6 | tf.keras.layers.Dense(64, activation='relu'). |
| 7 | tf.keras.layers.Dropout(o.2), |
| 8 | tf.keras.layers.Dense(10, activation='softmax') |
| 9 | 1) |

执行结果如下，准确率未见提升，反而微降

$$
\operatorname{l o s s} \colon0. 0 8 4 0
$$

$$
\mathrm{a c c u r a c y :} \ 0. 9 7 3 3
$$

$$
\hat{\Pi}_{\mathrm{H}}^{\mathrm{T}}=\hat{\Sigma} :
$$

![](figures/229-8-FIGURE.jpg)

执行结果如下，准确率略微提高，但不明显。

$$
\mathrm{l o s s} \! : \; 0. 0 7 7 5
$$

$$
\mathrm{a c c u r a c y \! :} \ 0. 9 7 6 4
$$

(2）同问题 $1$ ，照理来说，神经元个数越多，回归线就越多特征也越多，预测应该会越准确，但经过验证，准确率并未显著提高。依DeepLearning with TensorFlow 2.0 and Keras-书测试如图 4.8所示，也是有一个极限，超过这个极限准确率就会不升反降。

问题 $3$ .目前第 $- \imath_{\Xi}$ 的Activation Function 设为relu,
Dense
代表什么意义？设为其他值会有何不同?

Activation Function 有很多种，后面会有详尽介绍，读者可先参阅维基百科"，部分表格撷取见表4.1。其中包括函数的名称、概率分布图形、公式及一阶导数：

![](figures/230-3-FIGURE.jpg)

冬 $4. 8$ 准确率测试

）神经元个数越多，训练时间就越长，如图4.9所示 (3)

![](figures/230-6-FIGURE.jpg)

图4.9 训练时间

$$
\S=\Xi: \qquad\exists\mp\Xi: \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\Xi=\Xi
$$

## 表4.1部分Ativation Function 早期隐藏层大都使用 sigmoid函数，近几年发现 relu准确率较高，因比我们先尝试比较这两种函数，请参阅程序04 05_手写阿拉伯数字辨识_实验3.ipynb.

页 $4$ .优化器、损失函数、效果衡量指标有哪些选择？设为
问题
其他值会有何影响?

| Name $ ldentity  |  | Plot | Function, f( |  | Derivative | of f, f'(x) |
|  |  | c |  | 1 |  |
| Binary step |  |  | 0 ifc < 0 ifa >0 |  | fo undefined | if c f 0 if c = 0 |
| Logistic, sigmoid, or soft step |  |  | o(x)= 1 [1] 1+e- |  | f(c)(1- f( | ) |
| tanh |  |  | tanh(c)= e" -e-e" +e- |  | 1- f()' |  |
| Rectified linear unit ReLUl1 |  |  |  0 ifa < 0 ac ifc > 0 = max{0,:}=1 |  | 0 1 undefined | ifc < 0 ife > 0 if a = 0 |
| Gaussian error linear unit (GELU), |  |  | 1 C 2 C = w$(z)  |  | $(x)+@() |  |
| Sofpusl12] |  |  | ln(1+ c") |  |  1 1Le-2 |  |

(1）将relu $\mathrm{s i g m o i d}$ 。程序代码如下：
改为

1#建立模型
2 model = tf.keras.models.Sequential([
3 tf.keras.layers.Flatten(input shape=(28, 28)), 4 tf.keras.layers.Dense(128, activation='relu'), 5 tf.keras.layers.Dropout $( 0. 2 )$ ,
6 tf.keras.layers.Dense $1 0$ , activation='sigmoid') 7 ])

(2 . sigmoid准确率确实略低于relu 执行结果如下，

$$
\operatorname{l o s s} \colon0. 0 8 4 7
$$

$$
\mathrm{a c c u r a c y :} ~ 0. 9 7 6 2
$$

$$
\dag\mp\pm\pm\pm
$$

(1）优化器有很多种， ${\cal M}$ 最简单的固定值的学习※
(SGD) 到很复杂的动态改变的学习率，甚至是能够自定义优化器。请参考参考文献[8]或[9]。优化器的选择，主要会影响收敛的速度，大多数状况 $\mathrm{T}$ , 优化器都有不错的表现。
Adam

(2）损失函数也种类繁多，包括常见的MSE、Entropy，其他更多的损失函数请参考参考文献[10]或11]。损失函数的选择，主要也是影响着收敛的速度，另外，某些自定义损失函数有特殊功能， 如风格转换（Style Transier），它能够制作影像合成的效果，生成对抗网络 $( \mathrm{G A N} )$ ，后面章节会有详细的介绍。

(3）效果衡量指标：除了准确率，还可以计算精确率
(Precision）、召回率（Recall）、F1等，也可以同时设定多个效果衡量指标，请参考参考文献[12]，如下面程序代码所示，完整程序可参阅程序04 06 手写阿拉伯数字辨识实验4.ipynb

①注意：设定多个效果衡量指标时，准确率请不要使用 Accuracy，否则数值会非常低；而需使用 CategoricalAccuracy 表示分类的准确率，而非回归的准确率。

②执行结果如下

$$
\operatorname{l o s s} \colon\, 0. 0 7 5 7
$$

categorical accuracy: 0.9781

$$
\mathrm{p r e c i s i o n}_{-} 3 \colon\ 0. 9 8 1 0
$$

若Dropout比例为 0.1，我们测试看看，请参阅程序04 07手写阿拉伯数字辨识_实验 $5. \mathbf{i p y n b}$ 。部分代码如下

问题 $6$ .目前MNIST影像为单色灰阶，若是彩色可以辨识吗? 怎么修改？

$$
\mathrm{r e c a l l}_{-} 3 \colon\ 0. 9 7 5 1
$$

问题 $5$ ，目前Dropout比例为 $0. 2$ ，设为其他值会更好吗？

$$
\bigoplus_{H \pm}^{\pi\pm\Sigma} :
$$

![](figures/233-5-FIGURE.jpg)

执行结果如下，准确率略为提高

$$
\operatorname{l o s s} \colon0. 0 8 1 6
$$

$$
\mathrm{a c c u r a c y \! :} \ 0. 9 7 5 5
$$

可见抛弃比例过高时，准确率会陡降，如图4.10所示

![](figures/233-10-FIGURE.jpg)

图4.10准确率随抛弃比例变化

解答：可以，若颜色有助于辨识，可以将RGB三通道分别输入辨识，后面我们谈到卷积神经网络时会有范例说明。

解答：执行周期改为 $1 0$ ，请参阅程序04 08手写阿拉伯数字辨 $\grave{\imath} \pi$ 实验6.ipynb。部分代码如下：

理论 $\vdash$ ，训练周期越多，准确率越高，但是，过多的训练周期会导致过度拟合（Overiting 反而会使准确率降低，如图4.11 所示。

解答：很少模型准确率能够达到100 $\not\nabla_{O}$ ，除非是用数学证明， 然而,神经网络 $\P$ 是近似解而已，另一方面，神经网络是从训练数问题 $7$ ，目前执行周期设为 $5$ ，设为其他值会更好吗？

15 history = model.fit(x train norm, y train, epochs=10, validationsplit=0.2)

执行结果如 $\mathrm{T}$ ，准确率略为提高

$$
\mathrm{l o s s : ~ 0. 0 7 0 0 ~}
$$

$$
\mathrm{a c c u r a c y \! :} \ 0. 9 7 8 5
$$

![](figures/234-9-FIGURE.jpg)

图4.1，准确率随训练周期变化

问题 $8$ 。准确率可以达到 $1 0 0 \%$ ，以便企业安心导入吗？ 据中学习到知识，但是，测试或预测数据并不参与训练，若与训练
，甚至来自于不同的概率分布，则很难确保准的数据分布有所差异，
确率能达到100%

解答：我们只需修改很少的程序代码，就可以辨识其他对象。 例如，MNSIT 另一个数据集FashionMnist，它包含女人身上的10 种配件，请参阅04 09 FashionMnist 实验.ipynb，除了加载数据的指令不同之外，其他的程序代码几乎不变。这也说明 $7-\pm$ ，神经网络并不是真的认识0~9或女人身上的10个配件，它只是从像素数据中推估出的模型，即所谓的从数据中学习到知识

(Knowledge Discovery from Data, KDD 以MNIST而言， 模型 $\P$ 是统计 $0 \sim9$ 这一个数字，它们的像素大部分分布在那些位置而已。

解答：可以使用图像处理分割数字，再分别依序输入模型预测即可。还有更简单的方法，直接将视觉接口（UI）设计成 ${}_{4}$ 格，规定使用者只能在每格子内各输 $\lambda-$ 个数字即可。

解答：可以参考TensorFlow官网"或 Keras官网“，版本快速的更新已经使网络上的信息新旧杂陈，官网才是最新信息的正确来源。

以上的实验大多只对单一参数作做较，假如要同时比较多个变量， 「跑遍」所有参数组合，这样程序会很复杂吗？读者不
就必须

问题 $9$ ，如果要辨识其他对象，程序要修改哪些地方？

问题 $1 0$ 。如果要辨识多个数字，如输入4位数，要如何辨识？

问题 $1 1$ . 望了解更详细的相关信息，有哪些资源可以参阅？
希望

必担 $\imath\grave{\mathrm{L}} \imath$ ，有一些库可以帮忙，包括Keras Tuner、hyperopt、Ray Tune、Ax等，在后续超参数调校中会有较详细的介绍。

由于这个模型的辨识率很高，要观察超参数调整对模型的影响，并不容易，建议找一些辨识率较低的模型进行相关实验，例如 FashionMnistli、GiFar数据集，才能有比较显著的效果，笔者针对FashionMnist做了另一次实验，请参阅0409FashionMnist 实验 ${\bf i p y n b}$ 。

(1）Sequential model：顺序型的模型，按神经层的排列顺序，由上往下执行，每一层的输出都是下一层的输 $\lambda$ ，所以，除 $\overline{{\jmath}}$ 第 $\ --$ 层要设定输入的维度外，其他层都不需要设定

(2）Functional API：提供较有弹性的结构，允许非直线型的结构、共享的神经层及多输 $\lambda_{l}$ 输 $\uplus$ ，亦即模型结构可以有分叉或合并（SplitMerge)

## 4-2 Keras 模型种类

TensorFlowKeras 提供以下两类模型结构

(1）模型内可包含各式的神经层，简洁的写法是以List包住神经层。程序代码如 $\mathrm{F}$ :

注意：除了第一层要设定输入的维度（input shape）外，其他层都不需要设定，只要在第一个参数指定输出维度即可。

(2）可以变换另一种写法，将input shape拿掉，在mode 内设定输入层及维度参数（shape 程序代码如下：

(3）可以直接串连神经层。程序代码如 $\mathrm{F}$ ，请详见最后一列指令

## 4-2-1 Sequential mode

请参阅程序:04 10_ Sequential model.ipynb

![](figures/238-6-FIGURE.jpg)

![](figures/238-7-FIGURE.jpg)

(4）可以后续加减神经层，pop()会删)减最上层（Top) 注意：神经层是堆栈（Stack) 后进先出，即最后一层Dense. 1 model = tf.keras.models.Sequential([
2 tf.keras.layers.Flatten(input shape=(28, 28)), 3 tf.keras.layers.Dense(128, activation='relu'), 4 tf.keras.layers.Dropout $( 0. 2 )$ ,
5 tf.keras.layers.Dense(10, activation='softmax') 6]）
7
8#删减一层
9 model.pop()
10 print(f·神经层数： {len(model.layers)}')
11 model.layers

## 执行结果如下：

## 神经层数：3

[<tensorflow.python.keras.layers.core.Flatten at Ox205bbedff70>, <tensorflow.python.keras.layers.core.Dense at 0x205bbedff10>, <tensorflow.python.keras.layers.core.Dropout at 0x205bbec55bo>]

## (5）增加一层神经层。程序代码如下：

1#增加一层
2 model.add(tf.keras.layers.Dense(10)) 3 print(f'神经层数：{len(model.layers)}'） 4 model.layers

## 执行结果如下：

## 神经层数：4

[<tensorflow.python.keras.layers.core.Flatten at Ox205bbedff70>, <tensorflow.python.keras.layers.core.Dense at 0x205bbedff10>, <tensorflow.python.keras.layers.core.Dropout at Ox205bbec55bo> <tensorflow.python.keras.layers.core.Dense at 0x205bbee6250>]

(6）取得模型各神经层信息。程序代码如下执行结果：执行结果如 $\mathrm{T}$ ，包括 $3$ 层权重及 $3$ 层偏差，共 $6$ 类。

$$
( 9 ) ~ ~ \exists\b l \b< \i\b> \hbar\b> \line~ \imath\imath\setlength{\unitlength} {1 m m} \setlength{\unitlength} {1 m m} \setlength{\unitlength} {1 m m} \line\imath\imath\imath\imath\imath\imath\line\imath\imath\setlength{\unitlength} {1 m m} \line\imath\imath\setlength{\unitlength} {1 m m} \line\imath\imath\imath\imath\jmath\line\imath\imath\jmath\setlength{\unit5} \setlength{\unitlength} {\unit5 p t} \line\imath\jmath\displayskip{\unitlength} {1 m m} \line\jmath\uparrow\jmath\displayskip{\unit5 p t} \rightleftharpoons\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\jmath\tt\jmath\jmath\jmath\tt\jmath\jmath\tt\jmath\end\end\end\end\end\end\end\end
$$
边显示模型汇总信息，这样有利
于排除错误，查看中间处理结果

![](figures/240-2-FIGURE.jpg)

![](figures/240-3-FIGURE.jpg)

 $( 7 )$ 取得特定神经层信息。程序代码如下

1 print(f'{layer2.name}: {layer2.weights}')

(8）取得模型汇总信息。程序代码如下：

1 model.summary()

(10）取得每一层神经层的output：可设定模型的input和 output。程序代码如下

(11）取得特定神经层的output：设定模型的 output为特定的神经层。程序代码如下：

![](figures/241-2-FIGURE.jpg)

![](figures/241-3-FIGURE.jpg)

1#设定模型
2 initial model = tf.keras.Sequential(
3 【
4 tf.keras.Input(shape=(250, 250, 3)),
5 layers.Conv2D(32, 5, strides=2, activation="relu"),
6 layers.Conv2D(32, 3, activation="relu", name="my intermediate layer"), 7 layers.Conv2D(32, 3, activation="relu"),
8 ]
9
10
11 #设定模型的input和output
12 feature extractor = tf.keras.Model(
13 inputs=initial model.inputs,
14 outputs=initial model.get layer(name="my intermediate layer").output,
15
16
17#使用feature extractor 取得 output
18 x = tf.ones((1, 250, 250, 3)）
19 features = feature extractor(x)
20 features

由于 Functional API提供了较有弹性的结构，因此适用于相对复杂的模型结构，允许非直线型的结构、共享的神经层及多输 $\lambda_{l}$ 输 $\uplus$ ，结构可以分叉或合并（GpliMerge)

范例1、先看一个简单的程序语法，除了第一层之外，每一层均须设定前一层，同时，model函数必须指定输 $\lambda_{l}$ 输出

(input/output）是哪些神经层，它们都是List数据类型，允许多个输 $\lambda$ /输出。程序代码如下：

范例 $2$ 、模型包括 $\textbf{3}$ 个输入、 $2$ 个输出，我们先不管模型用途， $\R$ 观察程序语法，ayers.concatenate()函数可用于合并神经层。程序代码如下：

## 4-2-2 Functional APl

我们直接通过范例说明，请参阅程序04 11 Functional $\b A \mathbf{P l. i p y n b}$ 

![](figures/243-6-FIGURE.jpg)

$$
( 2 ) \textit{c o n c a t e n a t e} ( ) \oplus\pm
$$
1了3个layers，它们的输出维度大小
分别为128、32、 $1 2$ ，故合并后，输出维度大小为

![](figures/244-1-FIGURE.jpg)

## (1）最后一行程序代码绘制的模型图如图4.12所示

![](figures/244-3-FIGURE.jpg)

冬 $4. 1 2$ 绘制模型图

(3）最后一行程序代码的参数show shapes-True，结构图会额外添加含有input和 output信息

$$
1 2 8+3 2+1 2=1 7 2_{\circ}
$$

神经层是神经网络的主要成员，Tensorflow 有各式各样的神经 $\P$ ，详情可参阅 Keras 官网Ll0，目前包括以下类别，随着各种算法的发明，类别也会不断增加，就算TensorFlow 的更新脚步跟不 $\b L$ 用户的需求，用户也可以自定义神经层（Custom Layer)

由于中文翻译大部分都不能望文生义，因此后面的内容均使用英文术语。

## 4-3神经层

(1）核心神经层（CoreLayers
(2）卷积祖
$$
\b< \b> \equiv\mathrm{~ ( C o n v o l u t i o n \, L a y e r s ) ~} ~,
$$
(3）池化神经层（Pooling Layers)
(4）循环神经层（Recurrent Layers)
(5）前置神经层（Preprocessing Layers) (6）常态化神经层（Normalization Layers) (7）正则神经层（Regularization Layers) (8）注意力神经层（Attention Layers
(9）维度重置神经层（Reshaping Layers) (10）合并神经层（Merging Layers)
(11）激励神经层（Activation Layers

(1）核心神经层（Gore Layers O

(2）卷积神经层（Convolution Layers O

$$
( 3 ) \, \, \, i \neq1 \! \! \, \pm\! \, \pm\! \, \pm\! \, \, \mathrm{( P o o l i n g ~ L a y e r s )} \; \; \circ
$$

(4）循环神经层（Rocurrent Layers O

$$
\mathrm{( 5 ) ~ \pm\^{\prime}_{B I} \equiv\mathfrak{A}_{Z}^{\prime} \pm\^{\prime}_{Z} \equiv\ ~ ( P r e p r o c e s s i n )}
$$
gLayers O

(6）常态化神
$$
\pm\Xi\equiv\mathrm{( N o r m a l i z a t i o n \, L a y e r s )} \; \; \mathrm{{o}}
$$

(7）正则神经
$$
\pm\mathrm{\ ( R e g u l a r i z a t i o n \; L a y e r s ) \; \; o}
$$

(8）注意力神经层（Atention Layers O

(9）维度重置神经层（Reshaping Layers O

$$
( 1 0 ) ~ \rightharpoonup\# \hbar\hbar\hbar\Xi\equiv~ ( \mathrm{M e r g i n g ~ L a y e r s} ) ~ ~_{\circ} ~
$$

(11激励神经层（Activation Layers O 现阶段仅介绍之前用到的核 $\imath\grave{\mathrm{L}}$ 神经层，其他类型的神经层在后续算法用到时再说明。

Dense 是最常见的神经层，每个输入的神经元都会完全连接到输出神经元。

## 4-3-1 完全连接神经层

我们直接通过范例进行说明，请参阅程序04 12 神经层.ipynb.

范例。计算Dense的参数个数

1）模型结构如下：

![](figures/248-5-FIGURE.jpg)

## 执行结果如下，显示出了各层的output及参数个数

$$
\mathrm{~ M o d e l ~ : ~ ` ` s e q u e n t i a l ~^{n} ~}
$$

| Layer (type) | Output Shape | Param # |
| --- | --- | --- |
| flatten (Flatten) | (None, 784) | 0 |
| layer1 (Dense) | (None,128) | 100480 |
| dropout (Dropout) | (None,128) | 0 |
| layer2 (Dense) | (None,10) | 1290 |

(2）设定模型的output为第一层Dense，显示第 $- \pi$ Dense output个数 $( 1 2 8 )$ 。程序代码如下：

$$
\bullet\quad\mathrm{a c t i v a t i o n} \colon
$$
指定要使用Activation Function，也可以独立
使用
$$
\mathrm{A c t i v a t i o n} \, \mathrm{L a y e r}_{\circ}
$$

![](figures/249-2-FIGURE.jpg)

## (3）第一层Dense 的参数个数计算。程序代码如下：

![](figures/249-4-FIGURE.jpg)

执行结果：参数个数共有100480个 ，与模型汇总信息一致。

(4）第二层Dense的参数个数计算。程序代码如下：

![](figures/249-7-FIGURE.jpg)

执行结果：参数个数共有 $1 2 9 0 \ \uparrow$ ，与模型汇总信息一致。

Dense神经层的参数说明如下。

unts输出神经元个数

use bias: 权重参数估计是否要含偏差项

 $\bullet$ kernel initializer:：权重初始值，默认值是glorot uniform, 它是均匀分布的随机数。

 $\bullet$ kernel regularizer：权重是否要使用防止过度拟合的正则函数，默认值是无，也可以设为L1或L2。

 $\bullet$ bias regularizer: 偏差是否要使用防止过度拟合的正则函数，默认值是无，也可以设为L1或L2。

actvty regularize: ativation tunction 是否要使月防上过度拟合的正则函数，默认值是无，也可以设为L1或L2。

bias initializer:偏差初始值。

 $\bullet$ kernel constraint：权重是否有限制范围

 $\bullet$ bias constraint：偏差是否有限制范围 DropoutLayer 在每一Eooch/Step训练时，会随机丢弃设定比例的输入神经 $\overline{{\Pi}}$ ，避免过度拟合，只在训练时运作，预测时会忽视 Dropout，不会有任何作用。参数说明如 $\mathrm{F}$ 。

## 4-3-2 Dropout Layer

：丢弃的比例，介于（0,1) O rate:

training：是否在训练时运作。

根据大部分学者的经验，在神经网络中使用 Dropout会比 Regularizer效果好

Activation Function 是将线性方程转为非线性，目的是希望能提供更通用的解决方案。

Activation Function 有多种函数，读者可以参考维基百科表格 3，具体见表4.2。

## 4-4 激活函数

Output-acivation tun
$$
\mathrm{\bf{t i o n} ~} \ ( \mathrm{\bf{~ x ~}_{1} ~} W_{1}+\mathrm{\bf{~ x ~}_{2} ~} W_{2}+\mathrm{\bf{~ \ldots~}} X_{n} ~ W_{n}+\mathrm{\bf{b i a s}} )
$$

表 $4. 2$ Acivation Function列表（数据源：维基百科

| Name | Plot | Function, f(c) |  | Deriv | erivative of f, f'(a) |  | Range |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Identity |  | C |  | 1 |  |  | (-0,00) |  |
| Binary step |  | ifc < 0 ifa> 0 |  | 0 undefir |  if a f 0 efined ifa = 0 |  | {0,1} |  |
| Logistic, sigmoid, or soft step |  | o(r)= 1 [1] 1+e-" |  | f(x)(1 - | l-f()) |  | (0,1) |  |
| tanh |  | tanh(c)= e" -e-e" +e-" | 1 - f(w)? |  | (-1,1) |  |
| Rectified linear unit ReL)l1 |  |  0 ifc < 0 c if c > 0 = max{0,a}=a|z>o |  | 0 1 undefi |  ifc < 0 ifa> 0 lefined ifz=0 |  | [0, o) |  |
| Gaussian error linear unit (GELU)6  |  | 1 C 1+erf 2 C 2 = @(w)  |  | $(z)+x6(z) | (-0.17..., | o) |
| Sofiplusl12] |  | ln(1 + e*) |  |  1 1Le-2 | (0, oo) |  |

## (续表4.2)

TensorFlow支持大部分的函数，如果找不到，用户也能够自定义函数，它们可以直接设定神经层的参数，也可以是独立的函数

(1) ReLU (Rectified Linear Unit 是目前隐藏层最常用的函数，公式请参考上表，函数名称为relu。程序代码如下：

| Exponential linear unit (ELU)13] |  | a(e" -1) ifo < 0 aC ifa > 0 with parameter a  | ae* i 1 if 1 if | <0 >0 =0anda=1 | (-a, oo) |
| --- | --- | --- | --- | --- | --- |
| Scaled exponential linear unit (SEL)14] |  |  ifc < 0 C if c > 0 with parameters ) = 1.05 a= 1.67326  | ae" f f | <0 >0 | (-Xa,o0) |
| Leaky rectified linear unit (Leaky ReLU)l15] |  | 0.01c ifz<0 ae ifa>0 | (0.01 i 1 if | <0 >0 | (-00,o0) |
| Parameteric rectified linear unit (PRLUjI16) |  | ae ifc < 0 r if a> 0 with parameter a | a if a 1 if a | 0 0 | (-,o)P] |
| EliotSig,[7I18) softigl19I20 |  | c 1+|@| | 1 (1 +|:|)2 |  | (-1,1) |
| Square nonlinearity (SQNL)I21] |  | 1 2 g2 if c > 2.0 c 十 4 if0 < ac< 2.0 a2 if - 2.0 <c< 4 if z <-2.0 | 1干 c 2 |  | (-1,1) |
| S-shaped rectified linear activation unit (SReLu}22] |  | th + a(e- to ifa s at if t< t, tay(e - t,) ifa > where t, au, ty, a, are parameters  | a1 if a 1 if t a. if a | t, c t, t,  | (-0, 00) |
| Bent identity |  | /e2+1-1 +c 2  |  ac 2/a2+I | 1 | (-0o,00) |
| Sigmoid linear unit (SiLU,6l SiL,23] or Swish-1[241) |  | c 1+e- | 1+e-"+ (1 + e | -" | [-0.278..,00 |
| Gaussian |  |  | -2zwe-"? |  | (0, 1] |
| SQ-RBF | 0.s 0.4 0.2 | 0 2 if |c|<1 景(2-|z|)? if 1 < |al if |e|> 2 | 一t c-2sg 0  | if |a| <1 ) if 1 < |a| < 2 if |a| > 2  | [0, 1] |

范例。实际测试常用的 Activation Function

请参阅程序： 04 13 Activation Function.ipynb

个执行结果：结呆如图4.13所示，会忽视过小的外部输 $\lambda$ , 比如说，我们轻轻碰一下皮肤，大脑可能不会做出反应。

 $\bullet$ threshold：超过此阈值，y才会大于0。例如，threshold-s 时，如图4.14所示。

max value：y的上限。例如 max value-s时，如图4.5 所示

![](figures/254-3-FIGURE.jpg)

![](figures/254-4-FIGURE.jpg)

图4.13”测试执行结果

Preu函数有以 $\mathrm{F} \Xi$ 个参数。

![](figures/254-7-FIGURE.jpg)

图4.14朗值测试结果

 $\bullet$ alpha: $\imath\rfloor\omicron$ 于阈值，y会等于x*alpha。例如，alpha-0.5，如冬 4.16 所示，又称为Parameteric rectified linear unit
(PReLU），若alpha-0.01，则称为 Leaky rectified linear unit (Leaky ReLU)

(2）sigmoid：即Logistic 回归，因为函数为 $\mathsf{S}$ 型而得名，适用于二分类，可加在最后一层Dense 内。程序代码如下：

![](figures/255-2-FIGURE.jpg)

图 4.15 max value测试结果

![](figures/255-4-FIGURE.jpg)

图 $4. 1 6$ alpha-0.5测试结果

③相关测试请参阅程序

执行结果：函数最小值为0，最大值为 $1$ ，只有两条直线中间是模糊地带，但也是一个平滑改变的过程，而非阶梯形的函数，可降低预测的变异性（Variance) 如图4.17所示。

执行结果：函数最小值为-1，最大值为 $1$ ，只有两条直线中间是模糊地带，平滑改变的过程与 signoid相比较为陡峭，如图4.18

![](figures/256-2-FIGURE.jpg)

![](figures/256-3-FIGURE.jpg)

图4.17 sigmoid函数测试结果

(3）tanh：与sigmoid类似，但最小值是-1。程序代码如下：

![](figures/256-6-FIGURE.jpg)

(4）sofmax：这个函数会将输入转为概率，即所有值介于 (0,1) 总和为 $1$ ，适用于多分类，可加在最后一层Dense内程序代码如 $\mathrm{F}$ :

0执行结果：activations.softmax()输入必须是二维数据，设定 x为均匀分布，转换后，每一行总和为1。执行结果如下

$$
\F F_{\mathrm{I J}}, \overline{{\Pi}}_{\mathrm{o}}
$$

![](figures/257-3-FIGURE.jpg)

图4.18 tanh 函数测试结果

![](figures/257-5-FIGURE.jpg)

输入：
[[6.96374103 3.56759932 6.81461845 6.13818249] [8.21949231 8.64238308 4.10392038 1.15902837] [3.76801259 3.90992316 4.94608869 8.02829542] [2.30668926 9.62941128 2.58390131 7.15530129] [3.31859535 6.63781936 2.66373368 8.26399924] [5.51747795 2.21733791 1.11501206 8.05622922] [1.16947165 4.69649342 2.27081238 8.68047906] [5.44635867 5.48375512 8.09837217 8.3067494 ] [2.59147769 1.34277168 9.38268057 4.80765181] [2.8182047 1.81172631 2.75112306 8.46645002]1 加总：[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]

2使用 NumPy计算sotmax。程序代码如下：

(5）自定义函数，可以使用TensorFlow 张量函数，只要传回与输 $\lambda$ /输出相符合的维度和数据类型即可，例如：model.add
$$
\mathrm{( l a y e r s. D e n s e ~ ( 6 4, a c t i v a t i o n )}
$$
nti.nn.tanh)

$$
\frac{-A_{X}^{n} \pm i \pi, \ \mathrm{A c t i v a t i o n} \, F_{\circ}} {F_{\circ}} \Psi\times H
$$
unction 会接在神经层后面，示例如

TensorFlow/Keras为简化语法，允许将 Activation Function当作参数使用，直接包在神经层的定义中，示例如 $\mathrm{F}$ 

(6）其他的函数请参见 Keras官网，包括以下两个网址

Ohps:/ker
$$
\mathrm{a s. i o / a p i / l a y e r s / a c t i v a t i o n s /^{[ 1 7 ]} o}
$$

Phtps:/keras
$$
\mathrm{i o / a p i / l a y e r s / a c t i v a t i o n_{-} l a y e r s /^{[ 1 8 ]} \circ}
$$

$$
\textcircled{{\wp}} \textbf{x}=\mathrm{l a y e r s. D e n s e} \textit{( 1 0 )} \textit{( x )} \put{0 pt} \circ
$$
$$
\bf{\widehat{2}} \bf{x}=\bf{l a y e r s. L e a k y R e L U ( )} \bf{( x )} \bf{\epsilon} \4 m u_{o}
$$

Oit.keras.layers.D
$$
\mathrm{e n s e} \, \mathrm{~ ( 1 2 8, ~ a c t i v a t i o n='r e l u^{\prime} ) ~}_{\circ}
$$
Qtf.keras.layers.Dense (10, activation-'softmax

OMikra.layer
$$
\mathrm{~ s. D e n s e ~} ~ ( 1 2 8, \mathrm{a c t i v a t i o n=l r e l u^{\prime} ~} ) ~ ~_{\circ}
$$

$$
\widehat{2} \mathrm{t f. k e r a s. l a y e r s. D e n s e} ~ ( \mathrm{1 0, a c t i v a t i o} )
$$
n-'softmax') O

损失函数（Loss Functions）又称为目标函数（Objective Function）、成本函数（Cost Function) 模型以预测总误差最小化为目标，因而，学者因应不同的场域，以各种函数来定义总误差。

(1）概率相关的损失函数（Probabilistic Loss) 例如，二分类的交叉熵（Binary Crossentropy 多分类的交叉熵
(CategoricalCrossentropy)

(2）回归相关的损失函数（Regression Loss 比如，均方误差（MSE)

(3）铰链损失函数（Hinge Loss 。经常用在最大间格分类
$$
\mathrm{( M a x i m u m-m a r g i n ~ C )}
$$
:lassification) 适用于支持向量机（SVM)
等算法。

## 4-5 损失函数

TensorFlow损失函数分成以下三类，请参阅官网!

TensorFlow损失函数一般在 model.comile)中设定，例如：

model.compile (loss-'mean squared error' opimizer'sgd')

上面直接使用字符白，如果担心出错，也可以使用函数：

$$
\operatorname{f r o m} \operatorname{k e r a s} \mathrm{~ i m p o r t \, l o s s e s}
$$

BinaryCrossentropy: 熵 $( \mathrm{E n t r o p y} )$ 是指不
二分类的交叉熵，
确定的程度， $\big< \lambda$ 式为

model.compile (loss-losses.mean squared error, opimizer'sgd')

范例1。实际测试几个常用的损失函数

请参阅程序：04 14 Loss Function.ynb

s--p（x） logp（x dx（适用于连续型分布
s=- $\Sigma p$ （x）logp（x）（适用于离散型分布)
若是二分类y-0或 $1$ ，则离散型分布的二分类交叉熵为 S=-yl0g $( p ) ~-~ ( 1-y )$ log（1-p)
当y-0时，
$$
\mathrm{s=-l o g} ~ \left( 1-p \right)
$$
当y-1时，
$$
\mathrm{s=-l o g} ~ \left( p \right)
$$
使月sigmoid计算 $\boldsymbol{p}$ ，就可得到BinaryCrossentropy。

$$
\mathrm{s=-f} p ~ \left( \begin{array} {c} {x} \\ \end{array} \right) ~ \mathrm{l o g} p ~ \left( \begin{array} {c} {x} \\ \end{array} \right) ~ \mathrm{d} x
$$
(适用于连续型分布)

若是二分类 $y=0$ 则离散型分布的二分类交叉熵为
或1

使用 sigmoid计算 $\boldsymbol{p}$ ，就可得到Binaryrossnropy.

(1）两个数据的实际值和预测值如 $\mathrm{T}$ ，计算 BinaryCrossentropy。程序代码如下：

(2）CategoricalCrossentropy：多分类的交叉熵。两个数据的实际值和预测值如下，计算BinaryCrossentropy。程序代码如下:

(3） SparseCategoricalCrossentropy：稀疏矩阵的多分类交叉熵，预期的目标值是单一整数，而非 One-Hot Encoding的数据类型，所以，使用此损失函数有个好处是，前置处理就可以省去 One-Hot Encoding 的转换。

执行结果：0.8149.

依照公式验算，程序代码如下

![](figures/261-4-FIGURE.jpg)

执行结果:：0:8149. 5Bnarsossento $\mathbf{y} ( \medskip)$ 计算结果一致

![](figures/261-6-FIGURE.jpg)

执行结果：1.1769.

两个数据的实际值和预测值如 $\mathrm{F}$ ，计算
SparseCategoricalCrossentropy。程序代码如下： 两个数据的实际值和预测值如 $\mathbf{F}$ ，计算MeanSquaredEror 程序代码如 $\mathrm{F}$ :

 $( 5 )$ 铰链损失函数：常用于支持向量机，详细可参阅资料 [13].

不考虑负值的损失，所以也被称作单边损失函数，真实值
 $\mathrm{( y}$ true）通常是-1或 $1$ ，如果训练数据的y是 $0 / 1$ ，Hinge Loss 会自动将其转成 $- 1 / 1$ ，再计算损失。

执行结果：.1769

(4) 计算实际值和预测值的均方误差。

| 1 | #两个数据实际及预测值 |  |
| --- | --- | --- |
| 2 | y true= [[0.,1.1,[0.,0.]] | # 实际值 |
| 3 | y pred= [[1.,1.], [1.,0.]] | #预测值 |
| 4 |  |  |
| 5 | #多分类交叉熵(CategoricalCrossentropy) |
| 6 | mse = tf.keras.losses.MeanSquar | Error( |
| 7 | mse(y true, y pred).numpyO |  |

D执行结果：
$$
( \textit{( 1-1 )}^{2}+\textit{( 0-1 )}^{2} ) \textit{/ 2}=0. 5 \circ
$$

Psample weight：可加参数设定样本类别的权重比例

Breduction-tf.keras.osses.Reduction.SUM：取总和，即 SSE，而非MSE。

④大部分损失函数也可以加这些参数

t $\mathrm{o t a l \ l o s s}=\Sigma$ maximum $( 1-\mathrm{y \_t r u e}^{\star} \mathrm{y \_p r e d}, 0 )$ 两个数据的实际值和预测值如 $\mathrm{T}$ ，计算Hinge Loss。程序代码如下：

 $( 6 )$ 自定义损失函数（Custom Loss）：撰写一个函数，输入为y的实际值及预测值，输出为常数即可。下面的范例自定义损失为 MSE。程序代码如下：

![](figures/263-2-FIGURE.jpg)

$$
\pm\hbar\iiint\pm\pm\pm1. 3 。
$$

验算程序代码如下：

![](figures/263-5-FIGURE.jpg)

执行结果：与Hinge()执行结果相同优化器是神经网络中反向传导的求解方法，着重应用在以下两方面。

(2）避开马鞍点（Saddle Point）等局部最小值，并且找到全局的最小值（Global Minimum

优化器的类别一样在 model.compile()设定，Tensorlow支持的不同优化器如下，读者可参阅Keras官网

## 4-6优化器

!）设定学习率的变化，加速求解的收敛速度。

OSGD.
DRMSprop.
$$
\begin{array} {l} {\textcircled{3} \mathrm{A d a m} \circ} \\ {\textcircled{4} \mathrm{A d a d e l t a} \circ} \\ \end{array}
$$
$$
\begin{array} {c} {\emptyset\mathrm{A d a g r a d} \circ} \\ {\emptyset\mathrm{A d a m a x} \circ} \\ \end{array}
$$
$$
\protect\protect\bigoplus_{\mathrm{F t r l}_{\mathrm{o}}}
$$

$$
\copdotpown\qquad\mathrm{S G D}_{\circ}
$$

范例。实际操作几个常用的优化器

(1）批量梯度下降法（Batch Gracient Descent, BGD 以全部样本计算梯度，更新权重。

(2）随机梯度下降法（Stochastic Gradient Descent
SGD）：一次抽取一个样本计算梯度，并立即更新权重。其优点是更新速度快，但收敛会较曲折，因为训练过程中，可能拍到好样本，也可能抽到坏样本，如图4.19所示。

(3）小批量梯度下降法（Mini-batch Gradient Descent）：折中前两种做法，以一批样本计算梯度，再更新权重。 $\prime\rfloor\omicron$ 批量梯度下降法可涵盖前两种，批量等于全部样本，即为BGD；批量为 $1$ ，即为SGD，故小批量梯度下降法通称为随机梯度下降法（SGD)

请参阅程序:04 15 Optimizer.ipynb

## 1.随机梯度下降法

依据权重更新的时机差别，梯度下降法分为以下三种：

![](figures/265-6-FIGURE.jpg)

4.19随机梯度下降法求解图示

$$
\mathrm{S G D} \, i \pm i \pm\hbar\mathrm{I} \mathrm{F} \! :
$$

动能通常介于 $( 0, 1 )$ ，若等于 $0$ ，表示学习率为固定值。一般而言，刚开始训练，离最 $\imath\rfloor\omicron$ 值较远的时候，学习率可以「放胆迈大步」，越接近最小值时，学习率变动幅度就要变 $\rfloor\omicron$ ，以免错过最小值，这种动态调整学习率的方式，能够使求解收敛速度加快，又不会错过最小值。

## ）权重更新公式为

$$
w=w \mathrm{-} l e a r r i n g \_r a t e^{\star} g
$$

其中：w为权重；g为梯度：Leaning yate为学习率

$$
\textcircled{2} \bar{\ z} \hbar\ss\L\mathrm{\ ( m o m e n t u m )} \ : \ \triangle\hbar\gg
$$

velo
$$
c i t y=m o m e n t u m ~^{\star} v e l o c i t y-l e a r m i n g \_t
$$
ate"g

$$
w=w+v e l o c i t y
$$

3nesterov：是否使用 Nesterov momentum，默认值是 False。要了解技术细节可参阅「Understanding Nesteroy Momentun (NAG）」L4

(4）随机梯度下降法的简单测试。程序代码如下 ②执行结果：每 $1 0$ 步打印结果，越来越接近最 $\imath\rfloor\omicron$ 值 $0$ 。执行结果如下：

![](figures/267-1-FIGURE.jpg)

①损失函数 $x^{2} / 2_{\circ}$ 

| 优化的步骤:11, | 变数:0.3138105869293213 |
| 优化的步骤:21, | 变数:0.10941898822784424 |
| 优化的步骤:31， | 变数:0.03815204277634621 |
| 优化的步骤：41， | 变数:0.013302796520292759 |
| 优化的步骤:51， | 变数:0.004638398066163063 |

## (5）优化三次测试随机梯度下降法的动能。程序代码如下：

1 opt = tf.keras.optimizers.SGD(learning rate=0.1, momentum=0.9)
2 var = tf.Variable(1.0)
3
4#损失函数起始值
5 valo = var.value()
6 print(f'val0:{val0}')
7#损失函数
8 loss = lambda: (var ** 2)/2.0
9
10#优化第一灭
11 step count = opt.minimize(loss, [var]).numpy()
12 vall = var.value() $: \! \left\{( {\mathrm{v a l}} 0 \ -\ {\mathrm{v a l}} 1 ) \dots\operatorname{n u m p} y ( \, ) \, \right\}^{\prime} )$ 13 print(f'优化的步骤:{step count}, val1:{val1}，变化值
14
15#优化第二灭
16 step count = opt.minimize(loss, [var]).numpy()
17 val2 = var.value() $: \! \left\{( {\mathrm{v a l}} 1 \mathrm{~-~ v a l} 2 ) \mathrm{~. n u m p y ( \, )} \right\}^{\prime} )$ 18 print(f'优化的步骤:{step count}, val2:{val2}，变化值
19
20 #优化第三次
21 step count = opt.minimize(loss, [var]).numpy()
22 val3 = var.value() $: \! \left\{( {\mathrm{v a l}} 2 ~-~ {\mathrm{v a l}} 3 ) \,. \, \mathrm{n u m p y} ( \, ) \, \right\}^{\prime} )$ 23 print(f"优化的步骤:{step count}, val3:{va13}，变化值

## 执行结果如下： Adam（Adaptive Moment Estimation）是常用的优化器，这里就引用 Kingma等学者于 2014年发表的「Adam:A Method for Stochastic Optimization. 15一文所作的评论「Adam 计算效率高内存耗费 $\rfloor\downarrow$ ，适合大数据集及参数个数很多的模型」

 $\bullet$ beta 1：一阶动能衰减率（exponenial decay rate for the 1st moment estimates

 $\bullet$ beta 2：二阶动能衰减率（exponential decay rate for the 2nd moment estimates

 $\bullet$ amsgrad：是否使用 AMSGrad，默认值是False。技术细节可参阅「一文告诉你 Adan、AdamW、 $\mathbf{A}$ msgrad 区别和联系 161。Adam简单测试。程序代码如下：

## 2.Adam优化器

 $\mathbf{A}$ dam语法如下

| 1 | #Adam |
| --- | --- |
| 2 | tf.keras.optimizers.Adam( |
| 3 | learning rate=0.001, |
| 4 | beta 1=0.9, |
| 5 | beta 2=0.999, |
| 6 | epsilon=1e-07 |
| 7 | amsgrad=False, |
| 8 | name="Adam", |
| 9 |  |

epsilion：误差值， $\imath\rfloor\omicron$ 于这个值，优化即停止。 执行结果：SGD执行50步，Adam 只需要执行10步，就已收敛。执行结果如下:

(1) Adagrad (Adaptive Gradient-based opimization）：设定每个参数的学习率更新频率不同，较常变动的特征使用较小的学习率，较少调整：反之，使用较大的学习率，比较频繁地调整，主要是针对稀疏的数据集。

$$
( 2 ) \mathrm{\ R M S-P r o p}
$$
：每次学习率更新是除以均方梯度（Average
of Squared Gradients) 以指数的速度衰减。

$$
( 3 ) \mathrm{\ A D A M :}
$$
是Adagrad 改良版，学习率更新会配合过去的
平均梯度调整。

官网还有介绍其他的优化器，网络上也有许多优化器的比较和动画，有兴趣的读者可自行搜索。虽然研发领域也是一个值得探究

![](figures/269-5-FIGURE.jpg)

| 优 | 变数:0.7015870809555054 |
| 优 | 变数:0.5079653263092041 |
| 亿 | 变数:0.32342255115509033 |
| 优 | 变数:0.15358668565750122 |
| 亿 | ，变数:0.00513361394405365 |

## 3：几种常用的优化器的小宇宙，但毕竞我们学习东西，能不能实际派上用场最重要，所以本书的核 $\grave{\imath} \grave{\mathrm{L}}$ 是以实务为主，论文研究并不是本书的重点。

模型存盘，可以存储下列信息：①模型结构与组态；②权重，含偏差项；③模型compile 选项：④优化器的状态，这样训练即可白断点处继续执行。

TensorFlow SavedModel格式：官方建议采用这种格
(1)
式，以目录存储，目录下含多个文件，各司其职。

(2）Keras H5格式：Keras 库既有格式，以单一文件存储， 注意，此种格式无法存储自定义的神经层

均使用 model.save（< file path>）指令，如果设定扩展名
为.h5，则存档成Keras 既有格式。加载模型时调用load model(e file path>）指令。

## 5-2 模型存盘与加载

目前支持以下两种格式

范例，模型存盘与加载

请参阅程序：05 02 模型存盘与加载.iynb

$$
\star\equiv\lfloor\pm\lfloor\exists\pm\rfloor\rceil\rceil
$$

）执行结果如下，以整个目录存储模型信息。 (1)

②可以使用np.testing.assert allclose比较预测结果。程序代码如下：

$$
\mathrm{c o d e ~} \texttt{\wedge} \mathrm{m y \_m o d e l ~} \gg
$$

assets
variables
saved model.pb

## (2）加载模型，以变量名称 model2接收。程序代码如下：

1#模型载人
2 model2 = tf.keras.models.load model('my model')
3
4#评分（Score Model）
5 score=model2.evaluate(x test norm, y test, verbose=0) 6
7 for i, x in enumerate(score):
8 print(f'{model2.metrics names[i]}: {score[i]:.4f}')

D执行结果与之前存盘的模型相同

$$
\pi\exists\b< \omicron\mathrm{~ F} :
$$

(3）可以只取得模型结构，不含权重，有以下两种指令

$$
\mathbb{O g e t \_c o n f i g ( ) / f r o m \_c o n f i g ( )_{\circ}}
$$
程序代码如下

1#取得模型结构
2 config = model.get config()
3
4#载人模型结构
5 # Sequential model
6 new model - tf.keras.Sequential.from config(config) 7
8 # function API
9 # new model = tf.keras.Model.from config(config

 $\stackrel{\rightharpoonup} {2} \mathrm{J S O N}$ 。程序代码如下

(5）若有自定义神经层，则需先注册，才能从组态中还原模型，程序代码如下：

(6）加载模型权重，并不包含compile()选项，必须另外补填。程序代码如下：

(7）模型权重存盘：Custom Layer会出现错误。程序代码如下:

## (4）只取得模型权重。程序代码如下

![](figures/273-4-FIGURE.jpg)

| 35 | # Retrieve the config |
| 36 | config = model.get config() |
| 37 |  |
| B8 | # Custom Layer 需注册 |
| B9 | custom objects = {"CustomLayer": CustomLayer, custom activation": custom activation} |
| 40 | with tf.keras.utils.custom object scope(custom objects): |
| 41 | new model = tf.keras.Model.from config(config) |

![](figures/273-6-FIGURE.jpg)

$$
\mathrm{T} :
$$

1 #模型权重存盘，Custom Layer 会出现错误 2 model.save weights('my h5 model.weight')

(8）加载模型权重文件。程序代码如下 ## 详细相关信息可参考「Keras Models AP IW. 这一节我们来研究超参数（Hyperparameters）对效果的影响。在4-1-3节只对单一变量进行了调校，假如要同时调校多个超参数，有一些库可以帮 $\parallel\L$ ，包括Keras Tuner、hyperopt、Ray Tune、Ax等。

本节介绍 Keras Tuner的用法。首先我们要安装库:pipinstal keras-tuner.

请参阅程序：04 17 keras tuner超参数调校 $\mathbf{p y n b}$ 。设计步骤如图4.23所示。

## 4-8 超参数调校

范例.,超参数调校

![](figures/275-5-FIGURE.jpg)

图4.23、超参数调校设计步骤

(1）首先要建立模型，并设定超参数测试的范围学习率测试选项。0.01，0.001，0.0001.
2第一层Dense输出神经元数： $3 2$ 、64、96...512.
程序代码如下：

1）首先要建立模型，并设定超参数测试的范围

D学习率测试选项。0.01，0.01, 0.0001

②第一层Dense 输出神经元数： $3 2$ 、64、96...512
。

$$
\pi_{\pm}^{\Pi} \equiv\Gamma^{\pm} ( \exists, \pi) \Gamma^{\pm} :
$$

）设定callbacks：每个参数组合测试完成后，清除输出显示。

![](figures/276-1-FIGURE.jpg)

(2）调校设定：使月Hyperband)函数设定下列参数。

1目标函数（objective）：准确率。
最大执行周期（max epochs）为5。 执行周期数的递减因子（factor）为3。 ④存盘目录（directory）：my.dir.
项目名称（proiect name）：testl。

D目标函数（objective）：准确率

2最大执行周期（max epochs）为 $5$ 。
O

3执行周期数的递减因子（factor）为 $3$ 。
O

$$
\textcircled{4} \not\equiv\not\equiv\not\equiv\not\equiv\mathrm{( d i r e c t o r y )} ~ : \mathrm{~ m y_{-} d i r_{o} ~}
$$

⑤项目名称（project name testla

$$
\pi_{\pm}^{\Pi} \equiv\Gamma^{\pm} ( \exists\pm\pm\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp)
$$

| 1 | #调校设定，Hyperband:针对所有参数组合进行测试 |  |
| 2 | tuner = kt.Hyperband(model builder, | 模型定义 |
| 3 | objective = 'val accuracy | 日标函数 |
| 4 | max epochs=5. | 最大热行周期 |
| 5 | factor=3, | 执行周期数的递减因子 |
| 6 | directory='n my dir" | 存档目录 |
| 7 | project name = 'test1 | 专案名称 |

 $( 3 )$ 执行参数调校

DFixed：测试所有参数（tune new entries-True) 除了目前的参数，也可以依赖其他参数（(parent name 的设定。只有当其他参数值为特定值时，这个参数才会生效。

hyperparameters:取得最佳参数值

$$
\star\equiv1 \pm\mp\pm\pm\pm\pm\pm\pm
$$

| #参数调校 |
| 2 import IPython |
| 3  |
| #每个参数组台测完后，清除输出显示 |
| 5 class ClearTrainingOutput(tf.keras.callbacks.Callback): |
| 5 def on train end(*args, **kwargs): |
| 7 IPython.display.clear output(wait = True) |
| 8 |
| 0 |
| ) #调校执行 |
| tuner.search(x train norm, y train, epochs= 5. |
| 2 一一一 validation data = (x test norm. v test),#验证数据 |
| 3 一一 #执行每个参数组合后清除显示 callbacks= [ClearTrainingoutputO1)  |
| 1 |
| 6 #显示最佳参数值 |
| 5 best hps = tuner.get best hyperparameters(num trials = 1)ro] |
| 一一一— 7  |
| 3 print(f"最佳参数值\n第一层Dense输出:{best hps.get('units')}\n", |
| 0 "学习率:{best hps.get('learning rate')}") |

最佳参数组合如下。

D第 $- \pi$ Dense输出：160

D学习率:0.001

除此之外，Keras Tuner还有很多功能，介绍如下

1）超参数测试范围的设定，参阅参考文献

DBoolean：真 ${\mathit{M}} \mathrm{F_{X}^{\exists}}$ 
O

QChoice：多个设定选项

3ntFioat：整数/浮点数的连续范国

Sconditiona scope:条件式，类似Fixed，依赖其他参数， 只有当其他参数值为特定值时，这个条件才会生效。

②RandomSearch：若测试范围过大，可随机抽样部分组合， 加以测试

(3）Oracle：超参数调校的算法，为测试方法（Tuners）的参数，可决定测试方法的下次测试组合，读者可参阅参考文献”

另外再加码推荐，可搭配Hiplot可视化库，显示每一种参数组合的设定值与损失/准确度，用户需先安装库：pip install hiplot (2）测试方法 $( \mathrm{T u n e r s} )$ 请参阅参考文献

DHyperband：测试所有组合。

3BayesianOpimization：搭配高斯过程（Gaussian process 依照前次的测试结果，决定下次的测试内容

(1）解析 Keras TUuner测试的日志文件。程序代码如下：

| 1 | #解析Keras Tuner测试的日志文件 |
| 2 | import os |
| 3 | import json |
| 4 |  |
| 5 | vis data= [] |
| 6 | #扫描目录内每一个文件 |
| 7 | rootdir = 'my dir/test1' |
| 8 | for subdirs, dirs, files in os.walk(rootdir): |
| 9 | 衣 for file in files:  |
| 10 | if file.endswith("trial.ison"): |
| 11 | with open(subdirs t '/" + file, 'r') as json file: |
| 12 | data= json file.read() |
| 13 | vis data.append(json.loads(data)) |

(2）显示参数组合与测试结果。程序代码如下: 执行结果：uid为执行代码，可以看出第一列 $\mathrm{( u i d=7 )}$ 有最高的准确率，获选为最佳参数组合，如图4.24所示

参数调校是深度学习中非常重要的步骤，因为深度学习是一个黑箱科学，加上我们对于高维数据的联合概率分布也不熟悉，唯有通过大量的实验，才能获得较佳的模型。但困难的是，模型训练的执行非常耗时，如何通过各种方法或库的协助，缩短调校时间，是工程师建构A模型时须思考如何改善的课题。

![](figures/279-2-FIGURE.jpg)

![](figures/279-3-FIGURE.jpg)

图 4.24 显示参数组合与测试结果

除了建构模型外，TensorFlow 还贴心地提供了各种工具和指令，方便在程序开发流程中使用，包括前置处理、模型存盘/加载绘制、除错等功能，现在我们就来学习这些功能吧。

## 第5章

## TensorFlow 其他常用指令将类别变量转为多个虚拟变量（Dummy Variable），每个虚拟变量 $\P$ 含真/假值（1/0），为避免被算法误 $\grave{\i} \lambda$ ，该变量类别有顺序大小之分，如颜色红、蓝、绿会被转换成三个变量「是红色吗」 $\slash$ 是蓝色吗」/「是绿色吗」，而非 $1 / 2 / 3_{\circ}$ .

(2）num classes：类别个数，此参数可以不设定，函数会从数据中统计出类别个数。如下

## 5-1 特征转换

## 1. One-hot encoding

请参阅程序：05.01特征转换ipynb。程序代码如下：

1 # One-hot encoding
2 # num classes:类别个数，可不设定
3 tf.keras.utils.to categorical([0, 1, 2, 3], num classes=9)

）执行结果如下 $9$ 种类别，即产生 $9$ 个变量 (1) 指定 1

| array([[1., 0 | 0., 0 | 0.], |
| [0., 1 | 0., 0 | 0.], |
| [0., 0 | 1., 0 | 0.], |
| [0., 0 | 0., 1 | 0.]], dtype=float32) |

修改 MNIST手写阿拉伯数字辨识程序如下

(2）第23行损失函数使用 categorical crossentropy，而非 sparse categorical crossentropy，因后者的y不需 One-hot
encoding 转换

将所有数据标准化（Standardization) 使数据转换为标准正态分布N $( 0, 1 )$ ，类似于 sklearn 的 StandardScaler()。程序代码如下：

![](figures/282-2-FIGURE.jpg)

(1）第10-11行对y对 One-hot encoding转换

## 2.正态化（Normalization 执行结果：平均数： $\ 0. 0 0$ ，标准差： $1. 0 0$ a 模型存盘，可以存储下列信息：①模型结构与组态；②权重，含偏差项；③模型compile 选项：④优化器的状态，这样训练即可白断点处继续执行。

TensorFlow SavedModel格式：官方建议采用这种格
(1)
式，以目录存储，目录下含多个文件，各司其职。

(2）Keras H5格式：Keras 库既有格式，以单一文件存储， 注意，此种格式无法存储自定义的神经层

均使用 model.save（< file path>）指令，如果设定扩展名
为.h5，则存档成Keras 既有格式。加载模型时调用load model(e file path>）指令。

## 5-2 模型存盘与加载

目前支持以下两种格式

范例，模型存盘与加载

请参阅程序：05 02 模型存盘与加载.iynb

$$
\star\equiv\lfloor\pm\lfloor\exists\pm\rfloor\rceil\rceil
$$

）执行结果如下，以整个目录存储模型信息。 (1)

②可以使用np.testing.assert allclose比较预测结果。程序代码如下：

$$
\mathrm{c o d e ~} \texttt{\wedge} \mathrm{m y \_m o d e l ~} \gg
$$

assets
variables
saved model.pb

## (2）加载模型，以变量名称 model2接收。程序代码如下：

1#模型载人
2 model2 = tf.keras.models.load model('my model')
3
4#评分（Score Model）
5 score=model2.evaluate(x test norm, y test, verbose=0) 6
7 for i, x in enumerate(score):
8 print(f'{model2.metrics names[i]}: {score[i]:.4f}')

D执行结果与之前存盘的模型相同

$$
\pi\exists\b< \omicron\mathrm{~ F} :
$$

(3）可以只取得模型结构，不含权重，有以下两种指令

$$
\mathbb{O g e t \_c o n f i g ( ) / f r o m \_c o n f i g ( )_{\circ}}
$$
程序代码如下

1#取得模型结构
2 config = model.get config()
3
4#载人模型结构
5 # Sequential model
6 new model - tf.keras.Sequential.from config(config) 7
8 # function API
9 # new model = tf.keras.Model.from config(config

 $\stackrel{\rightharpoonup} {2} \mathrm{J S O N}$ 。程序代码如下

(5）若有自定义神经层，则需先注册，才能从组态中还原模型，程序代码如下：

(6）加载模型权重，并不包含compile()选项，必须另外补填。程序代码如下：

(7）模型权重存盘：Custom Layer会出现错误。程序代码如下:

## (4）只取得模型权重。程序代码如下

![](figures/286-4-FIGURE.jpg)

| 35 | # Retrieve the config |
| 36 | config = model.get config() |
| 37 |  |
| B8 | # Custom Layer 需注册 |
| B9 | custom objects = {"CustomLayer": CustomLayer, custom activation": custom activation} |
| 40 | with tf.keras.utils.custom object scope(custom objects): |
| 41 | new model = tf.keras.Model.from config(config) |

![](figures/286-6-FIGURE.jpg)

$$
\mathrm{T} :
$$

1 #模型权重存盘，Custom Layer 会出现错误 2 model.save weights('my h5 model.weight')

(8）加载模型权重文件。程序代码如下 ## 详细相关信息可参考「Keras Models AP IW. 如果设计一个复杂的模型，通常会希望有可视化的呈现，让我
，利于检查结构是否完整，TensorFlow 提供汇总信息， 们一三了然，
 $\operatorname\mathcal{Y}$ 表格显示，再提供模型结构图，帮助我们厘清整体流程。

(1）显示模型汇总：下列指令可取得汇总信息表格，包含每一层的输 $\lambda$ 、输出、参数个数。程序代码如下

## 5-3 模型汇总与结构图

范例。显示模型汇总与绘制结构图

请参阅程序：05 03_模型汇总与结构图Ipynb

$$
1 \mod\! \mathrm{e \, l \,. \, s u m m a r y \, ( \, ) \,}
$$

## 执行结果如下：

| ISUAnI+mAY | A+A+Chana | DAnSm# |
| Layer (type) Output Shape Param # |
| flatten (Flatten) | (None,784) | 0 |
| dense(Dense) | (None，256) | 200960 |
| dropout (Dropout) | (None,256) | 0 |
| dense 1 (Dense) | (None,10) | 2570 |
|  |  |  |
| Total params: 203,530 |
| Non-trainable params: o |

## (2）取得神经层信息。

D使用索引取得神经层信息。程序代码如下： 9绘制结构图：要绘制模型结构，需先完成以下步骤，才能顺利绘制图形。

$$
\mathrm{h t t p s :} / \mathrm{/ w w w. g r a p h v i z. o r g / d o w n}
$$
load，再把安装目录下的bin路径加
到环境变量Path 中

⑤以下绘图指令会先产生描述向量图的文本文件（.dot) 再使用 graphviz 的 dot.exe，将.dot 文件里的内容转化为图像文件 $\mathrm{(. p n g )}$ 程序代码如 $\mathrm{T}$ :

2使用名称取得神经层信息。程序代码如下：

3取得神经层权重。程序代码如下：

1#取得神经层权重
$$
2 \mod_{\mathrm{I \,. g e t \, \_\, I a y e r \, ( n a m e=` d e n s e " ) \,. w e i g h t s}}
$$

 $\clubsuit$ 安装graphviz软件，网址为：

 $\bigoplus$ 安装两个Python F:pipinstal raphvi 和 pip instal pydotplus.

6加上不同的参数，可显示各种的额外信息，例如

show shapes-True 显示输入/输出的神经元个数。

show cdtypeTrue 显示输入/输出的数据类型。

to fle- model.ang」：同时存档。 ## 程序代码如下：

| 1 | # 绘制结构图 |
| 2 | # show shapes=True:可显示输人/输出的神经元个数 |
| 3 | # show dtype=True:可显示输人/输出的数据类型 |
| 4 | #to file:可同时存档 |
| 5 | tf.keras.utils.plot model(model, show shapes=True, show dtype=True, |
| 6 | to file="model.png") |

## ⑦执行结果如下

![](figures/290-3-FIGURE.jpg)

⑧也可以产生dot格式文件。程序代码如下：

1#产生dot 格式及 $p n g$ 格式文件
2 import pydotplus as pdp
3 from IPython.display import display, Image
4
5 $\# \not=\pm\ d o t$ 格式文件
6 dot1 = tf.keras.utils.model to dot(model, show shapes=True, show dtype=True) 7 $\# \not=\pm p n g$ 格式文件
8 display(Image(dot1.create_png()))

如图5.1所示回调函数（Callbacks）是在模型训练过程中埋入要触发的事件，在每一个周期执行之前与之后，都可以使用
Callback 区数，TensorFlow/Keras提供了许多类型的 Callback采数，功能如 $\mathbf{F}$ 。

还有更多的其他功能，请参阅Keras官网的 Calback介绍? 除了内建Callback 函数，也能够自定义 Callback函数。

## 5-4 回调函数

(1）在训练过程中记录任何信息
(2）在每个检查点（Checkpoint）进行模型存盘。
(3）迫使训练提前结京。
(4）结合TensorBoard可视化工具，实时监看训练过程。 (5）将训练过程产生的信息写入CSV文件。
(6）使用其他PC 远程监控训练过程。

(1）在训练过程中记录任何信息。

(2）在每个检查点（Checkpoint）进行模型存盘。

 $( 3 )$ 迫使训练提前结束

(4）结合TensorBoard可视化工具，实时监看训练过程

(5）将训练过程产生的信息写入CS $\mathsf{V}$ 文件。

(6）使用其他PC远程监控训练过程

![](figures/291-10-FIGURE.jpg)

图 $5$ 』回调函数

透过Callback函数可以完全解构模型训练的过程，我们使用一些范例，说明各类型 Callback函数的用法。

EarlyStopping 用于设定训练提前结束的条件，我们可以设定较大的执行周期数，并搭配训练提前结束的条件，当训练效果一段时间内没有持续改善时，就可以提前结束训练，这样就能兼顾训练效果与训练时间了。

请参阅程序：05 04 Callback.ipynb。设计流程如图 $5. 2$ 所 $\overline{{\Pi}}$ .

(1）定义Gallback函数为「要是连续三个执行周期validation accuracy 没改善就停上训练程序代码如下：

执行结果：如图 $5. 3$ 所示。预计训练20次，但实际只训练了 $1 2$ 注意：每次执行结果可能不同。
次就停止了。

## 5-4-1 EarlyStopping Callbacks

## 范例.实际接操作Earlystopping Callbacks.

![](figures/293-6-FIGURE.jpg)

图 $5$ 2设计流程

![](figures/293-8-FIGURE.jpg)

(2）效果指标也可以改为验证的损失 $\mathrm{( v a l \_l o s s )}$ 只要连续三次没改善停上训练。程序代码如下：

![](figures/294-1-FIGURE.jpg)

图5.3，定义Callback 函数执行结果

![](figures/294-3-FIGURE.jpg)

执行结果：如图5.4所示

![](figures/294-5-FIGURE.jpg)

图5.4：修改效果指标执行结果

若训练过程过长，难免会发生训练到一半就断掉的状况，因此，我们可以利用ModelCheckpoint Callback，在每一个检查点存档，再次执行时，就可以从断点延续，继续训练。

(2）再执行3个周期，准确率会接续上一次的结果，继续改善。程序代码如 $\mathrm{F}$ :

## 5-4-2 ModelCheckpoint Callbacks

范例.实际操作 ModelCheckpoint Callback

请参阅程序：05 04 Callback.ipynb

(1）定义ModelCheckontoallback。、程序代码如下：

| 1 | #定义 ModelCheckpoint callback |  |
| 2 | checkpoint filepath = 'model.{epoc | ：02d}.h5'#存档名称，可用f-string 变量 |
| 3 | model checkpoint callback = tf.ker | s.callbacks.ModelCheckpoint( |
| 4 | filepath=checkpoint filepath, | 设定存档名称 |
| 5 | save weights only=True, | 只存权重 |
| 6 | monitor='val accuracy', | 监看验证数据的准确率 |
| 7 | mode='max', | 设定save best only=Truey,best是指 max or min |
| 8 | save best only=True) | 只存最好的模型 |
| 9 |
| 10 | EPOCHS=3 | 训练3丸 |
| 11 | model.fit(x train norm, y train, e | ochs=EPOCHS, validation split=0.2, |
| 12 | callbacks=[model checkpa | nt callback]) |

## 执行结果如下，最后的准确率等于0.9859

| Epoch 1/3 |
| 1500/1500 | ] | -6s | 4ms/step - | loss: | 0.0652 | -accuracy: | 0.9796 - | val loss: | 0.0521 | - val accuracy: |
| 0.9843 |
| Epoch 2/3 |
| 1500/1500 | ===== :=======] | -5s | 3ms/step- | loss: | 0.0520 | -accuracy: | 0.9835 - | val loss: | 0.0502 | - val accuracy: |
| 0.9844 |  |  |  |  |  |  |  |  |  |  |
| Epoch 3/3 |  |  |  |  |  |  |  |  |  |  |
| 1500/1500 | ] | -5s | 3ms/step - | loss: | 0.0431 | -accuracy: | 0.9859 - | val loss: | 0.0450 | - val_accuracy: |
| 0.9863 |  |  |  |  |  |  |  |  |  |

执行结果如下，最后的准确率等于0.9902

TensorBoard 是 TensorFlow 所提供的可视化诊断工具，功能非常强大，除了可以显示训练过程外，也能够显示图 $\breve{\Pi}$ 、语音及文字信息。将TensorBoard 整合至 Calback事件里，就可以在训练的过程中启动TensorBoard 网站，实时观看训练信息、

(3）使用浏览器输入以下网htp:/localhost:6006/，即可观看训练信息。界面如图5.5所示

## 5-4-3 TensorBoard Callbacks

范例实际操作 TensorBoard Callback.

请参阅程序:05 04 Callback.ipynb

(1）定义触发事件。程序代码如下

| 1 #定义 tensorboard callback |
| 2 tensorboard callback = [tf.keras.callbacks.TensorBoard(log dir='.\\logs |
| 3 histogram freq=1)] |
| 4 |
| 5 #2 训练5丸 |
| 5 history = model.fit(x train norm, y train, epochs=5, validation split=0.2, |
| 7 callbacks=tensorboard callback) |

(2)启动TensorBoard网风站: tensorboard-logdir-.logs 单击「Graph」选项，可以观察模型的运算图，显示模型的运算顺序，如图5.6所示

![](figures/298-1-FIGURE.jpg)

图 $5. 5$ 观看训练信息

![](figures/298-3-FIGURE.jpg)

图 $5. 6$ 观察模型的运算图

请注意：model.fit的 Callback参数值是一个list，一次可以加 $\lambda$ 多个 Galback函数，在训练时一并触发。同时设定提前结束训练、检查点及TensorBoard。程序代码如 $\mathrm{F}$ :

如果内建的 Callback 不能满足需求，也可以自定义
Callback，触发时机可包含训练、测试、预测阶段的之前与之后

$$
( 1 ) \mathrm{\ o n}_{-} \mathrm{( t r a i n \vert t e s t \vert p r e d i c t )} \mathrm{\ \_~ t h e r t ~}
$$
pegin：训练、测试及预测开始
前，可触发事件。

$$
{\bf( 2 )} \, \mathrm{\ o n}_{-} \, \mathrm{( t r a i n \vert t e s t \vert p r e d i c t )}
$$
end：训练、测试及预测结束
后，可触发事件。

$$
( 3 ) \mathrm{\ o n}_{-} \mathrm{( t r a i n \vert t e s t \vert p r e d i c t )} \mathrm{\ \_~ b c.}
$$
atch begin：每批训练、测试
及预测开始前，可触发事件

$$
\mathrm{( 4 )} \, \mathrm{\ o n}_{-} \, \mathrm{( t r a i n \vert t e s t \vert p r e d i c t )} \, \ \_\mathrm{\ b u l p h a t}
$$
atch end：每批训练、测试及
预测结束后，可触发事件。

(1）定义触发的时机及动作（Action） 以下动作只单纯显示文字信息，实际运用时可取得当时的状态或统计量写入工作记录文件。程序代码如下：

## 5-4-4 自定义Callback

(5） on epoco begin：每个执行周期开始前，可触发事件

(6） on epoch end：每个执行周期结束后，可触发事件

范例.自定义 Callback实践

请参阅程序:05 05 Custom Callback.pynb

| 3 4 5 C |  | self.task type'' self.epoch=0 self.batch=0  |  |
| 6 7 8 9 | de | f on train begin(self, logs=None): self.task type='训练 print("训练开始..."）  |  |
| 1 11 12 A一 | de | f on train end(self, logs=None): print("训练结束."）  |  |
| 13 14 15 16 | de | f on epoch begin(self, epoch, logs=None): self.epoch=epoch print(f"{self.task type}第{epoch} 执行周期开始.. | " |
| L 18 19 A | de | f on epoch end(self, epoch, logs=None): print(f"{self.task type}第 {epoch} 执行周期结束." |  |
| 20 21 22 23 | de | f on test begin(self, logs=None): self.task type='测试" print("测试开始...")  |  |
| 24 25 26 | de | f on test end(self, logs=None): print("测试结束."）  |  |
| 27 28 29 30 31 | de | f on predict begin(self, logs=None): self.task type="预测" print("预测开始...")  |  |
| 31 32 33 2A | de | f on predict end(self, logs=None): print("预测结束."）  |  |
| 35 36 37 | de | f on train batch begin(self, batch, logs=None): print(f"训练第{self.epoch}执行周期，第{batch} | 批次开始...") |
| 38 39 40 | de | f on train batch end(self, batch, logs=None): print(f"训练第{self.epoch} 执行周期，第{batch} | 批次结束.") |
| 41 42 43 | de | f on test batch begin(self, batch, logs=None): print(f"测试第{self.epoch} 执行周期，第{batch} | 批次开始.." |
| 44 45 46 | de | f on test batch end(self, batch, logs=None): print(f"测试第{self.epoch} 执行周期，第{batch} | 批次结束.") |
| 47 48 1O | de | f on predict batch begin(self, batch, logs=None): print(f"预测第{self.epoch} 执行周期，第{batch} | 批次开始...") |
| T 50 51 52 | de | f on predict batch end(self, batch, logs=None): print(f"预测第{self.epoch}执行周期，第{batch} | 批次结束.") |

(2）在训练、测试、预测使用此Callback。程序代码如下： 1#训练
2 model.fit(
3 x train norm, y train, epochs=5,
4 batch size=256, verbose=o,
5 validation split=0.2, callbacks=[CustomCallback()] 6
7
8#测试
9 model.evaluate(
10 x test norm, y test, batch size=128,
11 verbose=0, callbacks=[CustomCallback()]
12
13
14 #预测
15 model.predict(
16 x test norm, batch size=128,
17 callbacks=[CustomCallback()]
18 )

## 训练显示结果如下：

训练开始...
训练第 $\diamond$ 执行周期开始...
训练第 $\diamond$ 执行周期，第 $\diamond$ 批次开始... 训练第 $\diamond$ 执行周期,第 $\diamond$ 批次结束， 训练第 $\diamond$ 执行周期,第1批次开始.. 训练第 $\diamond$ 执行周期,第1批次结束。 训练第执行周期,第2批次开始.. 训练第执行周期,第2批次结束。 训练第0执行周期,第3批次开始... 训练第 $\diamond$ 执行周期，第3批次结束。 训练第 $\circ$ 执行周期,第4批次开始... 训练第 $\circ$ 执行周期，第 $4 ~$ 批次结束， 训练第0执行周期,第5 批次开始... 训练第0执行周期,第5批次结束。 训练第执行周期,第6批次开始... 训练第执行周期,第6批次结束。 训练第 $\circ$ 执行周期,第7批次开... 训练第 $\diamond$ 执行周期,第7批次结束。 训练第 $\diamond$ 执行周期，第 $8$ 批次开始... 训练第执行周期,第 $8$ 批次结束。 训练第执行周期,第9批次开始... 训练第 $\circ$ 执行周期,第9批次结束.

## 测试显示结果如下：

$$
\mu\nu=\mu\pm\mu\hspace{. 5 i n} ; \Pi\mid\tau\neq\pm\hspace{. 5 i n} ; \Pi\mid\tau\mid\mp\tau\leq\ldots
$$

测试第 $\diamond$ 执行周期，第 $\diamond$ 批次开始... 测试第 $\diamond$ 执行周期，第 $\diamond$ 批次结束. 测试第 $\diamond$ 执行周期,第1批次开始... 测试第 $\diamond$ 执行周期,第1批次结束。 测试第 $\diamond$ 执行周期,第2批次开始... 测试第 $\diamond$ 执行周期,第2批次结束。 测试第 $\diamond$ 执行周期,第3批次开始... 测试第 $\diamond$ 执行周期,第3批次结束. 测试第 $\diamond$ 执行周期,第4批次开始.. 测试第 $\diamond$ 执行周期,第4批次结束。 测试第 $\diamond$ 执行周期,第5批次开始... 测试第 $\diamond$ 执行周期,第5批次结束。 测试第 $\diamond$ 执行周期,第6批次开始.. 测试第 $\diamond$ 执行周期，第 $6$ 批次结束。 测试第 $\diamond$ 执行周期，第7批次开始.. 测试第 $\diamond$ 执行周期,第7批次结束测试第 $\diamond$ 执行周期，第 $8_{8}$ 批次开始... 测试第 $\diamond$ 执行周期,第
批次结束， 范例.通过自定义的 Callback，在每一批训练结束时记录损失，就可以在整个训练过程结束后依据收集的数据绘制线图

 $( 1 )$ 在每一批的训练结束后记录损失至Pandas DataFrame 中。程序代码如 $\mathrm{F}$ :

(2）图表显示的结果显 $\overline{{\Pi}}$ ，优化的过程中损失函数并不是一路递减，而是起起伏伏，但整体趋势为向下递减，如图5.7所示

(3）依执行周期作小计，取最小损失值，画出由线图，如图 5.8 所示

## 5-4-5 白定义Callback 应用

请参阅程序：05 Custom Callback O 06 loss.ioynb

![](figures/303-6-FIGURE.jpg)

![](figures/303-7-FIGURE.jpg)

图 $5. 7$ 每一批训练结束后记录损失

![](figures/304-0-FIGURE.jpg)

图 $5. 8$ 取最小损失值

借由自定义 Callback，我们能够更深入理解优化的过程。除 $\overline{{\jmath}}$ 损失函数以外，要取得其他信息也是可行的，比如可以在Callback 中使用selif.model.get weights()函数来取得每一神经层的权重，另 $\mathcal{Z} \vert$ ，自定义Callback 还有个便捷的功能，就是用来除错

(Debug) 假如训练出现错误，如Nan 或优化无法收敛的情形， 就可以利用Calback逐批检查，更多Callback的用法，可参阅 TKeras Gallback AP 

## 5-4-6 总结

TensorBoard 是一种可视化的诊断工具，功能非常强大，可以显示模型结构、训练过程，包括图 $\ \#$ 、文字和音频数据。在训练的过程中启动TensorBoard，能够实时观看训练过程。 $\mathrm{P y}$ Torch 安装时所包含的TensorBoardX，与TensorBoard 功能相似，这表明 TensorFlow和 PyTorch 虽然是竞争者，但TensorBoard仍然是 PyTorch 开发团队也不得不承认的优秀工具。

## 5-5 TensorBoard (1）追踪损失和准确率等效果衡量指标，并以可视化呈现如图 $5. 9$ 所示。

(2）显示运算图（Computational Graph 包括张量运算和神经层，如图5.10所示。

## 5-5-1 TensorBoard 功能

TensorBoard包含以下功能。

![](figures/307-4-FIGURE.jpg)

 $5. 9$ 追踪效果指标并呈现图

(3）直方图（Histogram) 显示训练过程中的权重、偏差的概率分布，如图5.11所示。

(4）词嵌入（Word Embedcing）展示：把词嵌入向量降维, 投影到三维空间来显示。画百右边可输入任意单字，如 king就会出现图5.12所示情形，将与其相近的单字显示出来，其原理是通过 Word2Vec将每个单字转为向量，再利用 Cosine Similarity计算相似性，详情会在后续章节介绍。

![](figures/308-2-FIGURE.jpg)

图 $5. 1 0$ 显示运算图

![](figures/308-4-FIGURE.jpg)

图 $5$ .1直方图

(5) F1 文字和音频数据，如图5.13所示显示图片、

![](figures/309-1-FIGURE.jpg)

图 $5. 1 2$ 词嵌入展 $\overline{{\Pi}}$ 

![](figures/309-3-FIGURE.jpg)

图 $5. 1 3$ 显示图片

(6）剖析TensorFlow程序流程

之前在 moel.it)内指定TensorBoard Callback，可写入工作日志文件（Log），内容类是固定的，如果要自定义写入的内容， 可以直接在程序中写 $\lambda$ 工作日志文件,但是不能使用model.fit) 要改用自动微分（tf.GradientTape）的方式训练。

范例。同样用 MNST辨识作测试，前面加载数据与建立模型程序代码的流程不变， $\mathcal{M}$ compile()开始做一些调整，下面仅列出关键的程序代码，完整的程序请参阅05 07TensorBoard.ipynb

(1）先设定优化器、损失函数、效果衡量指标的类别。程序代码如 $\mathrm{T}$ :

(2）写入效果衡量指标：使用自动微分（If.GradientTape）的方式训练模型。程序代码如下：

## 5-5-2 测试

![](figures/310-5-FIGURE.jpg)

(3）使用 f.summary.create file writer 开启工作日志文件程序代码如下：

(4)使用tf.summary.scalar写 $\lambda$ 字符串名称 $\mathrm{( N a m e )}$ 及数值（Value）至工作日志文件。程序代码如下：

(5）在终端机或DOS 内执行以下指令，启动Tensorboard 网站：tensorboard--logdir logs/gradient tape

$$
( 6 ) \, \pm\mp\mathrm{J l} \lambda\mp\mathrm{J u p y t e r ~ N o t e b o n}
$$
ok内启动，分为以下两个步
骤。

个先加载「TensorBoard notebook extension」扩充程序。程序代码如下：

![](figures/311-5-FIGURE.jpg)

(3）使用f.summary.oreate file writer开F启工作日志文件。

| 10 | train summary v writer = tf. summary.create file v writer(train logc dir) |
| 11 | test summaryv writer = tf.summary.create file 二 writer(test log dir) |

10 with train summary writer.as default():
11 tf.summary.scalar('loss', train loss.result(), step=epoch)
12 tf.summary.scalar("accuracy", train accuracy.result(), step=epoch) 注意： $\not\supset$ 为Jupyter Notebook的魔术方法前置符号，在终端机或DOS启动时，不需 $\not\sim$ 。

(7）使用浏览器输入网 tt:/ocahost:6006/，即可观看训练信息。

2启动Tensorboard 网站。程序代码如下：

 $^1_{2}$ #启动 $\mathrm{T e n s o r b o a r d}$ logs/gradient tape %tensorboard --logdir

范例。除了训练过程的信息，也可以随时把数据写 $\lambda$ 工作日志文件，以下示范如何将图片写 $\lambda$ 工作日志文件。

## 5-5-3 写入图片

(1）设定工作日志文件目录，写入图像。程序代码如下：

![](figures/313-3-FIGURE.jpg)

(2）启动Tensorboard 网站：tensorpoard -logdir logs/train data.

(3）使用浏览器观看图片（在lmages 选项 htto://locahost:6006/。

(4）执行结果：如图5.14所示。

![](figures/313-7-FIGURE.jpg)

图5.14写入图片执行结果

在后续章节介绍卷积神经网络（CNN）时，可以把卷积转换后的图片写 $\lambda$ ，以了解训练过程中图片是如何被转换的，有助于理解神经网络这个黑箱是如何辨识图片的，这就是所谓的「可解释的
A
$$
\mathrm{~ I \rfloor~} \quad( \mathrm{~ E x p l a i n a b l e ~ A I, X A I ) ~}
$$

要显示训练过程中的权重、偏差分布相当简单，只要在定义 TensorBoard Callback时加一个参数『histogran freo-l」就完成 $\overline{{\jmath}}$ ，意思是每一个执行周期绘制一张直方图

启动Tensorboard 网站，单击「histograms」选项，显示如下，前方的直方图为训练的最后结果，如图5.15所示

## 5-5-4、直方图

请参阅程序：05 04 Callback.iynb,

$$
\star\Xi\equiv\pm\pm\Xi\pm\pm\Gamma:
$$

![](figures/315-5-FIGURE.jpg)

图 $5. 1 5$ 直方图执行结果

$$
( 1 ) \textit{\ T e n s o r B o a r d} \b\Xi\llcorner
$$
eras Tuner类似，首先设定多个调校的
参数组合。程序代码如下

0启动TensorBoard网站，单击「『hparams」选项，显示如图 5.16所示第 $6$ [
回合准确率最佳

## 5-5-5 效果调校

范例。效果调校，找出最佳参数组合

请参阅程序:05 03 TensorBoard Tuning.ipynb

![](figures/316-5-FIGURE.jpg)

## (2）每一参数组合训练一个模型。程序代码如下：

![](figures/316-7-FIGURE.jpg)

![](figures/317-0-FIGURE.jpg)

图 $5. 1 6$ 启动网站

2详细信息如图5.17所示

![](figures/317-3-FIGURE.jpg)

图 $5. 1 7$ 详纸 $1 \vec{\Xi}$ 息

®从图5.18所示的粗体线可以找到最佳参数。
 $\bullet$ dropout rate-0.2.
 $\bigoplus$ 输出神经元数（nurm units） -32。
 $\bigoplus$ 优化器（optimizer）-adam.

® ${\cal M}$ 图5.18所示的粗体线可以找到最佳参数

$$
\bullet\quad\mathrm{d r o p o u t ~ r a t e=0. 2_{\circ} ~}
$$

输出神经元数（num units）-32

 $\bigoplus$ 优化器（optimizer)-adam $\bigoplus$ 获得最佳准确度0.9775

![](figures/318-1-FIGURE.jpg)

图 $5. 1 8$ 寻找最佳参数

(1）在图5.19 左边的字段中修改任意一个观察值，重新预测，即可观察变动的影响。

(2）单击「Partial dependence plots」选项，可以了解个别特征对预测结果的影响，如图5.20所示。

## 5-5-6 敏感度分析

敏感度分析能帮助我们更了解分类与回归
（Classification
 $( \mathrm{R e g r e s s i o n} )$ 模型，它拥有许多强大的功能，介绍如下

![](figures/319-4-FIGURE.jpg)

图 $5. 1 9$ 修改观察值，观察变动影响

![](figures/319-6-FIGURE.jpg)

图5.20观察个别特征对预测结果的影响

(3）切割训练数据数量了解测试资料预测结果的敏感度分析，如图5.21所示

详细操作说明可参考A Waikthrough with UCI Census Data [3，范例4可在Colaboratory环境中执行，这是一个二分类的模 $\sqsubseteq\underline{{\exists}}$ ， 因此笔者就不多作说明 $\overline{{\jmath}}$ 。
由于该范例不属于深度学习模型，

![](figures/320-1-FIGURE.jpg)

 $5. 2 1$ 切割训练资据数量图

TensorBoard 随着时间增加的功能越来越多，以上我们只做 $\overline{{\jmath}}$ 很简单的实验，如果需要更详细的信息，读者可以参阅
TensorBoard官网的指南’

## 5-5-7 总结

(3）边缘运算（IoTHub) 例如要侦测全省的温度，我们会在各县市安装上千个传感器，每个loT Hub会负责多个传感器的信号接收、初步过滤和分析，分析完成后再将数据后送到数据中心。

其呈现的方式可能是网页、手机.Ap 或桌面程序，下面先就网页开发进行说明。

## 5-6模型部署与 Tensorlow Serving

一般深度学习模型安装的选项如下。

(1）本地服务器（LoalSrver O

(2）云端服务器（Cloud Server O

若是自行开发网页程序，并且安装在本地服务器，则可以运用 $\mathrm{P y}$ thon框架，如Diango、Flask或 Streamlit，快速建立网页。其中以Streamlit最为简单，用户不需要懂得
HTML/CSS/Javascript，只靠Python 就可以搞定一个初阶的网站，下面我们实际建立一个手写阿拉伯数字的辨识网站。

(3）网页显示后，拖曳 MyDigits 目录内的任一文件至画面中的上传图文件区域，就会显示辨识结果，用户也可以使用画图等绘图软件书写数字，如图5.22所示

## 5-6-1自行开发网页程序

完整程序请参阅05 09 web.py

 $( 1 )$ 安装Streamlt包 pip instal sremlt

(2）执行Python程序，必须以streamlitrun开头，而非 python执行，如streamit run 05 10 web.py.

![](figures/323-6-FIGURE.jpg)

图 $5. 2 2$ 网页显示

程序代码说明如下。

（1）加载相关库。程序代码如下 2第25行：RGB的白色为255，但训练数据MNIST的白色为 $0$ ，故需反转颜色

## (2）模型加载。程序代码如下

(3) 。程序代码如下上传图片。

15#上传图片
16 uploaded file = st.file_uploader("上传图片(.png)）"，type="png")

(4）文件上传后，执行以下工作。程序代码如下：

D第18~23行：把图像缩小成宽高为（28,28 O

3第28行：辨识上传文件。

| 17 | if uploaded file is not None: |
| 18 | #读取上传图片文件 |
| 19 | imagel = io.imread(uploaded file, as gray=True) |
| 20 | #编小图形为(28,28) |
| 21 | image resized = resize(imagel, (28, 28), anti aliasing=True) |
| 22 | #插人第一维，代表批数 |
| 23 | Xl = image resized.reshape(1,28,28,1) |
| 24 | #颜色反转 |
| 25 | X1=np.abs(1-X1 |
| 26 |  |
| 27 | #预测 |
| 28 | predictions = model.predict classes(X1)ro] |
| 29 | #显示预测结果 |
| 30 | st.write(f'预测结果:{predictions}') |
| 31 | #显示上传图片文件 |
| 32 | st.image(imagel) |

另外用户也可以直接利用 TensorFlow Serving 架设网页服务接 $\sqtharpoondown$ ，它是一个高效的服务系统，不需撰写程序，就可提供 $\mathrm{A P I}$ ，让外界程序调用，支持gPRG、RESTAP」两种协议。依照文件说明，它日前只文持Linu×系统，因此，笔者以Windows 内建的 Linux subsystem--WSL环境进行实验。

先看一个简单的例子，假使要部署至云端服务器或IoT Hub, 考虑到需要在短时间内部署很多台，通常会选择使用容器
(Container）架构，建立虚拟机。我们以TensorFlow Serving 官网的案例说明。具体流程如图5.23所示。

(1）安装Docker：有关Docker的安装在此不作说明，笔者以Windows内建的 $\mathrm{W S L 2}$ 所整合的 Docker DeskTop 来说明整个程序。

(2）从Docker Repository下载一个TensorFlow Serving的 lmage: docker pull tensoriow/serving

## 5-6-2 TensorFlow Serving

![](figures/325-5-FIGURE.jpg)

 $5. 2 3$ 操作流程图

(3）使用Git指令复制TensorFlow Serving 程序代码：git clone https://github.com/iensorilow/serving

(4）指定模型：模型范例名称为half plus two，顾名思义就是将输入除以 $2$ ，再加 $2$ ，即：TESTDATA= 「S
$$
\mathrm{( p w d ) ~ / s e r v i n g / t e n s o r f l o w \_~ s}
$$
erving/servables/tensorlow/testdata

 $( 5 )$ 启动下载的Image：下列指令为同一行，提供REST APl: docker run -t --rm -0 8501: 8501 -v

$$
\begin{array} {c} {\mathrm{T g T E S T D A T A / s a v e d \_~ m o d e}} \\ {\mathrm{U s \_~ t w o d d} \quad-\mathrm{e ~ M O D E L \_~ N A M E}} \\ {\mathrm{\&_{o}}} \\ \end{array}
$$
half plus two cpu: /models/half p
tshalf plus two tensorflow/serving

(6）测试数据预测：使用curl 送出三批数据分别为1.0、2.0 $5. 0$ ，进行预测，下列指令为同一行：curl-d 『{「instances
[1.0, 2.0, 5.0]}J -X POST hitp:/localhost:
8501/v1/models/half plus two: predict.

(8）修改第 $6$ 步骤的数据[10.0，2.0，5.0]，得到预测结果：
$$
\{~ \mathrm{\lceil~ p r e d i c t i o n s \rfloor} ~ : ~ [ 7. 0, ~ \mathrm{\ 3. 0},
$$
4.5]}.

通过上述步骤，如法炮制，换上用户自己的模型，程序是一样的。

(7传回预测结
$$
\mathrm{\# \! : ~ \{~ \ r h e d i c t i o n s \j~ : ~ [ 2. 5, ~ \ 3. 0, ~ \ 4. 5 ] ~ \}_{\circ} ~}
$$

假设不使用Dooke，步骤如图5.24所示。 (1）首先指定TensorFlow Serving安装来源，以下指令为同一列：

echo "deb [arch-amd64
http:/storage.googleapis.com/tensorflow-serving-apt stable
tensorflow-model-server tensorflow-model-server-universal" sudo tee /etc/apt/sources.list.d/tensorflow-serving.ist &&\

(2）安装TensorFlow Serving，Ubuntu 操作系统使用 apt get。指令如 $\mathrm{F}$ :

(3）复制模型：将之前使用SavedModel 存盘的目录复制到/mntc/Users/mikec/, mikec为笔者登录账号的 Horne日录， $\operatorname{l o g}$ 下假设模型日录为my_model

![](figures/327-4-FIGURE.jpg)

图 $5. 2 4$ 不使用Docker的步骤

$$
\mathrm{\ c u r l}
$$

 htps:/storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.oub.gpg| sudo aptkey add

$$
\mathrm{s u d o ~ a p t-g e t ~ u p d a t e}
$$

sud art-etinstal iensoriow-model-server

 $( 4 )$ 复制后需要在 my model下新增一个名为「1」的子日录，代表版本类别，将原有子目录及文件，搬移到1」子目录内，如图5.25所示。

(5）启动TensorFlow Serving，模型名称（mode name）可任意取名，这里取名为 MLP。程序如下

(6）测试数据预测：执行客户端程序
05 11 t seving cint.py）调用 TensorFlow Serving。程序如下：

![](figures/328-3-FIGURE.jpg)

图 $5. 2 5$ 创建子目录

tensorflow model server--
model base path-/mnt/o/Users/mikec/my mode model name-MLP--rest aoi port-8501

(6）测试数据预测：执行客户端程序

python05 11Lf serving clien.py

执行后会传回辨识的结果 $4_{\textrm{c}}$ 

范例、客户端程序:05 10 f serving cdlent.pyi (1）第 $7$ 行：指定要辨识的图片文件。
(2）第8~14行：读取图片文件，将像素转为数组。
(3）第 $1 7 {\sim} 1 9$ 行：将预测数据转为JSON格式。
(4）第21~25行：使TensorFlow Serving $\mathrm{A P I}$ ，送出图形数组。

(4)第 $2 1 \mathrm{~ \sim~ 2 5 ~} / \mathrm{~ \bar{J} ~}$ ：使TensorFlow Serving $\mathrm{A P I}$ ，送出图形数组。

![](figures/329-2-FIGURE.jpg)

$$
\star\Xi\equiv\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\circ
$$

1第 $7$ 行：指定要辨识的图片文件。

(2）第8~14行：读取图片文件，将像素转为数组。

(3)第 $1 7 \sim1 9 \nmid\b F$ ：将预测数据转为JSON格式

(5）第28~29行：接收APl传回的数据，解析预测结果

由上述范例可以看出Server端完全不必撰写程序，非常方便，详细功能可参阅「TensorFlow Serving架构说明」 $\left[ 7 \right]$ 

TensorFlow Dataset类似于 Python Generator，可以根据需要逐批读取数据，不必完全把数据全部加载至内存，因为如果将庞大的数据量全部加载，内存可能就爆了。另外，它还有支持缓存 (Cache）、预取（Prefetch）、筛选（ilter）、转换（Map）等功能，官网有许多范例都会使用到Datasel，值得我们一探究竞。

## 5-7 TensorFlow Dataset (1)from tensor slices()：自 List或 NumPy ndarray数据类型转入。

(3）使用ier)函数将Dataset转成 terator，再使用next()欧数一次取一批数据。程序代码如下：

## 5-7-1 产生 Dataset

建立 Dataset有以下很多种方式。

(2) from tensors()：自 Tensorilow Tensor数据类型转入。 (3) from generator()：自 Python Generator数据类型转入 (4）TFRecordDataset()：自 TFRecord数据类型转入
(5）TextLineDataset()：自文本文件转入。

$$
\mathrm{( 2 ) \, ~ \mathrm{t r o m \_~ t e n s o r s ( ) \! :} ~} \boxplus\mathrm{T e n s o r f l o w ~ T e}
$$
nsor数据类型转入

(3) rom generator()：自 Python Generator数据类型转入

(4）TFRecordDataset()：自TFRecord 数据类型转入。

(s）TexlineDataset)：自文本文件转入。

范例。测试Dataset的相关操作

详细程序请参阅：05 11 Dataset.ipynb

(1）自List转入Dataset。程序代码如下：

1 import tensorflow as tf
2
3 $\# \not\equiv l i s t$ 转人
4 dataset = tf.data.Dataset.from tensor slices([8, 3, 0, 8, 2, 1])

(2）使用for循环即可自Dataset取出所有数据，必须以 nump $\mathrm{y ( O )}$ 函数转换才能打印数据内容。程序代码如下

1 #使用for循环可自 Dataset 取出所有数据 2 for elem in dataset:
print(elem.numpy())

(4）依照维度小计 $( \mathrm{r e c l u c e} )$ ，如呆数据维度是一维，即为总计。程序代码如 $\mathrm{F}$ :

执行结果为前两批： $8$ 3.

![](figures/332-2-FIGURE.jpg)

$$
\pm\hbar\arraycolsep1 p t \mathrm{1 t r} \pm\pm\pm\pm1 5_{\circ}
$$

(5）二维数据：按照列统计。程序代码如下：

![](figures/332-5-FIGURE.jpg)

执行结果 $[ ~ 7 9 ~ 1 1 ~ 1 3 ~ 1 5 ]_{\circ}$ 

（6）三维数据：依照第一维度小计。程序代码如下：

| 1 | #依照第一维度小计(reduce) |
| 2 | import numpy as np |
| 3 |  |
| 4 | #三维数据 |
| 5 | ds = tf.data.Dataset.from tensor slices(np.arange(1,13).reshape(2,2,3)) |
| 6 |  |
| 7 | print('原始数据:n'.np.arange(1,13).reshape(2,2.3), '八n'） |
| 8 |  |
| 8 9 | initial state=0 #起始值 |
| 10 | print('计算结果:\n', ds.reduce(initial state, lambda state, value: state + value).numpy()) |

## 执行结果如下：

(7）map：以函数套用到Dataset内每个元素，将每个元素乘 $\O/ 2$ 。程序代码如下：

(9）复制 $( \mathrm{r e p e a t} )$ 有时候训练数据过少，我们会希望复制训练数据，来提高模型的准确度。程序代码如下：

| 原始数据： T厂1 2 | ) 31 |
| LLL ← [4 5 | - 二J 6]] |
| [[7 8 | 9] |
| 一二 [14 16 1 | 18]] |

| 1 | #对每个元素应用函数(map) |
| 2 | 2 import numpy as np |
| 3 | 3 |
| 4 | #测试数据 |
| 6 | 一— 3  |
| 一 7 | #对每个元素应用函数(map） |
| 8 | 2 ds= ds.map(lambda x: x*2) |
| 9 | 0 |
| 10 | 0 #转成lterator，再显示 |
| 11 | print(list(ds.as numpy iteratorO)) |

执行结果 $[ 2, ~ ~ 4, ~ ~ 6, ~ ~ 8, ~ ~ 1 0 ]$ 

(8） 过滤ilter 。程序代码如下
将偶数取出。

| L #过滤 |
| 2 import numpy as np |
| 8 |
| 1 #测试数据 |
| 5 一一 |
| #对每个元素应用函数(map） 7  |
| 3 ds= ds.filter(lambda x: x %2= 0) |
| 0 |
| 9 #转成lterator，再显示 |
| 1 print(list(ds.as numpy iterator())) |

执行结果：[2，41

(10）Dataset分片 $( \mathrm{S h a r d} )$ 将数据依固定间隔取样，在分布式计算时，可利用比函数将数据分配给每一台工作站 $\mathrm{( W o r k e r )}$ 进行运算。程序代码如下：

另外还有许多函数，如take、skip、unbatch、window、ip 等，读者请参阅TensorFlow 官网关于 Dataset的说明明

![](figures/334-2-FIGURE.jpg)

执行结果：[1, 2，3，4,
$$
5, ~ ~ 1, ~ ~ 2, ~ ~ 3, ~ ~ 4, ~ ~ 5, ~ ~ 1, ~ ~ 2
$$
,
$$
3, ~ ~ 4, ~ ~ 5 ]
$$

![](figures/334-4-FIGURE.jpg)

## 执行结果如下：

| 原始数据: |  |  |
| 10,3,6,9] |  |  |

将MNIST数据转入Dataset。程序代码如下： (11

)shuffle $( 1 0 0 0 0 )$ 每次从dataset 取出10000批数据进行洗牌。

## 执行结果： 显示数据类型及维度。执行结果如下

<TensorsliceDataset shapes: ((28, 28), ()), types: (tf.float64, tf.uint8)>

 $( 1 2 )$ 逐批取得数据

Dbatch $( 1 0 0 0 )$ 随机抽出批数据
1000

$$
\pi\equiv\Xi\pm[ \mp, \exists\pm] \mp\mp
$$

| 1 | 每久随机抽出1000批 |
| 2 | shuffLe:每次从60000 批训练数据取出10000 批洗牌，batch:随机抽出1000批 |
| 3 | ain dataset = dataset.shuffle(10000).batch(1000) |
| 4 | =0 |
| 5 | or (x train, y train) in train dataset: |
| 6 | 一一一 ifi=0:  |
| 7 | print(x train.shape) |
| 8 | print(x trainfol) |
| 9 |
| 10 | i+=1 |
| 11 | int(i) |

执行结果：显示共60批数据，每批数据有 $1 0 0 0 \uparrow_{\circ}$ 

(13）随机数生成Dataset。程序代码如下：

1 import tensorflow as tf
2
3#随机乱数产生 Dataset
4 ds = tf.data.Dataset.from tensor slices(
5 tf.random.uniform([4, 10], minval=l, maxval=10, dtype=tf.int32)) 6
7#转成lterator，再显示
8 print(list(ds.as numpy iterator()))

执行结果：维度为 $( 4, 1 0 )$ ，每个值介于 $( 1, 1 0 )$ 。执行结果如下：

(14）从Tensoriow Tensor数据类型的变量转入Dataset。程序代码如下：

 $\fbox{\left[ \operatorname{a r r r a y} ( \left[ 1, \ 9, \ 1, \ 1, \ 6, \ 7, \ 5, \ 9, \ 8, \ 5 \right] ), \ \operatorname{a r r r a y} ( \left[ 3, \ 8, \ 1, \ 3, \ 9, \ 7, \ 1, \ 2, \ 3, \ 6 \right] ), \ \ \operatorname{a r r a y} ( \left[ 3, \ 8, \ 1, \ 3, \ 7, \ 1, \ 2, \ 3, \ 6 \right] ) \right]}$ [4, 1, 5, 4, 1, 8, 5, 7, 7, 9]), array([1,

![](figures/336-3-FIGURE.jpg)

## 执行结果如下：

$$
\boxed{\left[ \, \begin{matrix} {\, \left[ \, 1 \, \right. \otimes\, \otimes\, \emptyset} \\ {\, \left[ \, \otimes\, \otimes\, \left. 2 \, \right] \, \right] \,} \\ {\, \left[ \, \otimes\, \otimes\, \otimes\, \emptyset\, \right] \, \right] \,} \\ \end{matrix}}
$$

由于图像文件的尺寸通常都很大，不像MNIST的宽和高各只有（28,28），假使一次加载所有文件至内存，恐怕会发生内存不足的状况，因此，TensorFlow Dataset针对影像和文字进行了特殊处理，可以分批加载内存，同时提供数据增补（Data
Augmentation）的功能，能够对既有的图像进行图像处理，产生更多的训练数据，这些功能全都整合至Dataset，可在训练it）指令中指定数据源为Dataset，一气呵成。

范例。自Python Generator数据类型的变量转 $\lambda$ Dataset, 如从网络取得压缩文件，解压缩后，进行数据增补，作为训练数据。数据增补是提高图形辨识度非常有效的方法，利用图像处理的技 ${\bf I \! S}$ ，如放大、缩小、偏移、旋转、裁切等方式，产生各式的训练数据，让训练数据更加多样化，进而使模型有更高的辨识能力。

（1）从网络取得压缩文件，解压缩，并进行数据增补。程序代码如下：

## 5-7-2 图像Dataset

![](figures/337-4-FIGURE.jpg)

(2）试取一批文件，并显示第一批影像。程序代码如下：

(3）定义Generator的属性，包括取出数据的逻辑，再将之转为 Dataset。程序代码如下：

## 执行结果如下：

Found 3670 images belonging to 5 classes. labels: [0. 1. 0. 0. 0.]

![](figures/338-3-FIGURE.jpg)

![](figures/338-4-FIGURE.jpg)

(4）试取下一批数据。程序代码如下：

1 $^\sharp$ 取下一批数据
2 it = iter(ds)
3 images, ${\tt I a b e I}={\tt n e x t ( i t )}$ 
4 $\mathrm{p r i n t} \, ( \mathrm{n p. a r r a y ( \, i m a g e s \, ) \,. s h a p e \,, \; \; n p. a r r a y ( \, l a b e 1 \, ) \,. s h a p e )}$ 执行结果：取出的影像及标记维度分别为（32, 224, 224,3)
$$
( 3 2, 5 )
$$

可在训练指令中指定数据源为产生的Dataset，在下一章会有完整范例说明。

TFRecord 由 TensorFlow 团队所开发，遵循Google Protoco Buffer，希望实现跨平台、跨语言的数据结构（Record-Oriented Binary Format 每批记录能够存储各种数据类型的字段，类似于 JSON 格式， 可序列化（Serialization）为二进制的格式存储。

如图5.26所示，在操作TFRecord 时，需要借由
t.train.Example将数据封装成 protocol message，基本上
tfitrain.Example 的格式为「string」:tf.train.Feature}，而
tf.train.Feature 可接受 BytesList、FloatList、Int64List三种格式的数据。Bytes ist用于字符串或二进制的数据，如图像、语音等。

## 5-7-3 与 TFRecord Dataset

![](figures/340-3-FIGURE.jpg)

图 $5. 2 6$ TFRecord结构

范例。测试TFRecord相关操作

请参阅程序：05 12

(1）定ti.rain.Feature 转换函数。程序代码如下：

| 1 | #下列函数可转换为tf.train.Example 的tf.train.Feature |
| --- | --- |
| 2 | def bytes feature(value): |
| 3 |  |
| 4 | if isinstance(value, type(tf.constant(o))): |
| 5 | value = value.numpy() |
| 6 | return tf.train.Feature(bytes list=tf.train.BytesList(value=[value])) |
| 7 |  |
| 8 | def float feature(value): |
| 9 |  |
| 10 | return tf.train.Feature(float list=tf.train.Floatlist(value=[value])) |
| 11 |  |
| 12 | def int64 feature(value): |
| 13 |  |
| 14 | return tf.train.Feature(int64 list=tf.train.Int64List(value=[value])) |

## (2）简单测试。程序代码如下

1 print( bytes feature(b'test string'))
2 print( bytes feature(u'test bytes'.encode('utf-8'))) 3
4 print( float feature(np.exp(1)))
5
6 print( int64 feature(True))
7 print( int64 feature(1))

## 执行结果如下：

$$
\begin{array} {c} {{\mathrm{b y t e s} \_{1 i s t} {\mathrm{~ \{~}}}} \\ {{\mathrm{~ v a l u e : ~}^{\mathrm{~ ` `}} \mathrm{t e s t} \_{s t r i n g "}}} \\ {{\mathrm{~ \} ~}}} \end{array}
$$
bytes list {
value: "test bytes" $\}$ 
$$
\begin{array} {l} {{{\tt f l o a t \_1 i s t ~ \{}}} \\ {{{\tt v a l u e : ~ 2. 7 1 8 2 8 1 7}}} \\ {{{\tt\}}}} \end{array}
$$
$$
\begin{array} {c} {\mathrm{i n t} 6 4 \mathrm{~ l i s t ~} \{} \\ {\mathrm{~ v a l u e : ~} 1} \\ \end{array}
$$
 $\}$ 
$$
\begin{array} {c} {{\mathrm{i n t} 6 4 \mathrm{~ 1 i s t ~} \left\{\right.}} \\ {{\mathrm{~ v a l u e : ~} 1}} \\ {{\left. \right\}}} \end{array}
$$

(3）序列化测试。程序代码如下执行结果：b 『\×12x06Anx04Txf8-@」，为二进制的格式， 进行压缩。

(4）建立tf.ranExample信息，含有 ${}_{4}$ 个feature，即0~3. 程序代码如下：

(5）接下来要写 $\lambda$ TFRecord 文件，先定义t.train.Example 数据序列化函数。程序代码如下：

![](figures/342-3-FIGURE.jpg)

![](figures/342-4-FIGURE.jpg)

## (6）将一批记录写入TFRecord文件。程序代码如下：

1#将一批记录写人TFRecord文件
2 with tf.io.TFRecordwriter("test.tfrecords") as writer:
writer.write(serialized example)

(7）读取TFRecord文件。程序代码如下：

（8）若要取得原始数据，则需先反序列化（Deserialize) 设定原始数据的字段属性，透过parse single example()来进行 $\o^{\sum}$ 序列化。程序代码如下：

## 执行结果：执行结果如下，为二进制的格式

 $\left\{\begin{array} {l} {{\mathrm{( t t f, T e n s o r : ~ s h a p e=( 1 ) ~, ~ d t y e=s t r i n g, ~ n u m p y=b^{\prime} \leq~ n u \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ n \leq~ \leq~ n \leq~ n \leq~ \leq~ n \leq~ \leq~ n \leq~ \leq~ n \leq~ \leq~ n \leq~ \leq~ \leq~ n \leq~ \leq~ \leq~ \leq~ \leq~ \right. ~}}}} \end{array} .$ 0\n\x11\n\x08feature1\x12\x05\x1a\x
\n\x04[\xd3|?'>

![](figures/343-3-FIGURE.jpg)

## (9）取得每一个字段值。程序代码如下：

1 #反序列亿(Deserialize)
2 parsed dataset = raw dataset.map( parse function) 3
4 #取得每一个字段值
5 for parsed record in parsed dataset.take(10):
6 print(repr(parsed record))

## 执行结果如下：

## 10）从网络上取得官网的TFRecord文件。程序代码如下

1#从网络上取得宫网的TFRecord 文件
2 file path = "https://storage.googleapis.com/download.tensorflow.org/" + 3 "data/fsns-20160927/testdata/fsns-00000-of-00001
4 fsns test file = tf.keras.utils.get file("fsns.tfrec", file path)
5
6#显示存储位置
7 fsns test file

执行结果：显示默认存盘位置，其中mikec为用户目录 C:lUserslmikeoll.kerasl\datasetslisns.tfrec.

1）读取TFRecord 文件。程序代码如下

1#读取TFRecord 文件
2 dataset = tf.data.TFRecordDataset(filenames = [fsns test file]) #欧得下一批数据
4
5 raw example = next(iter(dataset))
6 parsed = tf.train.Example.FromString(raw example.numpy())
7 parsed.features.feature["image/text']

执行结果：该字段为一字符串：

$$
\begin{array} {c} {{\mathrm{b y t e s ~ I i s t ~} \left\{\right.}} \\ {{\mathrm{v a l u e : ~ u l l e ~} \mathrm{P e r r e y o n "}}} \\ {\left. \right\}} \\ \end{array}
$$

文本文件也可以像二进制文件一样存储在Dataset内，并且序列化后存档。我们同样来示范个简单的例子

(1）读取三个语料库文件，合并为一TextLineDataset。程序代码如下：

(3）轮 $\mathrm{i \hbar\Lambda\Lambda\ ( i n t e r l e a v e )}$ ：每个文件读 $3$ 批数据即切换为下一个文件读 $\exists\bar{x} ~ \mathrm{~ ( c y c l e \_l e n g t h=3 ) ~}$ 程序代码如下：

## 5-7-4 TextLineDataset

范例1。测试TextLineDataset相关操作。

请参阅程序：05 13 TextLineDataset.ipynb.

![](figures/345-6-FIGURE.jpg)

(2）读取 $5$ 批数据。程序代码如下

| 1 | # 读取5批数据 |
| 2 | for line inc ds.take(5): |
| 3 | print(line.numpy(O) |

## 执行结果如下：

b"\xef\xbb\xbfAchilles sing, O Goddess! Peleus son; b'His wrath pernicious, who ten thousand woes
b"Caused to Achaia's host, sent many a soul"
b'Illustrious into Ades premature,
b'And Heroes gave (so stood the will of Jove)

1 # interleave:每个文件轮流读取
2 files ds = tf.data.Dataset.from tensor slices(file paths)
3 lines ds = files ds.interleave(tf.data.TextlineDataset, cycle length=3) 4
5#各读3批，共9批
6 for i, line in enumerate(lines ds.take(9)):
7 if i % $3==\textcircled{0}$ :
8 print(）
9 print(line.numpy())

## 执行结果如下：

| b" | les sing, O Goddess! Peleus' son;" |
| b" b' | leus' son, Achilles, sing, O Muse," o goddess, the anger of Achilles son of Peleus, that brought' |
| b'H | pus, who ten thousand woes' |
| b' | ep and deadly; whence to Greece' |
| b'countless ills upon the Achaeans. Many a brave soul did it send' |
| b"Caused to Achaia's host, sent many a soul" |
| b' | hose; which many a soul' |
| b'| | Hades, and many a hero did it yield a prey to dogs and' |

## 范例2. TextLineDataset结合筛选（ilter）函数。

(1）读取泰坦尼克文本文件（.osv 存储至 TextLineDataset。程序代码如下

1 #读取泰坦尼克文本文件(.csv），存储至TextlineDataset
2 file path = "https://storage.googleapis.com/tf-datasets/titanic/train.csv 3 titanic file - tf.keras.utils.get file("train.csv", file path)
4 titanic lines = tf.data.TextlineDataset(titanic file)

## (2）筛选生存者的数据。程序代码如下

1 #筛选生存者的数据
B“AGRTARE
return tf.not equal(tf.strings.substr(line, 0, 1), "0") 5sum锁
survivors = titanic lines.skip(1).filter(survived)
7
8 #读取10批数据
9 for line in survivors.take(10):
10 print(line.numpy())

## 执行结果如下： b'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n'
 ${\sf b^{\prime} 1, f e m a l e, 2 6. 0, 0, 0, 7. 9 2 5,}$ Third,unknown,Southampton,y" ${\sf b^{\prime} 1, f e m a l e, 3 5. 0, 1, 0, 5 3. 1, P}$ irst,C,Southampton,n'
b'1,female,27.0,0,2,11.1333,Third,unknown,Southampton, $n^{\prime}$  ${\mathrm{b^{\prime} \, 1, f e m a l \, e \,, 1 \, 4 \,. \, 0, 1 \,, 0 \,, 3 0 \,. \, 0 7 \, 0 8 \,, 5}}$ econd, unknown, Cherbourg,n' ${\mathrm{b}}^{\prime} {\mathrm{1}}, {\mathrm{f e m a l e}} \,, 4. 0, 1, 1, 1 6. 7, 7$ Third,G,Southampton,n
 ${\mathrm{b}}^{\prime} 1, {\mathrm{m a l}} \, {\mathrm{e}} \,, 2 8. 0, 0, 0, 1 3. 0,$ Second,unknown,Southampton,y'
 $\mathrm{b} \cdot\mathrm{1}$ ,female,28.0,0,0,7.225,Third,unknown,Cherbourg,y'
 $b^{\cdot} 1$ ,male,28.0,0,0,35.5,First,A,Southampton,y
 ${\mathrm{b}}^{\prime} {\mathrm{1}}, {\mathrm{f e m a l}} {\mathrm{e}}, 3 8. 0, {\mathrm{1}}, 5, 3 1. 3 8 7 5,$ Third,unknown, Southampton,n

## 范例3. TexiLineDataset结合 DataFrame,

(1）读取泰坦尼克文本文件（.csv) 。程序代码如下：

| 1 | import pandas as pd |
| 2 |  |
| 3 | df=p pd.read csv(titanic file, index col=None) |
| 4 | df.head() |

## (2）存储Dataset，读取一批数据。程序代码如下：

1#存储Dataset
2 ds = tf.data.Dataset.from tensor slices(dict(df)) 3
4#读取一批数据
5 for feature batch in ds.take(1):
6 for key, value in feature batch.items():
7 $\mathrm{p r i n t} \big( \mathrm{f}^{n} \{\mathrm{k e y} ; 2 0 s \}$ : {value}")

## 执行结果如下：

survived $\emptyset$ 
sex $^\mathrm{b}$ 'male'
age $2 2, \emptyset$ 
n siblings spouses $1 ~$ 
parch $\emptyset$ 
fare $7. 2 5$ 
class $^\mathrm{b}$ 'Third"
deck $^\mathrm{b}$ 'unknown
embark town $^\mathrm{b}$ 'Southampton" alone b'n'

使用Dataset时，可利用预先读取 $\mathrm{( p r e f e t c h )}$ 缓存
(cache）等指令，来提升数据读取的效果。下面用时间轴的方式来展示 prefetch 和 cache的用途。

(1）prefetch：在训练时只利用到CPU/RAM，同时
TensorFlow 利用空档先读取 $\mathrm{T-}$ 批数据，并作转换，如图 $5. 2 7$ 和图 5.28所示。

$$
( 2 ) \, \mathrm{\ c a c h e :}
$$
可将读出的数据留在高速缓存里，之后可再重复
使用，如图5.29所示。

## 5-7-5 Dataset 效果提升

![](figures/348-4-FIGURE.jpg)

图5.27不使用preéetch（开启 Dataset、读取数据、训练这三个动作会依序进行，拉长运行时间

![](figures/348-6-FIGURE.jpg)

图5.28、使用prefetch（会在训练时，同时读取下一批数据， 故读取数据和训练一起同步进行）

Cached dataset

![](figures/349-0-FIGURE.jpg)

冬5.29 使用 cache（能够降低开启 Dataset 读取数据的次数，减少硬盘1O

详细的情形读者可参考「宫网Dataset效果说明」。

第三波人工智能浪潮在自然用户接（Natural User Interface, NUI）有了突破性的进展，包括影像（Image&Video）、语音
(Voice）与文字（Text）的辨识/生成/分析，机器学会利用人类日常生活中所使用的沟通方式，与使用者的互动不仅更具亲和力，也能对周遭的环境做出更合理、更有智慧的判断与反应。将这和能力附加到产品上，可使产品的应用发展焕发无限可能，包括自动驾驶 $\mathsf{K}$ Self-Driving）、无人机（Drone）、智能家庭（Smart
Home）、制造/服务机器人（Robot）、聊天机器 $\mathcal{A}$ （ChatBot) 等，不胜枚举。

从这一章开始，我们逐一来探讨影像、语音、文字的相关算法。

## 第6章

## 卷积神经网络之前我们只用 $\Im+\Pi$ 行程序即可实现辨识阿拉伯数字，但是， 模型使用像素（Pixel)为特征输 $\lambda$ ，似乎与人类辨识图形的方式并不一致，我们应该不会逐点辨识图形的内涵。

(1）手写阿拉伯数字，通常都会集中在中央，故在中央像素的重要性应远大于周边的像素。

(2）像素之间应有所关联，而非互相独 $\overrightarrow{\Psi}$ ，如1为一垂直线。

（3）人类视觉应该不是逐个像素辨识，而是观察数字的线条或轮廓。

因比，卷积神经网络（Convolutional Neural Network, CNN) 引进了卷积层（Gonvolution La $\mathrm{y e r} )$ ，进行特征提取（Feature Extraction），将像素转换为各种线条特征，再交给Dense 层辨 $\grave{\nu} \Pi$ ，这就是图 $1. 7$ 机器学习流程的第 $3$ 步骤特征工程（Feature Engineering)

卷积（Convolution）简单来说就是将图形逐步拍样化
(Abstraction），把不必要的信息删除，如色彩、背景等，图6.1 所示经过三层卷积后，有些图依稀可辨识出人脸的轮廓 $\overline{{\jmath}}$ ，因此， 模型就依据这些线条辨识出是人、车或其他动物。

## 6-1 卷积神经网络简介

(2）图像经过卷积层运算，变成特征图（FeatureMap），卷积可以指定很多个，卷积矩阵不是固定的，而是由反向传导推估出来的，与传统的图像处理不同，卷积层后面通常会附加ReLU
Activation Function。

(3）卷积层后面会接一个池化层（Pooling) 作下采样 (Down Sampling) 以降低模型的参数个数，避免模型过于庞大。

![](figures/352-2-FIGURE.jpg)

图6.1 卷积神经网络的特征萃取

6.2所示卷积神经网络的模型结构如图

![](figures/352-5-FIGURE.jpg)

冬 $6. 2$ 卷积神经网络的模型结构

(1）先输入一张图像，可以是彩色的，每个色彩通道 (Channel）分别卷积再合并。

(4）最后把特征图压扁（Flatten）成一维，交给Dense层辨识。

卷积是定 $\chi-$ 个滤波器（Filter）或称卷积核（Kernel 对图像进行「乘积和」运算，如图6.3所示，计算步骤如下

(4）逐步向右滑动窗口（见图6.4 回到步骤（1 计算下一格的值。

## 6-2 卷积

(1）依照滤波器将输入图像裁切为相同尺寸的部分图像。

(2）裁切的图像与相同的位置派 ®

(3）加总所有格的数值，即为输出的第一格数值。

(5）滑到最右边后，再往下滑动窗口，继续进行

![](figures/354-7-FIGURE.jpg)

图 $6. 3$ 卷积计算 $( 1 )$ 

网络上有许多动画或影片可以参考，如「Convolutional Neural Networks- Simplified」 !文中卷积计算的GIF动图印

![](figures/355-1-FIGURE.jpg)

冬 $6. 4$ 卷积计算 $( 2 )$ 

范例。使用程序计算卷积。

请参阅程序:06 01 convolutions.ipynb

(1）准备数据及滤波器。程序代码如下

1 import numpy as np
了#域数据
3
4 source map - np.array(list('1110001110001110011001100')).astype(np.int) 5 source map = source map.reshape(5,5)
6 print("原始数据："）
7 print(source map)
8
9#旅波器(Filter）
10 filter1 = np.array(list('101010101')).astype(np.int).reshape(3,3)
11 print('\n滤波器：'）
12 print(filter1)

## 执行结果如下： (3) SciPy 库提供的卷积函数验算，执行结果一致。程使用
序代码如下：

(1）补零（Padding) 上面的卷积计算会使得图像尺寸变 $\imath\rfloor\omicron$ ，因为滑动窗口时，裁切的窗口会不足两个，即滤波器宽度减 $1$ ，因此·Padding 有以下两个选项。

$$
\begin{array} {r r r r} {\scriptstyle{\left[ \begin{array} {l l l l} {\scriptstyle{\frac{1} {2}} \scriptstyle{\left[ \begin{array} {l l} {\scriptstyle1} & {\scriptstyle1} & {\scriptstyle1} & {\scriptstyle0} \\ {\scriptstyle1} & {\scriptstyle1} & {\scriptstyle1} & {\scriptstyle0} \\ \end{array} \right]}} \\ {\scriptstyle{\left[ \begin{array} {l l l l} {\scriptstyle0} & {\scriptstyle1} & {\scriptstyle1} & {\scriptstyle1} & {\scriptstyle0} \\ \end{array} \right]}} \\ {\scriptstyle{\left[ \begin{array} {l l l} {\scriptstyle0} & {\scriptstyle0} & {\scriptstyle1} & {\scriptstyle1} & {\scriptstyle1} \\ {\scriptstyle0} & {\scriptstyle1} & {\scriptstyle1} & {\scriptstyle0} \\ {\scriptstyle1} & {\scriptstyle1} & {\scriptstyle1} & {\scriptstyle0} \\ \end{array} \right]}} \end{array} \right]}} \\ {\scriptstyle{\left[ \begin{array} {l l l l} {\scriptstyle0} & {\scriptstyle0} & {\scriptstyle1} & {\scriptstyle1} & {\scriptstyle0} \\ {\scriptstyle1} & {\scriptstyle1} & {\scriptstyle1} & {\scriptstyle0} & {\scriptstyle1} \\ {\scriptstyle1} & {\scriptstyle1} & {\end{array} \right]}} \end{array}
$$

$$
, F=\frac{1+} {1+}=\frac{1} {1+}
$$
$$
\begin{array} {r r} {\left[ 1 \right. \textcircled{0} \mathbf{1} ]} \\ {\left[ 0 \begin{array} {l l} {1} & {0} \\ \end{array} \right]} \\ {\left[ 1 \begin{array} {l l} {0} & {1} \\ \end{array} \right]} \\ \end{array}
$$
1

## (2）计算卷积。程序代码如下

| #计算卷积 |
| 2 #初始化计算结果的矩库 |
| 3 width - height = source map.shape[o] - filterl.shape[o] + 1 |
| 1 result = np.zeros((width, height)) |
| 6 |
| 6 #计算每一格 |
| 7 for i in range(width): |
| 2 for j in range(height): |
| 3 valuel =source map[i:itfilter1.shape[o], j:jtfilter1.shape[1]] * filter1 |
| 0 result[i, j]= np.sum(valuel) |
| print(result) |

## 执行结果如下：

$$
\begin{array} {c c c} {\left[ 4 \,, \, \, \, 3 \,, \, \, \, 4 \,, \right]} \\ {\left[ \, 2 \,, \, \, \, 4 \,, \, \, \, \, 3 \,, \, \, \right]} \\ {\left[ \, 2 \,, \, \, \, 3 \,, \, \, \, \, 4 \,. \, \right]} \end{array}
$$

| 1 | # 使用SciPy 计拿卷积 |
| 2 | from scipy.signal import convolve2d |
| 3 |  |
| 一 4 | #convolve2d·二维卷积 |
| 5 | convolve2d(source map, filter1, mode='valid') |

卷积计算时，其实还有以下两个参数

OPadding-'same $\Gamma$ ：在图像周围补上不足的列与行，使计算结果的矩阵尺寸不变（same 与原始图像尺寸相同，如图6.5所 $\overline{{\Pi}}$ .

(2）滑动窗口的步数（Sride 图6.4所示是 Stride-1，图 6.6 所示是 Stride-2，可减少要估算的参数个数。

以上是二维的卷积 $( \mathrm{C o n v 2 D} )$ 的运作，通常应用在图像上。 TensorFlow还提供了 $\mathrm{C o n v 1 D}$ 、Conv3D，其中Gonv1D 因只考虑 $\pm$ 下文（Context Sensitive），所以可应用于语音或文字方面， Conv3D则可以应用于立体的对象。还有Conv2DTranspose提供了反卷积（Deconvolution）或称上采样（Up Sampling）的功能， 反向由特征图重建图像。卷积和友卷积两者相结合，还可以组合成 AutoEncoder模型，它是许多生成模型的基础算法，可以去除噪声，生成无干扰的图像。

Padding-vald ：不补零，计算后图像尺寸变小

![](figures/357-4-FIGURE.jpg)

冬 Padding-'same $\Gamma$ ，在图像周围补上不足的列与行 6.5

![](figures/357-6-FIGURE.jpg)

图6.6 Stride-2，一次滑动 $2$ 格窗口

虽然CNN会自动配置卷积的种类，不过我们还是来看看各式卷积的图像处理效果，进而使大家加深对CNN的理解。

 $( 2 )$ 安装Python OpenGV - ip install opencv-python，其中 OpenCV是一个图像处理的库

(3）将影像灰阶化：skimage全名为 scikit-image，也是一个图像处理的库，功能较 OpenCV简易。程序代码如下

## 6-3 各式卷积

(1) 一个卷积的影像转换函数，程序代码如下： 首先定义

![](figures/358-5-FIGURE.jpg)

(4）模糊化（Bur） 滤波器设定为周围点的平均，就可以让图像模糊化，一般用于消除红眼现象或噪声。程序代码如下：

![](figures/359-1-FIGURE.jpg)

执行结果： 6.7所示
如图

![](figures/359-3-FIGURE.jpg)

图 $6. 7$ 各式卷积执行结果

## (a）原图：（b）灰阶化

![](figures/359-6-FIGURE.jpg)

2大模糊：21x21矩阵，矩阵越大，影像越模糊，执行结果如图 $6. 9$ 所示

(5）锐化（Sharpen 可使图像的对比更加明显。程序代码如下：

O $\imath\rfloor\omicron$ 模糊：7x7矩阵，执行结果如图6.8所示

![](figures/360-3-FIGURE.jpg)

图6.8 $\prime\rfloor$ \模糊

![](figures/360-5-FIGURE.jpg)

冬 $6$ .，大模糊

![](figures/360-7-FIGURE.jpg)

执行结果：卷积凸显中间点，使图像特征越明显，如图6.10所示。

$$
T : \hspace{1 0 p t} ( 6 ) \mathrm{~ L a p l a c i a n ~} ) \hspace{1 0 p t} ( 6 ) \mathrm{~ L a p l a c i a n ~} 2 \hspace{1 0 p t} ( 3 \pi)
$$
则：可检测图像的轮廓。程序代码如

执行结果：卷积凸显外围，显现图像外围线条，如图6.11所示。

![](figures/361-3-FIGURE.jpg)

冬 $6$ .10 锐化结果

$$
\mathrm{T :}
$$

![](figures/361-6-FIGURE.jpg)

![](figures/361-7-FIGURE.jpg)

冬 $6. 1 1$ 边缘检测结果

(7）SobelX轴边缘检测：沿着 $\mathbf{X}$ 轴检测边缘，故可检测手直线特征，程序代码如下

执行结果：卷积列白小至大，显现图像垂直线条，如图6.12所示。

(8）SobelY轴边缘检测：沿着 $\mathbf{Y}$ 轴检测边缘，故可检测水平线特征。程序代码如下：

![](figures/362-3-FIGURE.jpg)

![](figures/362-4-FIGURE.jpg)

图 $6. 1 2$ Sobel×轴边缘检测结果

![](figures/362-6-FIGURE.jpg)

执行结果：卷积行白小至大，显现图像水平线条，如图6.13所示。

![](figures/363-1-FIGURE.jpg)

图 6.13 Sobel $\mathbf{Y}$ 轴边缘检测结果

通常我们会设定每个卷积层的滤波器个数为 $4$ 的倍数，因此总输出等于（笔数 $\times\mathrm{W}$ out $\mathbf{x}$ H out×滤波器个数），会使输出尺寸变得很大，因此，我们必须透过池化层（Pooling Layer）进行采样， 只取滑动窗口的最大值或平均值。换句话说，就是将每个滑动窗□ 转化为一个点，就能有效降低每一层输入的尺寸了，同时也能保留每个窗口的特征。我们来举个例子说明会比较清楚。

(3） $( 2, \, 2 )$ 如图6.14 左上角的框，取最大值为滑动窗口取
6.

## 6-4 池化层

以最大池化层（Max Pooling）为例：

（1）图6.14所示左边为原始图像

![](figures/364-5-FIGURE.jpg)

冬 $6. 1 4$ 最大池化层

(2）假设滤波器尺寸为 $( 2, 2 )$ ，Stride $= 2_{\alpha}$ 2

）接着再滑动2步，如图6.15 所示，取最大值为 $8$ . (4)

![](figures/364-9-FIGURE.jpg)

冬最大池化层一一滑动 $2$ f 6.15 一步

一般卷积会采用3×3或5×5的滤波器，尺寸越大，可以提取越大的特征，但相对地，较小的特征就容易被忽略。而池化层通常会采用2x2，stride-2的滤波器，使用越大的尺寸，会使得参数个数减少很多，但提取到的特征也相对减少。

(1）加载MNIST手写阿拉伯数字数据，完全不需改变。程序代码如下：

(2）改用CNN模型：使用
$$
\frac{\mathrm{F Z H}} {\mathrm{C o n v} 2 \mathrm{D / M a x P o o l i n g 2 D}_{\circ}}
$$
程
序代码如下：

## 6-5 CNN 模型实践

以下就先以CNN 模型实践 MNIST辨识。

范例1。将手写阿拉伯数字辨识的模型改用CNN

请参阅程序:06 02 MNIST CNN.ipynb

![](figures/366-7-FIGURE.jpg)

CNN的卷积层 $( \mathrm{C o n v 2 D} )$ 的输 $\lambda$ 多一个维度，代表色彩通道，单色为 $1$ ，RGB色系则设为 $3$ 。因此，输入数据须增加一维第 $7 \mathrm{\sim} 8$ 行程序即是使用np.expand dims增加了一维在最后面

(1）也有模型采用连续两个 Conv2D，再接 $- \uparrow$ 
MaxPooling2D，并没有硬性规定，可根据数据多少进行实验，调校出最佳模型及最佳参数。

![](figures/367-2-FIGURE.jpg)

）模型训练，程序代码不需做任何改变。程序代码如下： (3

![](figures/367-4-FIGURE.jpg)

执行结果：准确率为0.9892，较之前的模型略高

注意事项如下。

(2）再强调一次，CNN 不须指定要使用何种滤波器，
TensorFlow 会自动配置，且参数会在训练过程中找到最佳参数值， 我们只要指定滤波器的个数即可。

(3）可使用 model.summa $\mathbf{r y} ( )$ 函数，观察输出维度及参数个数。

式中：W out为输出的宽度：W为原图像的宽度： $\boldsymbol{F}$ 为滤波器的宽度；P为单边补零的列数；S为滑动的步数。

(4）依上述公式验算第一层Conv2D 输出宽度 $( \mathrm{W \_o u t} ) ~ :$ 
$$
\mathrm{W \_o u t=f l o o r ~ \langle~ \langle~ W-F+2 P ~ \rangle~ \rangle~} /
$$
S
$$
1 ) ~=~ ( 2 8-3+2^{\star} 0 ) ~ / 1+1 \!=\! 2 6_{\circ}
$$

(5）验算第一层Conv2D输出参数：Output Filter数量 (Filter完*Filter 高*nput Fiter数量 $+ 1 ) ~=3 2$ *(3*3*1+1) 32* $1 0=3 2 0$ 。其中加1为回归线的偏差项。

卷积层输出的宽度/高度公式为

$$
\mathrm{~ W \_~ o u t=~ ( W-F+2 P ) ~ / S+1 ~}
$$

$$
\mathrm{M o d e l : ~ ` ` s e q u e n t i a l''}
$$

| Layer (type) | Output Shape | Param # |
| --- | --- | --- |
| conv2d (Conv2D) (None,26,26,32) 320 |
| max pooling2d (MaxPooling2D) | （None, 13,13,32) | 0 |
| conv2d 1 (Conv2D) | (None,11, 11, 64) | 18496 |
| max pooling2d 1 (MaxPooling2 | （None,5, 5, 64) | 0 |
| flatten (Flatten) | (None,1600) | 0 |
| dropout (Dropout) | (None,1600) | 0 |
| dense(Dense) | (None,10) | 16010 |

(7）验算第 $- \pi$ Conv2D 输出参数：Output Filter 数量大 Filter E*Filter 高*lnput ilter 数量+1) - 64 * (3*3*32 +1 18496.

(1）部分连接（Locally Connected or $\mathbf{S}$ parse
Connectivity）：Dense 层的每个神经元完全连接（Fll
Connected）至下一层的每个神经元，但卷积层的输出神经元则只连接滑动窗口神经元,如图6.16所示。想象 $- \mathrm{T}$ ，假设在手臂上拍打一 $\mathrm{T}$ ，手臂以外的神经元应该不会收到信号，既然没收到信号， 理所当然就不必往下一层传送信号了，所以，下一层的神经元只会收到上一层少数神经元的 $\imath\rightleftharpoons\Xi$ ，接收到的范围称之为感知域
 $\mathsf{I}$ Reception Field）。

由于部分连接的关系，神经层中每条回归线的输入特征大幅减 $\ ~$ ，要估算的权重个数也就少了很多，于是模型即可大幅简化。

 $( 6 )$ 第一层MaxPooling2D输出宽度（W out) W out -floor(（W-F) /S+1) (26-2) $/ 2+1=1 3$ (无条件舍去）

 $\mathcal{M}$ 卷积层运算观察，CNN模型有以下两个特点

![](figures/369-5-FIGURE.jpg)

冬 $6. 1 6$ 部分连接

(2）权重共享（Weight Sharing）：单一滤波器应用到滑动窗口时，卷积矩阵值都是一样的，如图 6.17所示，基于这个假设，要估计的权重个数就减少了许多，模型复杂度因而进一步得到了简化。

另外，为什么CNN模型输入数据要加入色彩信道呢?这是因为有些情况加入色彩，会比较容易辨识，比如狮子大部分是金黄色的，又或者检测是否有戴口罩，只要图像上有一块白色的矩形，我们应该就能假定有戴口罩，当然目前口罩颜色已经是五花八门，这种情况需要更多的训练数据，才能正确辨识。

以下我们使用TensorFlow 内建的Cifar图像·，比较单色与彩色的图像辨识准确率。

![](figures/370-3-FIGURE.jpg)

 $6$ .17权重共享图

所以，基于以上的两个假设，CNN模型训练时间就不会过长。

范例 $2$ 。单色的图像辨识

请参阅程序：06 03 Clar gray CNN.ipynb

(1) Cifar1O 数据。程序代码如下： 加载

执行结果：训练/测试数据各为5000/10000笔，图像的宽和高均各为 $3 2$ ，为RGB色系。执行结果如下

(2）转成单色：使用TensorFlow内建的rgb to grayscale( 函数。程序代码如 $\mathrm{T}$ :

(1）加载·Cifar1O数据，与单色的图像辨识相同，但不需转换为单色。

(50000,32, 32, 3)（50000,1) （10000, 32, 32, 3)（10000,1)

1#转成单色：rgb to grayscale
2 x train - tf.image.rgb to grayscale(x train) 3 x test = tf.image.rgb to grayscale(x test) 4 print(x train.shape, x test.shape)

执行结果：执行结果如 $\mathbf{F}$ ，最后一维为1

(50000,32, 32, 1) (10000, 32, 32, 1

$$
( 5 0 0 0 0, \ 3 2, \ 3 2, \ 1 ) \ \ ( 1 0 0 0 0, \ 3 2, \ 3 2, \ 1 )
$$

(3）后续的程序代码与 MNIST辨识相同。
|

执行结果：执行结果如 $\mathrm{F}$ ，准确率只有32 $\not\supset$ 。
O

$$
\begin{array} {l} {\texttt{l o s s :} 1. 8 8 1 6} \\ {\texttt{a c c u r a c y :} 0. 3 2 5 8} \end{array}
$$

范例 $\textbf{3}$ .彩色的图像辨识。

请参阅程序:06 04 Cifar RGB CN.ipynb

(2）修改模型为CNN，第 $3$ 行inouts 为三维，最后一
shape
维是色彩通道。程序代码如下：

![](figures/372-1-FIGURE.jpg)

执行结果：准确率提升为 $7 0 \%$ ，辨识效果显著

(1）使用MNIST的测试数据，辨识率达 $9 8 \%$ ，但如果以在绘图软件里使用鼠标书写的文件测试，辨识率就差很多 $\overline{{\jmath}}_{\circ}$ 这是因为 MNIST的训练数据与鼠标撰写的样式有所差异，MNIST 的数据应该是请受测者先写在纸 $\b L$ ，再扫描存盘的，所以图像会有深浅 $\pi-$ 的灰阶和锯齿状，与我们直接使用鼠标在绘图软件内书写的情况不太一样，所以，如果要实际应用，应该要自行收集训练数据，准确率才会提升。

(2）若要自行收集数据，须找上万个测试者，可能不太容易，又加上有些人书写可能不规范，这会影响预测准确度，我们可以借由数据增补的方法，自动产生各种变形的训练数据，让模型更强健（Robust) 可容忍这些缺点。

数据增补可将一张正常图像，转换成各式有缺陷的图像，如增加旋转、偏移、拉近/拉远、亮度等效果，再将这些数据当作训练数据，这样训练出来的模型，就比较能辨识有缺陷的图像。

TensorFlow/Keras 提供的数据增补函数lmageDataGenerator 的参数有多 $\overline{{\pi}}$ ，介绍如下

(1) shiftrange：图像宽度偏移的点（Pixel）数或比
wioth
例。

## 6-6 影像数据增补

之前我们介绍的辨识手写阿拉伯数字程序，具有以下缺点。 (2) height shit-range:图像高度偏移的点（Pixel）数或比例。

(3)brightness 图像亮度偏移的范围。
range:

(4） shear range：图像顺时针歪斜的范围

(5） zoom range：图像拉近/拉远的比例。

(6) fil mode：图像填满的方式，有constant、nearest、 reélect、wrap四种方式，详见 Keras官网

(7） horizontal flip：图像水平翻转。 (8) vertical flip：图像垂直翻转。 (9） rescale：特征缩放

(7) horizontal fip：图像水平翻转

(8） verical lip：图像垂直翻转

$$
( 9 ) \, \mathrm{\ r e s c a l e : \} \sharp\pm\hbar\pm\unit{Z} \pm\hbar\hbar\mathtt{X}_{\circ}
$$

示例如图6.18所示，详细情形可参考Keras官网

![](figures/374-10-FIGURE.jpg)

图6.18 左上角的原始图像经过数据增补后，变成各种角度旋
转的图像

范例. MNIST加上Data Augmentation

请参阅程序:06 05 Data MNIST.ipynb. (1）加载 MNIST数据与模型定义与之前介绍相同，完全不需改变。

①准确度并没有提升，但没关系，因为我们的目的是要看自行绘制的数字是否被正确辨识。

9加入数据增补后，训练时间拉长为两倍多，以笔者的运行设备为例，白原本的5s拉长至12s。

(3) ，原来的模型无法正确辨识笔者写
测试自行绘制的数字
的 $9$ ，经过数据增补后，已经可以正确辨识了。程序代码如下： (2）训练之前先进行数据增补。程序代码如下

![](figures/375-5-FIGURE.jpg)

$$
\pm\pm\pm\pm\pm\pm\pm
$$

之前都是使用TensorFlowKeras 内建的数据集，而这个范例使用Kaggle 所提供的数据集，它需要做前置处理，就是进行数据清理（Data Clean），这会比较接近现实的状 $\gg\Pi$ ，但由于数据量较 $\ ~$ ，准确率较差，因此我们使用更复杂的CNN模型，再加上数据增补，以提升准确率。

Kaggle 为知名的 $\mathbf{A} \mathbf{I}$ 竞赛网站，也是一个很好的学习园地，这里有很多 $\mathcal{A}$ 上免费提供程序代码和数据集，各位读者可以进行参阅。

4869-8368-6DEBA77B919F/kaggleca
$$
\begin{array} {l} {\mathrm{m l o a d} / 3 / \mathrm{E} / 1 / 3 \mathrm{E 1 C 3 F 2 1-E C D B}} \\ {\mathrm{a t s a n d o g s_{-} 3 3 6 7 a. z i p_{o}}} \\ \end{array}
$$
htips://download.microsoft.com/dow

原始程序来自于 Keras官网所提供的范例「Image
classification from scratch」 1 ，笔者拿来做了一些修改及批注。具体步骤如图6.19所示。

![](figures/376-4-FIGURE.jpg)

## 范例2。宠物数据集的处理

## 请参阅程序:06 06 Data Pets.ipynb.

宠物数据集网址：

(2）过滤不合格的文件：扫描每一个文件，若表头不含
FJFIFJ 就不是图片文件，归类为不合格的文件，不纳入训练数据内。程序代码如下：

![](figures/377-1-FIGURE.jpg)

图 $6. 1 9$ 具体步骤

(1）从网络取得压缩文件，并且解压缩。程序代码如下

![](figures/377-4-FIGURE.jpg)

$$
\begin{array} {r} {( 5 ) \; \; i \hbar\Xi\mathrm{p r e f e t c h :} \; \; 3 \pi5} \\ {( 3 \times1 ) \pi:} \\ \end{array}
$$
先读取训练数据，以提升效果。程序代

![](figures/378-1-FIGURE.jpg)

(3）以文件目录为基础，建立训练（Training）及验证 (Validation）数据集。程序代码如下

![](figures/378-3-FIGURE.jpg)

## (4）定义数据增补。程序代码如下

| # RandomFlip("horizontal"）:水平翻转 |
| 2 # RandomRotation(O.1）:旋转0.1 比例 |
| 3 data augmentation = keras.Sequential( |
| 1 [ |
| 5 layers.experimental.preprocessing.RandomFlip("horizontal"), |
| 5 layers.experimental.preprocessing.RandomRotation(o.1), |
| 7 |
| 3 |

(6）建立模型：原作者使用了较复杂的模型（类似
ResNet) 部分神经层我们还没说到，会在后续章节进行讲解。程序代码如下：

(7）训练模型：因为标 $\exists\models$ 有两种，故使用
binary crossentropy 损失函数。fi()可直接使用 Dataset。程序代码如下：

![](figures/379-2-FIGURE.jpg)

D训练模型用时甚久，可将程序改在Google Colaboratory云端环境执行，记得要设定使用GPU或TPU。

（8）从目录中任选一个文件进行测试，建议从网络上下载文件来测试，但由于涉及图片版权，因此请读者自行修改第 $3$ 行文件路径。程序代码如下：

除了TensorFlow/Keras 提供的数据，如增补功能之外，还有其他的函数库，提供更多的数据增补效果。比如
Albumentationsl包含的类型多达70种，很多都是

![](figures/380-3-FIGURE.jpg)

执行结果如下。

D训练 $5$ epochs准确率约为 76%

3依据原作者实验，若训练50epochs，验证准确率可达 $9 6 \mathbb{7}_{0}$ 

![](figures/380-7-FIGURE.jpg)

执行结果：是猫的概率 $= 9 7. 2 7 \%_{\circ}$ TensorFlow/Keras所没有的效果，如图6.20所示的颜色数据增补

![](figures/381-1-FIGURE.jpg)

图 $6$ 20 颜色数据增补

虽然前文有说过深度学习是黑箱科学，但是，科学家们依然试图解释模型是如何辨识的，这方面的研究领域统称为可解释的 $\mathbf{A} \mathbf{I}$ (eXplainable Al, XAl) 研究目的如下。

(1）确认模型辨识的结果是合理的：深度学习永远不会跟你说错，「垃圾进，垃圾出」（Garbage ln, Garbage Out 确认模型推估的合理性是相当重要的。

(2）改良算法：唯有知其所以然，才能有较大的进步，光是靠参数的调校，只能有微幅的改善。日前机器学习还只能从数据中学习到知识（Knowledge Discovery from Data, KDD），要进阶到机器能具有智慧（Wisdom）及感知 $( \mathrm{F e e l i n g} )$ 能力，实现真正的人工智能，势必要有更突破性的发展。

(1）使用卷积层提取图像的线条特征，我们可以观察到转换后的结果吗？

(2）甚至更进一步，我们可以知道哪些线条对辨识最有帮助吗？

范例1。重建卷积层处理后的影像：观察线条特征。透过多次的卷积层/池化层处理，观察图像会有何种变化

## 6-7 可解释的AI

目前XA1用可视化的方式呈现特征对模型的影响力，例如：

接下来我们以两个范例展示相关的做法请参阅程序：06 07 CNN Visualization.ipynb，此程序修改自『Machine Learning Mastery」中的文章。设计步骤如图6.2 所示。

(2）加载VGG16模型：VGG16为知名的影像辨识模型
TensorFlow当然也有内建此模型，包含已训练好的模型参数，后续章节会介绍到比类预先训练好的模型用法。程序代码如下：

![](figures/383-2-FIGURE.jpg)

冬 $6. 2 1$ 设计步骤

(1）载 $\lambda$ 库。程序代码如下

![](figures/383-5-FIGURE.jpg)

1#载人 VGG16 模型 $^2_{3}$  $\mathrm{m o d e 1} \,=\, \mathrm{V G G 1 6 \, ( \c)}$  $y ( )$ model.sumnar

执行结果：执行结果如 $\mathrm{T}$ ，包括16层卷积/池化层

Model: "vgg16"

| Layer (type) | Output Shape | Param # |
| --- | --- | --- |
| input_1 （Inputlayer) | [(None,224,224,3)] | 0 |
| block1 conv1 （Conv2D) | (None，224,224，64) | 1792 |
| block1 conv2 （Conv2D) | (None，224,224，64) | 36928 |
| blockl pool (MaxPooling2D) | (None，112,112,64) | 0 |
| block2 conv1 （Conv2D) | (None，112,112, 128) | 73856 |
| block2 conv2 （Conv2D) | (None,112,112,128) | 147584 |
| block2_pool (MaxPooling2D) | (None，56,56,128) | 0 |
| block3 conv1 (Conv2D) | (None，56,56,2 256) | 295168 |
| block3 conv2 （Conv2D) | (None，56, 56, 256) | 590080 |
| block3 conv3 (Conv2D) | (None,56,56,256) | 590080 |

执行结呆：如图6.22所示，可以看出每个滤波器均不相同，表示做了不同的图像处理

执行结果：如图6.23所示，与图6.22相对照，可以看出与第一层不同，表示又做了不同的图像处理。

(3）定义可视化滤波器的函数。程序代码如下

![](figures/384-3-FIGURE.jpg)

(4）可视化第一层的滤波器。程序代码如下

1 Visualize(1)

![](figures/384-5-FIGURE.jpg)

冬 $6. 2 2$ 可视化第一层的滤波器

(5）可视化第15层的滤波器。程序代码如下

1 Visualize(15)

(6）重建第一个卷积层的输出图像：以鸟的图片为例，先进行卷积，接着再重建图像。程序代码如 $\mathrm{F}$ :

执行结果：如图6.24所示可以看见第一层图像处理结果，有的滤波器可以抓到线条，有的则是漆黑 $\neg H$ 

![](figures/385-2-FIGURE.jpg)

冬 $6. 2 3$ 可视化第 $1 5$ 层的流波器

![](figures/385-4-FIGURE.jpg)

(7）重建 $2,$  $5$ ，9，13，17多层卷积层的输出图像。程序代码如下：

执行结果：在第 $9$ 层图像处理结果中，还能够明显看到线条， 但第 $1 7$ 层图像处理结果，已经是抽象到认不出来是鸟的地步 $\overline{{\jmath}}$ 

![](figures/386-2-FIGURE.jpg)

图6.2重建第一个卷积层绘出图像

![](figures/386-4-FIGURE.jpg)

第 $9$ 层图像处理结果如图6.25 所示；第17层图像处理结果如图 6.26所示。

![](figures/387-1-FIGURE.jpg)

冬 $6. 2 5$ 第 $9$ 层图像处理结果

![](figures/387-3-FIGURE.jpg)

冬 $6. 2 6$ 第 $1 7$ 层图像处理结果

从以上的实验，可以很清楚看到CNN的处理过程，我们虽然不明白辨识的逻辑，但是至少能够观察到整个模型处理的过程。

范例 $2.$ 使用SHAP 库，观察图像的哪些位置对辨识最有帮助。

SHAP (SHapley Additive exPlanations）库是由 Scott
Lundberg 及 Su-ln Le 所开发的，提供 Shapley Value的计算，并具有可视化的接口，目标是希望能解释各种机器学习模型。库使用说明可参考网址母，下面仅说明神经网络的应用。

Shapley Value 是博变论（Game Theory）而发展出来的，原本是用来分配利益给团队中的每个人时所使用的分配函数，沿用到了机器学习的领域，被应用在了特征对预测结果的个别影响力评估。详细的介绍可参考维基百科

请参阅程序：06 08 Shap MNIST.ipynb。设计步骤如图6.27 所示。

(1）载入MNIST数据集：请注意，目前TensorFlow 2.x 版执行SHAP 有Bug，所以务必使用

SHAP 库安装 pipiasall shap

![](figures/388-7-FIGURE.jpg)

图 $6. 2 7$ 设计步骤

tf.compat.v.disable v2 behavior)，切换回 TensorFlow 1.x版。程序代码如下：

(2）定义CNN模型：与前百模型相同，也可使用其他模型进行测试。程序代码如下

![](figures/389-2-FIGURE.jpg)

![](figures/389-3-FIGURE.jpg)

## (3）模型训练：与前面相同。程序代码如下：

1#模型训练
2 history = model.fit(x train norm, y train, epochs=5, validation split=0.2) 3
4 #评分(Score Model）
5 score=model.evaluate(x test norm, y test, verbose=0)
6
7 for i, x in enumerate(score):
8 print(f'{model.metrics names[i]}: {score[i]:.4f}')

$$
\pi:
$$
算：测试第1批数据。程序代码如

执行结果：会显示图像中每一个像素的归因，每个像素一共有 10个数值，每个数值代表辨识为 $0 \sim9$ 的贡献率。可使用下列指令观察执行结果的维度（10,1,28,28,1)

(5）绘制 $5$ 批测试数据的特征归因：红色的区块（请参看程序）代表贡献率较大的区域。程序代码如下

执行结果：如图6.28所示每一行第一个数字为真实的标记，后面为预测每个数字贡献率较大的区域。

从SHAP 库的功能，我们很容易判断出中央位置是辨识的重点区域，这与我们认知是一致的。另一个名为LIME的库，与

![](figures/390-5-FIGURE.jpg)

1
$$
n p. a r r a y \left( \, \mathrm{s h a p \_v a l u e s} \, \right). \mathrm{s h a p e}
$$

![](figures/390-7-FIGURE.jpg)

![](figures/390-8-FIGURE.jpg)

冬 $6. 2 8$ 绘制特征归因结果

SHAP库齐名，读者如果对这领域有兴趣，可以由此深入研究，在此不作具体说明。

还有一篇论文世提出 $\overline{{\jmath}}$ Class Activation Mapping 概念，可以描绘辨识的热区，如图6.29所示。Kaggle 也有一篇超赞的实践 1u2]，值得大家好好赏读。

透过以上可视化的辅助，不仅可以帮助我们更了解CNN模型的运 $\small\gamma_{\mathrm{F}}$ ，也能够让我们在收集数据时，有较为明确的方向知道重点应该要放在哪里，当然，如果未来能有更创新的想法来改良算法是值得进一步发展研究的。

![](figures/391-3-FIGURE.jpg)

图6.29左上角的图像为原图，左下角的图像显示了辨识热 $\overline{{\chi}}$ ，即猴子的头和颈部都是辨识的主要关键区域

透过CNN模型和数据增补的强化，我们已经能够建立准确度还不错的模型。然而，与近几年影像辨识竞赛中的冠、亚军模型相比较，只能算是小巫见大巫了，冠、亚军模型的神经层数量有些高达 $1 0 0$ 多 $\P$ ，若要自行训练这些模型就需要花上几天甚至 $\prod$ 个星期的时间，难道缩短训练时间的办法，只剩购置企业级服务器这个选项吗？

TensorFlow/Keras、PyTorch等深度学习框架早已为中小企业设想好 $\overline{{\jmath}}$ ，套件提供了事先训练好的模型，我们可以直接套用，也可以只采用部分模型，再接上自定义的神经层，进行其他对象的辨 $\grave{\nu} \P$ ，这些预先训练好的模型就称为Pre-trained Model 或 Keras $\mathbf{A}$ pplications。

## 第7章

## 预先训练的模型在mageNet 历年举办的竞赛 $\left( \mathrm{I L S V R C} \right)$ 当中，近几年产生的冠、亚军，大都是CNN模型的变形，整个演进过程非常精彩， 简述如 $\mathrm{F}$ 。

(1）2012年冠军 $\mathrm{A l e x N e t} \,-$ 举将错误率减少 $1 0 \% \sqcup\mathrm{T X} \pm,$ 且首度导入Dropout层

(2）2014年亚军VGGNet承袭AlexNet 思路，建立了更多层的模型，VGG16/19 分别包括16及19层卷积层及池化层。

(3）2014年图像分类冠至GoogNet & Inception同时导 $\lambda$ 多种不同尺寸的 Kernel，让系统决定最佳Kernel尺寸。Inception引 $\lambda$ 了 Batch Normalization等观念，参见Batch Normalizatiorn.
Accelerating Deep Network $\taur a i n i n g \, b y$ Reducing Interna
Covariate Shitl,

(4）2015年冠军ResNets 发现到20 层以上的模型其前面几层会发生退化（degradation）的状 $\gg\Pi$ ，因而提出以残差
(Residual）方法来解决问题，参见Deep Residual Learning for image Recognitor'l

## 7-1 预先训练的模型简介

Keras 收录了许多预先训练的模型，称为 Keras
Applicationsti，随着版本的更新，提供的模型越来越多，目前 (2021 年）包括的模型如图7.1 所示。

| Model | Size | Top-1 Accuracy | Top-5 Accuracy | Parameters | Depth |
| --- | --- | --- | --- | --- | --- |
| Xception | 88 MB | 0.790 | 0.945 | 22,910,480 | 126 |
| VGG16 | 528 MB | 0.713 | 0.901 | 138,357,544 | 23 |
| VGG19 | 549 MB | 0.713 | 0.900 | 143,667.240 | 26 |
| ResNet5O | 98MB | 0.749 | 0.921 | 25,636,712 |  |
| ResNet101 | 1711 MB | 0.764 | 0.928 | 44,707,176 |  |
| ResNet152 | 232 MB | 0.766 | 0.931 | 60,419,944 |  |
| ResNet5OV2 | 98 MB | 0.760 | 0.930 | 25,613,800 |  |
| ResNet101V2 | 171 MB | 0.772 | 0.938 | 44,675,560 |  |
| ResNet152V2 | 232 MB | 0.780 | 0.942 | 60,380,648 |  |
| InceptionV3 | 92 MB | 0.779 | 0.937 | 23,851,784 | 159 |
| InceptionResNetV2 | 215MB | 0.803 | 0.953 | 55,873,736 | 572 |
| MobileNet | 16MB | 0.704 | 0.895 | 4,253,864 | 88 |
| MobileNetV2 | 14MB | 0.713 | 0.901 | 3,538,984 | 88 |
| DenseNet121 | 33MB | 0.750 | 0.923 | 8,062,504 | 121 |
| DenseNet169 | 57 MB | 0.762 | 0.932 | 14,307,880 | 169 |
| DenseNet201 | 80 MB | 0.773 | 0.936 | 20,242,984 | 201 |
| NASNetMobile | 23 MB | 0.744 | 0.919 | 5,326,716 |  |
| NASNetLarge | 343MB | 0.825 | 0.960 | 88,949,818 |  |
| EfficientNetBO | 29 MB |  | - | 5,330,571 |  |
| EfficientNetB1 | 31 MB | - | - | 7,856,239 |  |
| EfficientNetB2 | 36MB |  |  | 9,177.569 |  |
| EfficientNetB3 | 48 MB |  |  | 12,320,535 |  |
| EfficientNetB4 | 75 MB |  |  | 19,466,823 |  |
| EfficientNetB5 | 118MB |  | - | 30,562,527 |  |
| EfficientNetB6 | 166MB | - | - | 43,265.143 |  |
| EfficientNetB7 | 256 MB |  |  | 66,658,687 |  |

冬 $7. 1$ Keras提供的预先训练模型（Pre-trained Mode）

上述模型的字段说明如下。

(1）Size：模型文件大小
(2) Top-1 Accuracy：预测一次就正确的准确率。
(3） Top-5 Accuracy：预测五次中有一次正确的准确率。 (4）Parameters：模型参数(权重、偏差）的数口。 (5） Depth：模型层数。

(1）Size：模型文件大小

$$
( 2 ) ~ \mathrm{T o p-1 ~ A c c u r a c y} \colon
$$
预测一次就正确的准确率

(3） Top- Accuracy：预测五次中有一次正确的准确率

(4）Parameters：模型参数（权重、偏差）的数目

Keras 研发团队将这些模型先进行训练与参数调校，并且存档，使用者就不用自行训练，直接套用即可，故称为预先训练的模型（Pre-trained Model)

这些预先训练的模型主要应用在图像辨识领域，各模型结构的复杂度和准确率有所差异，图7.2所示是各模型的比较，这里提供给各 $\imath\vec{u}-$ 个简单的选用原则，如果是注重准确率，可选择准确率较高的模型，如ResNet 152； $\nabla\overrightarrow{\vphantom{\vphantom{p}}}$ ，如果要部暑在手机上，就可以考虑使用文件较小的模型，如MobileNet。

这些模型使用」mageNet 100万张图片作为训练数据集，内含 1000种类别，详情请参考yrevar GitHub-l，训练数据集几乎涵盖了日常生活中会看到的对象类别，如动物、植物、交通工具等，所以如果要辨识的对象属于这1000种类别，就可以直接套用模型，反

![](figures/395-3-FIGURE.jpg)

图 $7. 2$ 预先训练模型的准确率与计算速度之比较

(图形来源：How to Choose the Best Keras Pre-Traineo Model for irmage Classificationl

 $\grave{\varUpsilon}$ ，如果要辨识这1000种类别以外的对象，就需要接上自定义的输 $\lambda$ 层及辨识层，只利用预先训练模型的中间层提取特征

(1）采用完整的模型，可辨识lmageNef所提供1000种对象。

3）采用部分模型，并接上自定义的输入层和辨识层，即可辨识这1000种以外的对象。

因此应用这些预先训练的模型，有以下三种方式。

(2）采用部分模型，只提取持征，不作辨识。

以下我们就依照这三种方式各实践一次。 预先训练的模型的第一种用法，是采用完整的模型来辨识1000 种对象，直截了当。

(2）加载 $\mathbf{V}$ GG16模型：显示和绘制模型结构。程序代码如下:

①执行VGG16时，系统会先下载模型文件至用户Home 目录下的/.keras/models/a

## 7-2采用完整的模型

范例使用VGG16模型进行对象的辨识。

设计步骤如图 $7. 3$ 所示

![](figures/397-6-FIGURE.jpg)

图 $7. 3$ 设计步骤

请参阅程序:0701 Keras applications.ipynb

(1）载 $\lambda$ 库。程序代码如下：

![](figures/397-10-FIGURE.jpg)

| 1 | model = VGG16(weights='imagenet |
| 2 | print(model.summaryO) |
| 3 |  |
| 4 | # 绘制模型结构 |
| 5 | tf.keras.utils.plot model(model, ,to file='vgg16.png |

 $\bullet$ lmagenet：已使用!mageNet图片完成训练，加载该模型权重

False：不包含最上面的三层，一层是Flatten、另外两层则是Dense。注意：最上面是指最后面的神经层，文件名会包括
notop, 为 vgg16 weights tf dim ordering tf kernels notop.hs.

⑤执行结果：VGG16使用多组的卷积/池化层，共有 $1 6$ 层的卷积/池化层，执行结果如 $\mathrm{F}$ 

$$
\bullet\quad\mathrm{L i n u x / \mathrm{M a c} \colon\sim/ \mathrm{k e r a s / \mathrm{m o d e l s} / \diamond}}
$$

Windows:%HomePath $\mathcal{V}_{O} /$ .keras/models

2文件名的为gl.weinhs ridim ordring f kermels.hs

3参数weight有以下三种选项

None：表示此模型还未经训练，只有模型结构

 $\bigoplus$ 文件路径：使用自定义的权重文件

Oinclude top有以下两种选项

Tue：默认值，表示采用完整的模型模型结构图：为单纯的顺序型模型，后三层为Dense，如图 7.4 所示。

Model: "vgg16"

| Layer (type) | Output Shape | Param# |
| --- | --- | --- |
| input_2 (Inputlayer) | [(None,224, 224, 3)] | 0 |
| block1_conv1 (Conv2D) | (None,224, 224, 64) | 1792 |
| block1.conv2(Conv2D) | (None， 224, 224, 64) | 36928 |
| blockl pool (MaxPooling2D) | (None， 112， 112， 64) | 0 |
| block2_conv1 (Conv2D) | (None, 112, 112, 128) | 73856 |
| block2 conv2（Conv2D) | (None, 112, 112, 128) | 147584 |
| block2_pool (MaxPooling2D) | (None, 56, 56, 128) | ° |
| block3_conv1(Conv2D) | (None， ,56,56, 256) | 295168 |
| block3_conv2(Conv2D) | (None,56,56, 256) | 590080 |
| block3_conv3(Conv2D) | (None, 56, 56, 256) | 590080 |
| block3 pool (MaxPooling2D) | (None, 28, 28, 256) | 0 |
| block4_conv1 (Conv2D) | (None, 28， 28, 512) | 1180160 |
| block4 conv2(Conv2D) | (None， 28， 28, 512) | 2359808 |
| block4 conv3(Conv2D) | (None,28, 28, 512) | 2359808 |
| block4 pool(MaxPooling2D) | (None, 14， 14, 512) | 0 |
| block5 conv1(Conv2D) | (None， 14， 14, 512) | 2359808 |
| block5.conv2(Conv2D) | (None， 14， 14, 512) | 2359808 |
| block5 conv3 (Conv2D) | (None， 14, 14, 512) | 2359808 |
| block5 pool (MaxPooling2D) | (None， ,7, 7,512) | 0 |
| flatten(Flatten) | (None,25088) | 0 |
| fc1（Dense) | (None,4096) | 102764544 |
| fc2（Dense) | (None，4096) | 16781312 |
| predictions （Dense) | (None,1000) | 4097000 |

(3）任选一张图 $\mathbb{H}$ ，如大象的侧百照，进行模型预测。程序代码如 $\mathrm{F}$ :

![](figures/400-1-FIGURE.jpg)

冬 $7. 4$ 模型结构图

('n02504013', Indian elephant, 0.71942127 ('n02504458', 'African elephant',0.24141161) ('n01871265','tusker'
0.03627622)7

(4）再换一张图片，如大象的正百照，进行模型预测。程序代码如下：

0执行结果：前三名的结果分别是图斯克象、非洲象、印度象。

公e国中oaeae公中o
$$
\mathrm{k e r^{\i}, 0. 6 2 6 7 5 3 9 ) \; \;, \; \; \; ( \mathrm{` n} 0 2 5 0 4 4 5 8^{\i}, \, 3 4 1 6 ) \; \;, \; \; ( \mathrm{'n} 0 2 5 0 4 0 1 3^{\i}, \; \mathrm{'n d i a n}_{\_} \mathrm{e l}
$$
ephant
elephant',0.330
0.04290244)]

![](figures/401-4-FIGURE.jpg)

执行结果：前三名的结果分别是印度象、非洲象、图斯克象。

![](figures/401-6-FIGURE.jpg)

(5）改用并加载ResNet50模型，使用其他模型亦可。程序代码如 $\mathrm{T}$ :

(6）任选一张图片，如老虎的大头照，进行模型预测。程序代码如下：

$$
[ {\mathrm{~ (^{t} n 0 2 1 2 9 6 0 4, ~^{t} ~^{t} i g e r^{t}, ~ 0. 8 0 ~} \atop{0. 1 3 3 7 1 0 6 2}}, {\mathrm{~ ( \^{t} n 0 2 1 2 8 9 2 5^{t}, ~^{t} ] ~^{t}}}
$$
aguar
$$
\arraycolsep=3 p t \def\arraystretch{0. 9} \begin{array} {c c c c} {5 ) \linethicklineeq} & {\mathrm{( ` n 0 2 1 2 3 1 5 9, ~ t i g e r \_c a t^{\prime},}} \\ {\line{0. 0. 0 0 0 4 6 8 7 2 2 2 9 2 )}} & {\mathrm{]}} \\ \end{array}
$$
65789

②可以改用tigerl.jpg、tiger2,jpg再尝试看看，结果应该相差不多

②不论正面或是侧面，都可以正确辨 $\grave{\imath} \P$ ，也不用另外去背

![](figures/402-5-FIGURE.jpg)

![](figures/402-6-FIGURE.jpg)

D执行结果：前三名的结果分别是老虎、虎猫、美洲虎预先训练模型的第二种用法，是采用部分模型，只提取特征， 不做辨识。例如，一个3D模型的网站，提供模型搜寻功能，首先用户上传要搜导的图 $\mathbb{H}$ ，网站实时比对出相似的图文件，显示在网页上让用户勾选下载，操作请参考Sketchfab网站，类似的功能应可以适用到许多领域，如比对嫌疑犯、商品推荐等，如图 $7. 5$ 所 $\overline{{\pi}}$ .

## 7-3采用部分模型

![](figures/403-2-FIGURE.jpg)

图 $7. 5$ 3D模型搜导

(数据源：Using Keras' Pretrained Neural Networks for Visual Similarity

范例,使用VGG16模型进行对象的辨识。

7.6所示设计步骤如图

![](figures/403-7-FIGURE.jpg)

 $7$ 6设计步骤冬

(2）加载 $\mathsf{V}$ GG16模型：include top-False 表示不包含最上面的 $\Xi\F$ （辨识层程序代码如下：

(3）提取特征：任选一张图 $\ \#$ ，如大象的侧面照，取得图 $\ \#$ 文件的特征向量。程序代码如下：

## 请参阅程序：0702图像相似度比较ipynb

## (1）载入库。程序代码如下

1#载入VGG 16 ，不含最上面的三层(辨识层)
模型,
2 model = VGG16(weights='imagenet', include top=False) 3 model.summary()

执行结果：执行结果如 $\mathrm{F}$ ，模型不包含Dense

$$
\mathrm{M o d e 1 : ~ ` ` v g g 1 6^{'^{\prime}} ~}
$$

| Layer (type) | Output Shape | Param # |
| --- | --- | --- |
| input_1 (InputLayer) | [(None, None, None, 3)] | e |
| block1 conv1（Conv2D) | (None, None, None, 64) | 1792 |
| block1_conv2 （Conv2D) | (None, None, None, 64) | 36928 |
| block1 pool (MaxPooling2D) | (None， None， None， 64) | e |
| block2 conv1（Conv2D) | (None， None， None, 128) | 73856 |
| block2_conv2（Conv2D) | (None, None, None, 128) | 147584 |
| block2_pool (MaxPooling2D) | (None, None, None, 128) | e |
| block3conv1（Conv2D) | (None， None， None, 256) | 295168 |
| block3 conv2（Conv2D) | (None, None, None, 256) | 590080 |
| block3 conv3 (Conv2D) | (None， None, None, 256) | 590080 |
| block3pool(MaxPooling2D) | (None, None, None, 256) | e |
| block4_conv1 （Conv2D) | (None, None, None, 512) | 1180160 |
| block4_conv2 （Conv2D) | (None， None， None, 512) | 2359808 |
| block4 conv3 (Conv2D) | (None， None， None, 512) | 2359808 |
| block4_pool 1(MaxPooling2D) | (None, None, None, 512) | e |
| block5conv1 (Conv2D) | (None, None, None, 512) | 2359808 |
| block5 conv2 2（Conv2D) | (None, None， None, 512) | 2359808 |
| block5 conv3 （Conv2D) | (None, None, None, 512) | 2359808 |
| block5 pool (MaxPooling2D) | (None， None， None, 512) | e |

 $( 5 )$ 先取得images test目录下所有jpg文件名。程序代码如下：

$$
( 6 ) \boxplus\sp{1 \pm i m a g e s}_{-} \sp{1 \pm i \pm i \pm i \mp i m a g e s}_{-} \sp{1 \pm i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp i \mp1 \mp i \mp i \mp1 \mp i \mp1 \mp i \mp1 \mp\mp i \mp1 \mp\mp\times
$$
I录下所有jpg文件的像索。程序代码

![](figures/405-2-FIGURE.jpg)

## 执行结果：得到图片文件的特征向量如下

| [[[ | 0. | 0. | 0. | 0. | 0. | 0. | ] |
| --- | --- | --- | --- | --- | --- | --- | --- |
| [ | 0. | 0. | 41.877056 | 0. | 0. | 0. | ] |
| [ | 1.0921738 | 0. | 22.865002 | 0. | 0. | 0. | ] |
| [ | 0. | 0. | 0. | 0. | 0. | 0. | ] |
|  | 0. | 0. | 0. * | “ 0. | 0. | 0. | ] |
| [ | 0. | 0. | 0. | 0. | 0. | 0. | ]] |
| [[ | 0. | 0. | 36.385143 | 0. | 0. | 3.260 | 6328]1 |
| [ | 0. | 0. | 80.49929 | 8.425463 | 0. | 0. | ] |
| [ | 0. | 0. | 48.48268 | 0. | 0. | 0. | ] |
| • [ | ·· 0. | 0. | 0. • | . 4.342996 | 0. | 0. | ] |
| [ | 0. | 0. | 0. • | • 0. | 0. | 0. | ] |
| [ | 0. | 0. | 0. ” | 0. | 0. | 0. | ]] |

(4）相似度比较：使月cosine similarity比较特征向量。

$$
\mathrm{T} :
$$

| 1 | from os j import listdin |
| 2 | from os.path n import isfile, join |
| 3 |  |
| 4 | #取得images test 日录下所有.jpg文件名称 |
| 5 | 一 img path = './images test/"  |
| 6 | image files = np.array([f for f in listdir(img path) |
| 7 | 一 if isfile(join(img path, f)） and f「-3:1 == ipg']) |
| 8 | image files |

## 执行结果如下。

array(['bird.jpg' 'bird2.jpg', 'deer.jpg 'elephant.jpg
'elephant2.jpg 'lion1.jpg 'lion2.jpg' panda1.jpg 'panda2.jpg' panda3.jpg 'tiger1.jpg' 'tiger2.jpg tiger3.jpg'], dtype='<U13

(8）使用cosine similarity函数比较特征向量相似度。cosine similarity计算两个向量的夹角，如图 $7. 7$ 所示，判断两个向量的方向是否近似，cosine 函数的值介于 $(-1, 1 )$ ，越接近 $1$ ，表示方向越相近。

![](figures/406-1-FIGURE.jpg)

(7）取得所有图片文件的特征向量。程序代码如下

| 1 | # 取得所有图片的特征向量 |
| 2 | features = model.predict(X) |
| 3 |  |
| 4 | features.shape, X.shape |

1#取得所有图片的特征向量
2 features = model.predict(X) 3
4 features.shape, X.shape

执行结果：输出与输入的维度比较

$$
( \begin{array} {c} {( 1 3, 7, 7, 5 1 2 )} \\ {, \ ( 1 3, 2 2 4, 2 2 4, 3 )} \\ \end{array} )
$$

![](figures/406-7-FIGURE.jpg)

图 $7. 7$ 夹角与cosine函数

观察比对的结果，如预期一样是正确的。利用这种方式，不只能够比较|mageNet 1000 类中的对象，也可以比较其他的对象，如 3D模型图片文件，读者可以在网络上自行下载一些图片进行测试。

执行结果：与tiger2jpg比较的相似度如下。

$$
\begin{array} {c} {[ 0. 3 5 1 1 7 5 3 7 \: 0. 2 6 6 6 1 6 4 3 \: 0. 1 9 4 0 1 2 8 4 \: 0. 1} \\ {0. 1 4 2 9 8 2 4 1 \: 0. 1 0 6 6 1 6 7 1 \: 0. 1 0 6 1 2 2 1 2 \: 0. 0 9 7 4 1} \\ {0. 0 8 4 4 0 3 5 1 \: 0. 0 8 0 9 7 0 8 3} \\ \end{array}
$$
19142228 0.1704499
708 0.09370482

对应的文件名如下：

'tiger1.jpg" 'tiger3.jpg' 'lion1.jpg' 'elephant.jpg
'elephant2.jpg' 'lion2.jpg 'panda2.jpg 'panda3.jpg'
'bird.jpg' 'panda1.jpg' 'bird2.jpg' 'deer.jpg'], dtype='<U13

预先训练的模型的第三种用法，是采用部分的模型，再加上自定义的输 $\lambda$ 层和辨识层，如比就能够不受限于模型原先辨识的对象，也就是所谓的转移学习（Transfer Learning），或者翻译为迁移学习。其实不使用预先训练的模型，直接建构CNN模型，也是可以辨识出任何对象的，然而为什么要使用预先训练的模型呢?原因归纳如下。

(1）使用大量高质量的数据（ImageNet为普林斯顿大学与斯坦福大学所主导的项目） 又加上设计较复杂的模型结构，如
ResNet模型高达150 $\P$ ，准确率因此大大提高。

$$
\fbox{( 3 )} i \Vert z \neq1 0 \pm1 0 \pm4 x \vert\pm1
$$
只需要重新训练自定义的辨识层即

(1）建立预先训练的模型：包括之前的 Keras Applications, 以及后面章节会谈到的自然语言模型Transformer模型，它包含目前最火的BERT，利用大量的训练数据和复杂的模型结构，取得通用性的图像与自然语言特征向量。

(2）微调（Fine Tuning 依照特定应用领域的需求，个别建模并训练，如本节所述，利用预先训练模型的前半段，再加 $\lambda$ 自定义的神经层，进行特殊类别的辨识。

## 7-4 转移学习

(2）使用较少的训练数据：因为模型前半段已经训练好了

一般的转移学习分为以下两阶段。 范例.使用 ResNet152V2模型，辨识花朵数据集（程序源自 Tensorflow 官网所提供的范例『Load images」1，笔者进行了一些修改和批注)

设计步骤如图 $7. 8$ 所示。

![](figures/409-2-FIGURE.jpg)

1 $7$ 8设计步骤冬

请参阅程序：07 03 Flower ResNet.ipynb

(1）载入库：引进ResNetl52V2模型。程序代码如下：

(2）载入Flower数据集。程序代码如下：

执行结果：共3670个文件、 $5$ 种类别 $( \mathrm{c l a s s} )$ 其中
2936个文件作为训练之用，734个文件作为验证使用。

(3）进行特征二程，将特征缩放在 $( 0, 1 )$ 之间。程序代码如下:

D共有152层卷积/池 $\bounderbrace{\chi} \equiv$ ，再加上其他类型的神经层，总共有 $5 6 6 \ | \Xi$ 

![](figures/410-3-FIGURE.jpg)

![](figures/410-4-FIGURE.jpg)

## (4）显示ResNet152V2完整的模型结构。程序代码如下：

1 base model = ResNet152V2(weights='imagenet') 2 print(base model.summary())

执行结果如下。

(5）建立模型结构：使用Function AP!加上自定义的辨识层 (GlobalAveragePooling、Dense) 再指定 Mode 的输入输 $\uplus$ 。 程序代码如下：

(6）模型训练：设定缓存、预存取，以提升训练效率。程序代码如下：

## 2输入层（Input-ayer）维度为（224,224,3 。具体如下：

| Model: resnet152v2 |
| Layer (type) | Output s Shape |
| input 5 (InputLayer) | [(None, 224, 224，3) |

3最后两层为 GlobalAveragePooling、Dense，若加上 include top-False，这两层则会被移除。具体如下：

| post bn (BatchNormalization) | (None,7,7,2048) |
| --- | --- |
| post relu (Activation) | (None,7,7,2048) |
| avg pool (GlobalAveragePooling2 (None,2048) |
| predictions （Dense) | (None, 1000) |

![](figures/411-6-FIGURE.jpg)

执行结果：如图 $7. 9$ 所示，随着训练周期的增长，验证准确率并没有提高，这是因为预先训练的模型已将大部分的神经层训练过 $\overline{{\jmath}}$ 。

执行结果如下。

D训练准确率:93.33%

②验证准确率： $8 7. 7 4 \%_{\circ}$ 

(7）绘制训练过程的准确率/损失函数。程序代码如下：

1#对训练过程的准确率绘图
2 import matplotlib.pyplot as plt
3 plt.rcParams['font.sans-serif' $]=[^{\prime}$ Microsoft JhengHei']
4 plt.rcParams['axes.unicode minus'] = False
5
6 plt.figure(figsize=(8, 6))
7 plt.plot(history.history['accuracy'], $r^{\prime}$ ，label='训练准确率"
8 plt.plot(history.history['val accuracy'], "g', label='验证准确率'） 9 plt.xlabel('Epoch')
10 plt.ylabel("准确率"）
11 plt.legend()

![](figures/412-6-FIGURE.jpg)

 $7. 9$ 训练过程准确率冬

(9）预测：任选一张图片，如玫瑰花，预测结呆正确。程序代码如 $\mathrm{T}$ :

注意：笔者一开始使用cifar 10内建数据集，它的图片宽高只有（28,28），而ResNet的训练模型的输入维度则为（224
224），虽然还是可以训练，因为ResNet 会自动将cifar 10 数据放 $\mathcal{X}$ ，但也因此导致图像模糊，模型辨识能力变差。提醒读者在应用预先训练的模型时要特别留意，大部分模型的输入维度都在（224, 224）以上。

(8）显示辨识的类别。程序代码如下：

1#显示辨识的类别
2 class names = train ds.class names 3 print(class names)

执行结果： $\lfloor$ daisyJ 『dandelionu Trosesj 'sunflowersJ Ftulips $\rceil$ 

![](figures/413-4-FIGURE.jpg)

## D执行结果如下

预测概率(%）:[0.03,0.
$$
\emptyset, \, \, \, 9 9 \,. 7 8 \,, \, \, \, \otimes. 8 4 \,, \, \, \, \emptyset. 1 5 \,, \, \, \, \emptyset. \emptyset\,, \, \, \, \emptyset. \emptyset\,, \, \, \, \emptyset. \emptyset\,, \, \, \, \emptyset. \emptyset\, ]
$$
预测类别：roses

2再任选一张图片，如雏菊，执行结果也正确

上一节我们使用复杂的ResNet152V2模型，其中内含许多的 Batch Normalization 神经层，它在神经网络的反向传导时可消除梯度消失（Gradient Vanishing）或梯度爆炸（Gradient Exploding) 现象，所以，我们研究了其原理与应用时机。

当神经网络包含很多神经层时，经常会在其中放置一些Batch Normalization 层，顾名思 $\grave{\chi}$ ，它的用途应该是特征缩放，然而，究竟内部是如何运作的？有哪些好处？运用的时机为何？摆放的位置为何?

Sergey lotie 与 Christian Szegedy 在 2015年首次提出了 Batch Normalization，论文标题为Batch Normalization:
 $A c c e l e r a t i n g \, D e e p$ Network Trai
$$
\mathit{n i n g \, b y \, R e d u c i n g \, I n t e r n a l}
$$
Covariate $S h i f t^{[ 8 ]}$ 。简单来说，Batch Normalization 即为特征缩放，将前一层的输出标准化后，再转至 $\mathrm{F-F}_{\omicron}.$ 

标准化的好处就是让收敛速度快一些，假如没有标准化，模型通常会对梯度较大的变量先优化，进而导致收敛路线曲折前进，如图7.10所示，左图是特征未标准化的优化路径，右图则是标准化后的优化路径。

## 7-5 Batch Normalization 说明

![](figures/414-5-FIGURE.jpg)

![](figures/414-6-FIGURE.jpg)

![](figures/414-7-FIGURE.jpg)

Gradient of larger parameter dominates the update

Both parameters can be
updated in equal proportions

Batch Normalization另外再引进两个变量， $\beta,$ 分别控制规模缩放（Scale）和偏移（Shift) 17.11所示。
如图

(1）标准化是在训练时逐批处理的，而非所有数据一起标准 $\Psi$ ，通常加在 Activation Function之前。

(3）（B值是在训练过程中计算出来的，并不是事先设定好的。

假设我们要建立小狗的辨识模型，却收集黄狗的图片进行训练，模型完成后，拿花狗的图片来辨 $\grave{\imath} \P$ ，效果当然会变差，要改善的话则必须重新收集数据再训练 $- \lambda\pi$ ，这种现象就称为Covariate 图 $7. 1 0$ 未标准化&标准化优化过程的示意图

(图片来源：Why Batoh Nomaization Matters?

| Input: Values of z over a mini Parameters to be learne Output: {yi= BN,y,s(z;)} | batch: B = {ai...m}; l:y,B  |
| MB← 1 m Ei /mini-batch mean m > i-1  |
|  2—1 1 m (Gi:-MB) // mini-batch variance m 中  |
|  2=1 D← Di-LB 2E | // normalize |
|  VBTE y:<-E;+B=BN,s(:) | // scale and shift |

图7.11 Batch Normalization公式

(图片来源：Why Batch Normalization Maters?9) 补充说明如下。

(图片来源：Why Batch Normaization Matters? )

$$
\star\textsc{h e i n h e r e}_{\circ}
$$

(2）8是为了避免分母为0而加上的一个微小正数 Shift，正式的定义是「假设我们要使用 ${\bf X}$ 预测 $\mathbf{Y}$ 时，当 $\mathbf{X}$ 的分配随着时间有所变化时，模型就会逐渐失效」。股价预测也是类似的情形，当股价长期趋势上涨时，原来的模型就慢慢失准 $\overline{{\jmath}}$ ，除非纳 $\lambda$ 最新的数据重新训练模型。

由于神经网络的权重会随着反向传导不断更新，每一层的输出都会受到上一层的输出影响，它是一种回归的关系，随着神经层越多，整个神经网络的输出就有可能会逐渐偏移，此种现象称之为 Internal Covariate Shift.

而 Batch Normalization 就可以矫正 Internal Covariate Shit现象，它在输出至下一层的神经层时，每批数据都会先被标准化，这使得输入数据的分布全属于N（0,1）的标准正态分布，因此，不管有多 $\acute{\omicron}$ 层神经层，都不用担 $\imath\grave{\mathrm{L}} \imath$ 发生输出逐渐偏移的问题。

至于什么是梯度消失和梯度爆炸?白于CNN模型共享权值 (Shared Weights）的关系，使得梯度逐渐消失或爆炸，原因如下，相同的 $\mathit{W}$ 直若是经过很多层。

(1）如果W<1→模型前几层的r愈大，W会趋近于 $0,$ 、则影响力逐渐消失，即梯度消失。

(2）如果1>1 →模型前几层的n愈大，W"会趋近于 $\infty$ ，则会导致模型优化无法收敛，即梯度爆炸

只要经过Batch Nornalization，将每一批数据标准化后，梯度都会重新计算，这样就不会有梯度消失和梯度爆炸的状况发生 $\overline{{\jmath}}$ 。 除比之外，根据原作者的说法，Batch Normalization还有以下优点。

(2）可使用较大的学习率（Usehigher learning rates) 加速训练过程。

(4）不使用Batch Normalization时，Activation function容易在训练过程中消失或提早停上学习，但如果经过Batch
Normalization 则又会再复活（Makes activation functions viable
by regulating the inputs to them

(6）类似于Dropout的效果，可防止过度拟合（t adds noise which reduces overitting with a regularization effect) 所以，当使用Batch Normalization时，就不需要加Dropout层 $\overline{{\jmath}}$ ，就是为避免效果加乘过强，反而造成低度拟合（Underiting)

有一篇文章「On The Peris of Batch Norm」的讲述 $\Im-$ 个很有趣的实验，使用两个数据集模拟Internal Covariate Shift现象， 一个是 MNIST数据集，背景是单纯白色，另一个则是SVHN数据集，有复杂的背景，实验过程如 $\mathrm{F}$ 。

(1）首先合并两个数据集来训练第一种模型，如图 $7. 1 2$ 所示。

$$
( 1 ) \, \not\in\{\mathrm{~ ( ~ k ~ ) ~} \not\equiv\mathrm{~ ( T r a i n ~ f a s t e r ) ~} \ 。
$$

(3）权重初始化较容易（Paramtrinitialization is easier

(5）准确率全百性提是升rsusovral) O

(2）再使用两个数据集各自分别训练模型，但共享权值，为第二种模型，如图7.13所示。

两种模型都有插入Batch Normalization，比较结果，前者为单一模型，准确度较高，因为Batch Normalization 可以矫正 Internal Covariate Shit现象：后者则白于数据集内容的不同，两个模型共享权值本来就不合理，如图7.14所示

![](figures/418-2-FIGURE.jpg)

图 $7. 1 2$ 合并两个数据集来训练一个模型

![](figures/418-4-FIGURE.jpg)

图7.13：使用两个数据集个别训练模型，但共享权值

(3）第三种模型：使用两个数据集训练两个模型，分别作 Batch Normalization，但不共享权值。比较结果为：第三种模型效果最好，如图7.15所示。

![](figures/419-1-FIGURE.jpg)

冬两种模型准确率比较 7.14

![](figures/419-3-FIGURE.jpg)

图7.15、三种模型准确率的比较

恭喜各位勇士们通过卷积神经网络（CNN）关卡，越过 $\Im-\pi$ 高山。本篇我们就来好好秀一下努力的成果，展现CNN在各领域应用上有哪些厉害的功能吧！

## 第三篇、进阶的影像应用

本篇包括下列主题。

(1）目标检测（Object Detection）
(2）语义分割（Semantic Segmentation
(3）人脸辨识（Facial Recognition
(4）风格转换（Style Transiern
(5）光学文字辨识（Optical Character Recognition OCR)

(1）目标检测（Objeot Detecion O

(2）语义分割（Semantic O

(3）人脸辨识（Facial Recognition O

(4）风格转换（Style Transter O

(5）光学文字辨识（Optical Character Recogntion, OCR)

前面介绍的图像辨识模型，一张图片中仅含有一个对象，接下来要登场的日标检测可以同时检测多个目标，并且标示出目标的位置。但是标示位置有什么用处呢？现今最热门的目标检测算法
YOLO，发明 $\mathcal{A}$ Joseph Redmon 提出了一张有趣的照片，如图 8.1 所示。

机器人要能完成煎饼的任务，它必须知道煎饼的所在位置，才能够将饼翻面，如果有两张以上的饼，还需知道要翻哪一张。不 $\P$ 机器人工作时需要计算机视觉，其他领域也会用到目标检测，举例如下。

(1）自动驾驶汽
$$
\mathrm{i \mp\Pi( S e l f-d r i v i n g \ C a r )}
$$
：需要实时掌握前方路
况及闪避障碍物。

## 第8章

## 目标检测

![](figures/421-5-FIGURE.jpg)

8.1 机器人制作煎饼冬

(图片来源：Real-Time Grasp Detection Using Convolutiona Neural Networkst

(2）智能交通：车辆检测，利用一辆车在两个时间点的位置，计算车速，进而可以推算道路拥塞的状 $\gg\Pi$ ，也可以用来检测违规车辆。

(4）异常检测（Anomaly Detection 可以在生产线架设摄影机，实时检测异常的瑕疵，如印制电路板、产品外观等。

） 玩具、无人机、飞弹等都可以做类似的应用 (3)

(5）无人商店的购物篮扫描、自动结账等纵观历年ImageNet LSVRC挑战赛（Large Scale Visual Recognition Challenge）的竞赛题目， ${\cal M}$ 2011年的影像分类

(Classification）与定位（Classification with Localization） $[ 2 ]$ ，到 201 $7$ 年，题目扩展至物体定位（Object Localization）、目标检测、视频目标检测（Object Detection from Video）。我们可以从中观察到图像辨识模型的发展历程，了解到整个技术的演进。目前图像辨识大概分为以下四大类型，如图 $8. 2$ 所示。

(1）语义分割：按照对象类别来划分像素区域，但不区分实例（Instance）。拿图 8.2的第四张照片为例，照片中有两只狗都使用同一种颜色表达，即是语义分割；两只狗使用不同颜色来表 $\overline{{\Pi}}$ ，区分实例，则称为实例分割。

## 8-1 图像辨识模型的发展

![](figures/423-4-FIGURE.jpg)

冬 $8. 2$ 目标检测类型

## (图片来源：Delecion and Segmentaion

(2）定位（Classification + Localzation）：标记单一对象 (Single Object）的类别与所在的位置。

(3）目标检测：标注多个目标（Multiple Obiect）的类别与所在的位置

(4）实例分割（Instance Segmentation) 标记实例，同一类的对象可以区分，并标示个别的位置，尤其是对象之间有重叠时。

接下来我们就逐一介绍上述四类算法，并说明如何利用 TensorFlow 进行实践。

目标检测要能够同时辨识对象的类别与位置，如果拆开来看就是以下两项任务（Task

(2）回归（Regression 找到对象的位置，包括对象左上角的坐标和宽度/高度。

最原始的算法是采用滑动窗口（Sliding Window 与前面介绍的卷积作法相似，设计步骤如图8.3所示。

(1）设定某一尺寸的窗口，比如宽高各为128像素，白原图左上角起裁剪成窗口大小。

(3）滑动窗口，再次裁剪，并回到步骤 $2$ ，直到全图扫描完为止。

## 8-2滑动窗口

$$
( 1 ) \not\exists\not\equiv\mathrm{( C l a s s i f i c a t i o n )}
$$
：辨识对象的类别。

![](figures/425-7-FIGURE.jpg)

图8.3设计步骤

(2）辨识窗口内是否有对象存在 (4）缩小原图尺寸后，万重新回到步骤 $1$ ，寻找更大尺寸的对象。

这种将原图缩小成各种尺寸的方式称为影像金字塔（mage $\mathrm{P y}$ ramid) 详情请参阅image Pyramids with Python and OpenGV IS，如图8.4所示

$$
\begin{array} {l} {\mathrm{o 8 \_O 1_{-} S l i d i n g \_W i n d o w \_\mathrm{A n d}}} \\ {\mathrm{g m \_O l i d i n g \: W i n d o w s \: f o r \: O b i e}} \\ {\mathrm{o p e n C V_{\perp} ~^{[ 1 7 ]_{o}} ~}} \\ \end{array}
$$
上中gsa上公湖中上a中品，国 格饮
Image Pyramid.ipynb.

![](figures/426-3-FIGURE.jpg)

8.4 影像金字塔（Image Pyramid)

(最下层为原图，往上逐步缩小原图尺寸，图片来源：
$$
\mathrm{I I P I m a g e^{[ 6 ]} \, )}
$$

(最下层为原图，往上逐步缩小原图尺寸，图片来源：

范例。对图片滑动窗口并做影像金字塔

请参阅程序：

1）加载库，需先安装 OpenCV、imutls。程序代码如下：

(2）定义影像金字塔操作函数：逐步缩小原图尺寸，以便找到较大尺寸的对象。程序代码如下

| 1 | #影像金字塔操作 |
| 2 | #image·原图，scale：每火缩小倍数，minSize:最小史寸 |
| 3 | def pyramid(image, scale=1.5, minSize=(30, 30)): |
| 4 | #第一久传回原图 |
| 5 | vield image |
| 6 |  |
| 7 | while True: |
| 8 | #计算缩小后的寸 |
| 9 | w= int(image.shape[11 / scale) |
| 10 | #缩小 |
| 11 | image = imutils.resize(image, width=w |
| 12 | #直到最小尺立为止 |
| 13 | if image.shape[o] < minSize[1] or image.shape[1] < minSize[o]: |
| 14 | break |
| 15 | #传回缩小后的图缘 |
| 16 | yield image |

## (3）定义滑动窗口函数。程序代码如下

| 1 | 窗口 |
| 2 | iding window(image, stepSize, windowSize): |
| 3 | ry in range(o,j image.shape[o], stepSize): # 向下海动 stepSize 格 m |
| 4 | for x in range(o, image.shape[1], stepSize)：#向石海动stepSize 格 1 |
| 5 | #1 传回裁剪后的窗口 |
| 6 | yield (x,y, image[y:y + windowSize[1l, x:x + windowSize[01]) |

(4) 。程序代码如下测试。

![](figures/428-0-FIGURE.jpg)

D执行结果：如图8.5所示

![](figures/428-2-FIGURE.jpg)

 $8. 5$ 测试结果冬

②全程的执行结果可参阅视频文件1
video\Sliding Window And mage Pyramid.mp4

方向梯度直方图（Histogram of oriented gradient, HOG），是
龙 $( \mathrm{C e l l} ) ~ ~, ~ ~ N$ 抓取图像轮廓线条的算法，先将图片切成很多个区域
每个区域中找出方向梯度，并把它描绘出来，就形成了对象的轮廓，与其他边缘提取的算法比起来，它对环境的变化，如光线有较强（Robust）的辨识能力，如图8.6所示。有关H0G算法的详细处理方法可参阅「方向梯度直方图」。

根据「Histogram of Oriented Gradients and Object
Detection」 9一文的介绍，结合HOG 的日标检测，流程如图 $8. 7$ 所示。

## 8-3方向梯度直方图

![](figures/429-3-FIGURE.jpg)

图8.6 HOG处理

(左图为原图，右图为HOG 处理过后的图）

(1）收集正样本（Positive setb 集结目标对象的各式图像
尺寸、背景的图像。
样本，包括不同视角、

(2）收集负样本（Negative sel) 集结无目标对象的各式图像样本，若有找到相近的对象则更好，可以增加辨识准确度。

(3）使用以上正/负样本与分类算法训练二分类模型，判断是否包含目标对象， $( \mathrm{S V M} )$ 算法
一般使用文持向量机

(4）Hard-negative Mining：扫描负样本，使用滑动窗口的技 $\Im$ ，将每个窗口导入模型来预测，如果有检测到目标对象，即是伪旧性（False Positive），接着将这些图像加到训练数据集中进行重新训练，这个步骤可以重复很多次，能够有效地提高模型准确率， 类似于 Boosting 整体学习的算法。

(5）使用最后的模型进行目标检测：将目标对象的图像使用滑动窗口与影像金字塔技巧，导入模型进行辨识，找出合格的窗 。

![](figures/430-5-FIGURE.jpg)

图 $8. 7$ 结合HOCG的目标检测之流程图

(6）筛选台格的窗口：使用Non-Maximum Suppression (NMS）算法，剔除多余重叠的窗口

请参阅程序：08 02 修改自
HOG-Face-Detection.ipynb,
Sckit-lmage 的范例。

(1）载入库：本例使月用scikit-image 库， $\mathrm{O p e n C V}$ 也有支持类似的函数。程序代码如下：

(2）HOG 测试：使用Scikit-lmage 内建的女航天员图像来测试HOG的效果。程序代码如 $\mathrm{F}$ :

范例1，使用HOG、滑动窗口及 SVM 进行目标检测

![](figures/431-4-FIGURE.jpg)

1 # Scikit-Image 的范例
2#载人库
3 import numpy as np
4 import matplotlib.pyplot as plt 5 from skimage.feature import hog 6 from skimage import data, exposure

![](figures/431-6-FIGURE.jpg)

执行结果：原图与HOG处理过后的图比较如图8.8所示

![](figures/431-8-FIGURE.jpg)

(3）收集正样本：使用 sciki-learn 内建的人脸数据集作为正样本，共有13233个。程序代码如下：

(5）收集负样本：使用Scikit-Image 内建的数据集，共有 $9$ 批。程序代码如下：

图 $8. 8$ HOG 处理前后对比

![](figures/432-3-FIGURE.jpg)

(4）观察正样本中部分图 $\Hat{H}$ 。程序代码如下
O

1#显示正样本部分图片
2 fig, ax = plt.subplots $( 4, 6 )$ 
3 for i, axi in enumerate(ax.flat):
4 axi.imshow(positive patches[500 * i], cmap='gray 5 axi.axis('off')

执行结果：每张图片宽高为 $( 6 2, 4 7 )$ ，如图8.9所示。

![](figures/432-7-FIGURE.jpg)

图 $8. 9$ 观察正样本中部分图片

(6）增加负样本批数：将负样本转换为不同的尺寸，也可以使用数据增补技术。程序代码如下：

![](figures/433-1-FIGURE.jpg)

执行结果：产生27000批图像

(7）观察负样本中部分图 $\ H$ 。程序代码如下：

1#显示部分负样本
2 fig, ax = plt.subplots $( 4, 6 )$ 
3 for i, axi in enumerate(ax.flat):
4 axi.imshow(negative patches[ $6 0 0 \, * \, \mathtt{i} ]$ ,cmap='gray 5 axi.axis('off')

执行结果：如图8.10所示。

(9）使用SVM进行二分类的训练：使用GridSearchCV寻求最佳参数值。程序代码如下：

![](figures/434-1-FIGURE.jpg)

图8.10 负样本中部分图 $\ H$ 

(8）合并正样本与负样本。程序代码如下：

| #合并正样本与负样本 |
| 2 from skimage import feature # To use skimage.feature.hog) |
| 3 from itertools import chain |
|  |
| 3 X train = np.arrav([feature.hog(im) |
| 一 for im in chain(positive patches, |
| negative patches) |
| y train = np.zeros(X train.shape ol) 2  |
| 1 y train[:positive patches.shaperoll= 1 |

执行结果：最佳模型准确率为98.77%

10）取得最佳参数值。程序代码如下 (1依最佳参数值再训练 $- \lambda\pi$ ，取得最终模型。程序代码如下：

$$
\mathrm{T} :
$$

1#依最佳参数值再训练一灭
2 model = grid.best estimator 3 model.fit(X train, y train)

## (12）新图像测试：需先转为灰阶图像。程序代码如下：

$$
\begin{array} {r l} {{1}} & {{\# \, \, \, \, 2 \chi\, \mathrm{\footnotesize\scriptstyle{\scriptstyle{\scriptstyle{3}}}} \, \mathrm{\footnotesize\scriptstyle{\scriptstyle{3}}} \, \, \mathrm{\footnotesize\scriptstyle{\scriptstyle{3}}} \, \, \mathrm{\footnotesize\scriptstyle{\scriptstyle{3}}} \, \, \mathrm{\footnotesize\scriptstyle{\scriptstyle{3}}} \, \, \mathrm{\footnotesize\scriptstyle{\scriptstyle{3}}} \, \, \mathrm{\footnotesize\scriptstyle{\scriptstyle{3}}} \, \, \mathrm{\footnotesize\scriptstyle{\scriptstyle{3}}} \, \, \mathrm{\footnotesize\scriptstyle{\scriptstyle{\scriptstyle{3}}}} \, \, \mathrm{\footnotesize\scriptstyle{\scriptstyle{\scriptstyle{3}}}} \, \,}}} \\ {{2}} & {{\mathrm{\footnotesize\tt{\scriptstyle{t e s t}}}_{-} \, \mathrm{\footnotesize\mathrm{\footnotesize\small{\scriptstyle{\scriptstyle{3}}}}} \, \mathrm{\footnotesize\mathrm{\footnotesize\mathrm{\footnotesize\scriptstyle{3}}}} \, \mathrm{\footnotesize\mathrm{\footnotesize\mathrm{\footnotesize\scriptstyle{3}}}} \, \, \mathrm{\footnotesize\mathrm{\footnotesize\mathrm{\footnotesize\scriptstyle{3}}}} \, \, \mathrm{\footnotesize\mathrm{\footnotesize\mathrm{\footnotesize\scriptstyle{\scriptstyle{3}}}}} \, \,=\, \mathrm{\footnotesize\mathrm{\footnotesize\mathrm{\footnotesize\mathrm{\footnotesize\scriptstyle{3}}}}} \, \mathrm{\tt{\small{\small{~ \scriptstyle{~ 3}}}}} \, \, \mathrm{\footnotesize\mathrm{\footnotesize\mathrm{\footnotesize\mathrm{\footnotesize\scriptstyle{3}}}}} \, \, \mathrm{\footnotesize\mathrm{\footnotesize\mathrm{\footnotesize\scriptstyle{~ \scriptstyle{3}}}}} \, \, \mathrm{\footnotesize\mathrm{\footnotesize\mathrm{\footnotesize\scriptstyle{~ \scriptstyle{~
$$
6
7
8 plt.imshow(test img, cmap='gray')
9 plt.axis("off');

执行结果：如图8.11所示。

![](figures/435-5-FIGURE.jpg)

图8.1新图像测试

13）定义滑动窗口函数。程序代码如下：

| 1 | #海动窗口函数 |
| --- | --- |
| 2 | def sliding window(img, patch size=positive patches[o].shape, |
| 3 | istep=2, jstep=2, scale=1.0): |
| 4 | Ni, Nj = (int(scale * s) for s in patch size) |
| 5 | for i in range(o, img.shape[O] - Ni, istep): |
| 6 | for i in range(o, img.shape[11 - Ni, jstep): |
| 7 | patch= img[i:i + Ni, i:i+ NiT  |
| 8 | if scale != 1:  |
| 9 | patch = transform.resize(patch, patch size) |
| 10 | yield (i, j), patch |

 $( 1 6 )$ 筛选合格窗口：使用 Non-Maximum Suppression $\mathrm{( N M S )}$ 算法，剔除多余的窗口。以下采用Non-Maximum
$$
S u p p r e s s i o n \: f o r \: O b j e c t \: D e t e
$$
ction i $n \, \bigskip\bigskip\medskip\medskip\medskip\medskip\medskip$ 文的程序代码。

(14）计算HOG：使用滑动窗口来计算每一滑动窗口的 HOG，导入模型辨识。程序代码如下：

![](figures/436-2-FIGURE.jpg)

执行结果： 合格窗口共有55个

 $( 1 5 )$ 显示这55个合格固口。程序代码如下：

| 1 | #将每一个检测到的窗口显示出来 |
| 2 | fig, ax = plt.subplots() |
| 3 | ax.imshow(test img, cmap='gray') |
| 4 | ax.axis('off') |
| 5 |  |
| 6 | #取得左上角坐标 |
| 7 | Ni, Nj = positive patchesrol.shape |
| 8 | indices = np.array(indices)  |
| 9 |  |
| 10 | #显示 |
| 11 | fori, jin indices[labels == 11: |
| 12 | ax.add patch(plt.Rectangle((j, i), Nj, Ni, edgecolor='red', |
| 13 | alpha=0.3,lw=2, facecolor='none')) |

执行结果：如图8.12所示

![](figures/436-7-FIGURE.jpg)

 $8. 1 2$ 显示合格窗口图

定义NMS算法函数：这是由 Pedro Felipe Felzenszwalb等学者发明的算法，执行速度较慢，Tomasz Malisiewicz 因此提出了改善的算法。函数的重叠比例阈值（OverlapThresh）参数般设为0.3~0.5。程序代码如下：

(17）呼叫Inon max suppression slow函数，剔除多余的窗口。程序代码如下：

| 1 | #Non-Maximum Suppression演算法 by Felzenszwalb et al. |
| 1 2 3 4 5 C | #Non-Maximum Suppression演算法 by Felzenszwalb et al. # boxes：所有候选的窗口overlapThresh：窗口重叠的比例阈值 def non max suppression slow(boxes, overlapThresh=0.5): if len(boxes) == 0: return []  |
| 6 |  |
| 12 |  |
| 13 1月 | #计莫疾选视支的面积 6米1 |
| 一 19 | 欢一-— #最后一笔  |
| 40 21 | 上C一LC isidxs[last]  |
| 21 99 | l= 1dxs[last」 AA |
| 23 | suppress = [last] |
| 24 |  |
| 25 | #比对最后一笔与其他窗口重叠的比例 |
| 29 N | #以待的有药的您益鸡生三14797 |
| 31 一A | yy1 = max(y1[i], y1[j]) 0 |
| 32 33 | xx2= min(x2[i], x2[j]) 55816-71 |
| 53 34 | yy2= min(yzu, yzLJJ) wmaxO.x-x1+1 |
| 36 |  |
| 一 37 | #计算重叠出例 |
| 27 38 | #I开星豆光 ovenlan- float(w*h）Iaroari1 |
| 一 40 | #物美术十闭值，川存储秘来 |
| TM 41 | WRN if overlap > overlapThresh: |
| T士 42 | rovetadproycilcpaim col. cupnrecc.annend(noc)  |
| 汽一 43 | FFFF-aAFe |
| 44 | #删除合格的窗口，然续比对 |
| 45 | idxs = np.delete(idxs, suppress) |
| 46 |  |
| 47 | #传回合格的窗口 |

以上范例的过程中省略了一些细节，如Hard-negative
mining、影像金字塔，这个例子无法检测多个不同实体与不同尺寸的对象。所以我们再来看一个范例，它可以使用任何CNN模型结合影像金字塔，进行多对象、多实体的检测。

![](figures/438-1-FIGURE.jpg)

执行结果：如图8.13所示，得到了两个台格商口

![](figures/438-3-FIGURE.jpg)

图 $8. 1 3$ 易除多余窗口

范例 $2$ .使用 ResNet50进行目标检测，并标示出位置

8.14斤示设计步骤如图

![](figures/438-7-FIGURE.jpg)

(1）载入库：需要额外安装imutils库，它是一个简单的图像处理库。程序代码如下：

(2）参数设定：此范例是辨识三只斑马同时存在的图像，另外也可以辨识骑自行车的图像（bike.jpg 程序代码如下：

(5）定义滑动窗口和影像金字塔函数，这部分与范例 $2$ 的流程相同。程序代码如下：

## 图 8.14 设计步骤

请参阅程序:08 03 Object Detection.ipynb

![](figures/439-5-FIGURE.jpg)

![](figures/439-6-FIGURE.jpg)

## (3）加载 ResNet5O 模型。程序代码如下：

1 $^\sharp$ 载人 ResNet50 模型
2 model = ResNet50(weights="imagenet", include top=True)

(4）读取要辨识的图片。程序代码如下

1#读取要辨识的图片
2 ${\mathrm{o r i g}} \ =\ c \vee2$ .imread(image path)
3 orig = imutils.resize(orig, width=WIDTH) 4 $\left( {\mathsf{H}}, ~ {\mathsf{W}} \right) \;=\; \mathrm{o r \, i g \,. \, s h a p e \, [ \, : 2 \, ]}$ 

![](figures/440-0-FIGURE.jpg)

## (6）产生影像金字塔，并逐一进行窗口辨识。程序代码如

$$
\mathrm{T :}
$$

![](figures/440-3-FIGURE.jpg)

(7）预测：辨识概率必须大于设定值，并进行 NMS。程序代码如下：

执行结果：因为图中的斑马有重叠，所以少辨识到一匹马，如图8.15所示。

![](figures/441-2-FIGURE.jpg)

改为骑自行车的图像，images test/bike.jpg，执行结果也辨识到两辆，如图8.16所示。后续我们会运用其他算法来改善这个缺点。

白于目标检测的应用范围广大，因此有许多学者前仆后继地提出各种改良的算法，试图提高准确率并加快辨识速度，接下来我们就沿着前辈们的研究轨迹，逐步深入探讨。

![](figures/442-2-FIGURE.jpg)

 $8. 1 5$ 窗口识别1 冬

![](figures/442-4-FIGURE.jpg)

图 $8$ .16窗口识别2

滑动窗口并结合HOG的算法虽然很好用，但是它具有以下缺点。

因此，从2014年开始，每年都有改良的算法出现，如图8.17 所示。

如图8.18所示，第一个神经网络的算法Regions with CNN (以下简称R-CNN）于2014年由 Ross B. Girshick提出，论文标题为Rich feature hierarchies for accurate object detection and sermantic segmentation 1l。架构如下。

## 8-4 R-CNN 日标检测

(1）滑动窗口加上影像金字塔，需要检查的窗口个数太多 $\overline{{\jmath}}$ ，耗时过久。

 $( 2 ) \ -\uparrow\mathrm{S V M}$ 分类器只能检测一个对象

(3）通用性的CNN模型辨识并不准确，尤其是重叠的对象。

![](figures/443-7-FIGURE.jpg)

图 $8. 1 7$ 目标检测算法的发展过程

1）读取要辨识的图 $\ H$ 。
O (2）使用区域推荐（Region Proposal）算法，找到2000个候选窗口。

(3) ICNN提取特征使用

 $( 4 )$  $\mathrm{S V M}$ 辨识。 使用

![](figures/444-3-FIGURE.jpg)

图 8.18 $\mathrm{R-C N N}$ 架构

(图片来源：Rich feature hierarchies for accurate objec detection and semantic segmentation)

更详纽的架构如图8.19所示。

![](figures/444-7-FIGURE.jpg)

 $8. 1 9$ 另一视角的R-CNN架构图

程序处理流程如图8.20所示。

(1）区域推荐（Region Proposal) 用途为改善滑动窗口的
P
过程检查过多窗口的问题，使用区域推荐算法，只找出2000个候选框（Bounding Box）输入到模型。

区域推荐也有多种算法，R-CNN 所采用的是Selective
Search，它会依据颜色（Color）、纹理（Texture）、规模
(Scale)、空间关系（Enclosure）来进行合并，接着再选取2000 个最有可能包含对象的区域，称之为候选框（Bounding Box），如图8.21所示。

(2）特征提取（Feature Extractor 将2000个候选框使用影像变形转换（Image Warping 转成固定尺寸227×227的图

![](figures/445-3-FIGURE.jpg)

图8.20 R-CNN处理流程

![](figures/445-5-FIGURE.jpg)

 $8$ .21区域推荐冬

(最左边的图为原图，将颜色、纹理、规模、空间关系相近的区域合并，最后变成最右边图的区域)

像，导入CNN进行特征提取，如果采用AexNet的话，则每个候选框转换成4096个特征向量。

(3）SVM分类器：比对特征向量，检测对象是否存在与其所属的类别。注意：一种类使用 $- \uparrow\underline{{-}}$ 分类SVM

(4）使用Non-Maximum Suppression $\mathrm{( N M S )}$ 筛选合格的框：选取可信度较高的候选框为基准，计算与基准框的loU
(Intersection-over Union），高loU 值表示高度重叠，就可以把它们过滤掉，类似于 $\b=$ 一节的做法，如图8.22所示。

(5）位置微调：利用回归（Bounding-box Regression）微调候选框的位置。

利用回归计算候选框的四个变量：中心点 $( P_{x}, P_{y} )$ 与宽高
$$
( P_{w}, P_{h} )
$$
其微调 $\big< \lambda$ 式如下，G为预估值。推论过程比较复杂，
详情可参考原文附录 $\mathbf{C}$ 。

![](figures/446-5-FIGURE.jpg)

冬8.22 loU（分母为与目标框联集的面积，分子为与目标框
交集的面积）

$$
\begin{aligned} {\hat{G}_{x}} & {{}=P_{\ub^{\prime}} d_{x} ( P )+P x} \\ {\hat{G}_{\Downarrow}} & {{}=P_{h} d_{y} ( P )+P y} \\ {\hat{G}_{\voEwoheadrightarrow}} & {{}=P_{\o} \exp( d_{\o} ( P ) )} \\ {\hat{G}_{\hoheadrightarrow}} & {{}=P_{\o} \exp( d_{\ho} ( P ) )} \\ \end{aligned}
$$

损失函数如 $\mathrm{T}$ ，采用Ridge Regression，以普通最 $\imath\rfloor\omicron$ 二乘法估算出来的权重为

(2）特征提取：AlexNet，也可采取 $\mathbf{V}$ GG 或者其他 CNN模型。

$$
w_{\star}=\underset{\hat{w}_{\star}} {\operatorname{a r g m i n}} \sum_{i}^{N} ( t_{\star}^{i}-\hat{\boldsymbol{w}}_{\star}^{T} \phi_{s} ( P^{i} ) )^{2}+\lambda\left\| \begin{array} {c} {\hat{\boldsymbol{w}}_{\star}} \\ \end{array} \right\|^{2}
$$

微调后的目标值f为

$$
\begin{aligned} {t_{x}=} & {{} \left( G_{x}-P_{x} \right) / P_{w}} \\ {t_{y}=} & {{} \left( G_{y}-P_{y} \right) / P_{h}} \\ {t_{w}=} & {{} \operatorname{l o g} ( G_{w} / P_{w} )} \\ {t_{h}=} & {{} \operatorname{l o g} ( G_{h} / P_{h} ) )} \\ \end{aligned}
$$

整^R-CNN处理流程涉及相当多的算法，包括如下。

（1）区域推荐：Selective Search.

$$
( 3 ) \textup{S V M} \' J ) \neq\q\bigoplus_{\mathrm{n n o}}
$$

(4)Non-Max
$$
\mathrm{i m u m ~ S u p p r e s s i o n ~ \langle N M S \rangle~_{\circ} ~}
$$

$$
\mathrm{( 5 ) \; \; B o u n d i n g-b o x \; R e g r e s s i o n_{o}}
$$

范例。使用R-CNN 检测图片中的飞机。

设计步骤如图8.23所示

(1）需安装下列OpenCV扩展版：先卸载 $\mathrm{O p e n C V}$ ，再安装扩展版，一般版与扩展版只能择其一。

![](figures/448-1-FIGURE.jpg)

图 $8. 2 3$ 设计步骤

请参阅程序:03 04 RCNN.ipynb

pip uninstall opencv-contrib-python opencv-python pio install opencv-contrib-python

pip uninstallopenov-contrh-ython opencv-python

$$
\mathrm{p i p ~ i n s t a l l ~ o p e n c v-c o n t r i b-p y t h o n}
$$

(2）解压缩图像训练数据。程序代码如下

| 1 import zipfile |
| 2 import os |
| 3 |
| 4 #图象训练数据 |
| 5 path to zip file = './images Object Detection/Images.zip |
| 6 directory to extract to = './images Object Detection/ |
|  |
| 8 #检查目录是否存在 |
| 9 if not os.path.isdir(directory to extract to): |
| 0 #解压缩 |
| 1 with zipfile.ZipFile(path to zip file, 'r') as zip ref: |
| 2 zip ref.extractall(directory to extract to) |

## (3）解压缩标注训练数据。程序代码如下：

1#标注训练数据
2 path to zip file = './images Object Detection/Airplanes Annotations.zip' 3 directory to extract to = './images Object Detection/"
4
5#检查日录是否存在
6 if not os.path.isdir(directory to extract to):
7 #解乐缩 $r^{\prime} )$ as zip ref:
8 with zipfile.ZipFile(path to zip file,
9 zip ref.extractall(directory to extract to)

(6）区域推荐：使月Selective Search算法，OpenCV扩展版提供现成的函数 creat eSelectiveSearchSegmentation()，假如用

## (4）载入库。程序代码如下：

![](figures/449-2-FIGURE.jpg)

(5）显示一张图像训练数据并包含标注。程序代码如下：

![](figures/449-4-FIGURE.jpg)

执行结果：如图8.24所示。

![](figures/449-6-FIGURE.jpg)

图8.24显示一张图像训练数据

户要自行开发，可以参照「R-CNN学习笔记，LaptrinhX的l一文的内容。程序代码如下：

执行结果：如图8.25所示，会依颜色、纹理等提取出2000个候选框（绿色)

![](figures/450-2-FIGURE.jpg)

![](figures/450-3-FIGURE.jpg)

图 $8$ .25区域推荐

(7）定义loU 计算函数：计算两个框的loU。程序代码如下

(8）筛选训练数据：找出文件名为 airplane开头的文件，并使用区域推荐将每个文件各取出2000个侯选框。要留意的是：每个文件必须包含30个以上的正样本（loU>70%）与30个以上的负样本 $\mathrm{( l o U < 3 0 \% )}$ ，才能被列为训练数据。程序代码如 $\mathrm{T}$ :

![](figures/451-1-FIGURE.jpg)

| 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 | 数辉 |
| 半本及负样本的候选框 ges=[] els=[]  |
| -个标注 |
| n enumerate(os.listdir(annot)): |
|  |
| 取得飞机的图像 |
| f i.startswith("airplane"): |
| ——— filename = i.split(".")[0]+".jpg |
| “二50 print(e,filename)  |
| #读取标注文件 |
| #读取标注文件 image = cv2.imread(os.path.join(path,filename)) |
| 16 17 | image = cv2.imread(os.path.join(path,filename）) df = pd.read csv(os.path.join(annot,i))  |
| L 18 | I-p.icau-ovio·Pacrjoln cio,-/ |
| LO 10 | 世取得所有标注的丛标 |
| 20 | gtvalues=[]  |
| 22 | x1= int(row[1][O].split(" ")[O]) |
| 23 | y1= int(row[1][0].split("")[1]) 12080 |
| 24 A- | x2= int(row[1][0].split(" ")[2]) A41611200971 |
| 25 26 | y2=int(row[lj[o.spiit(i3]) otvalues.annend("y1".x1 "".x2 "v1".v1."".2) |
| 一 27 | o |
| 26 29 | 丹公够达仔 ss.setBaseTmape(image) |
| 29 30 | ss.setBaselmage( image) ss.switchToSelectiveSearchFast() |
| 31 | ssresults = ss.process() |
| 52 22 | lmout = image.copy) |
| 34 | #初始亿 |
| 35 | counter=o #正样本批数 |
| 20 37 | c-ocEt4TX flag=0 #1:正负样本批数约=30 |
| 37 38 | tlag=0 #1:止贝件本批数月=30 ff1a0=0 #1·正样水批数30  |
| 38 AA | fflag =o #1:正样本批数=30 欢迎友业儿务 |
| 39 | #1:负样本批数>=30 bflag = 0  |
| 40 |  |
| 41 | #扫描每一个候选框 |
| 42 43 | tor e,result in enumerate(ssresuits) ifQ<2000and f1ag元0:  |
| 45 | x,y,w,h = result #比较区域推荐区域与标注的ⅠoU |
| 46 |
| 4/ 48 | lou = get lou(gtval,t x1":X, X2":X+w, yI":y, y2":yth}) |
| 48 AO | 进价结30出正样水 |
| 50 | if counter < 30: |
| 51 | if iou> 0.70: |
| 52 一 | timage = imout[y:y+h,x:x+w] 6 |
| 53 一A | resized = cv2.resize(timage, (224,224), 中16- |
| 54 55 |  Interpolation = CVZ.INIER AKEA train images.append(resized)  |
| 55 56 | train images.append(resized) tnain laholcannend(1)  |
| 20 57 | trdllldbeis·dppeiull, counter t= 1  |
| 58 | else: |
| 59 | fflag =1 |
| 60 |  |
| 61 | #收集30批负样本 |
| 62 | if falsecounter <30: |
| 63 | if iou< 0.3: |
| 64 | timage = imout[y:y+h,x:x+w] |
| 65 | resized = cv2.resize(timage, (224,224), |
| 66 | interpolation = cv2.INTER AREA |

(9）定义模型： 月 $\mathrm{V G G} \, 1 6$ ，加上自定义的神经层。程序代
使用
码如下：

(10）定义转换函数：将标记 $\mathbf{Y}$ 转为两个变数。程序代码如下：

| 67 |  | train images.append(resized) |
| 68 |  | train labels.append(o) |
| 69 |  | falsecounter t= 1 |
| 70 |  | else: |
| 71 | bflag=1 |
| 72 |  |  |
| 73 | # | #超过30批正样本及负样本表示有目标在框里面 |
| 74 | i | if fflag == 1 and bflag == 1: |
| 75 | except Exception print(e) print("error continue  | print("inside") |
| 76 | flag=1 |
| 77 | n as e: |
| 78 |  |
| 79 | r in "+filename) |
| 80 |  |

![](figures/453-3-FIGURE.jpg)

$$
\mathrm{T} :
$$

| 1 | #定义的数，将标记Y转为工个变数 |
| 2 | from sklearn.preprocessing import LabelBinarizer |
| 3 |
| 4 | class MyLabelBinarizer(LabelBinarizer): |
| 5 | def transform(self, y): |
| 6 | Y= super().transform(y) |
| 7 | if self.y type == 'binary': |
| 8 | 一一 return np.hstack((Y, 1-Y)) |
| 9 | else: |
| 10 | return Y |
| 11 | def inverse transform(self, Y, threshold=None): |
| 12 | if self.y type == 'binary': |
| 13 | return super().inverse transform(Y[:, 0], threshold) |
| 14 | else: |
| 15 | return super().inverse transform(Y, threshold) |

 $( 1 2 )$ 进行数据增补，以提高模型准确率，因为飞机停放的方向可能会有偏斜。程序代码如下

(13）模型训练：原作者训练周期达1000次之多，故设定检查点与提前结束的 Callback，以缩短训练时间。然而笔者只测试了 20epochs，未使用到检查点与提前结束的 Galback。程序代码如下:

## 11）前置处理及训练数据测试数据分割。程序代码如下

![](figures/454-3-FIGURE.jpg)

| 1 | #数据增(Data Augmentation) |
| 2 | trdata = ImageDataGenerator(horizontal flip=True, |
| 3 | vertical flip=True, rotation range=9o) |
| 4 | traindata = trdata.flow(x=X train, y=y train) |
| 5 | tsdata - ImageDataGenerator(horizontal flip=True, |
| 6 | vertical flip=True, rotation range=90) |
| 7 | testdata - tsdata.flow(x=X test, y=y test) |

![](figures/454-5-FIGURE.jpg)

## (14）绘制模型训练过程的准确率。程序代码如下： 执行结果：如图8.26所示，准确率并未稳定上升，这表明训练周期不足。由于笔者只着重在算法的研究，所以没有继续训练下去，如果用于正式项目，则务必多训练几个周期比较妥当。

![](figures/455-1-FIGURE.jpg)

![](figures/455-2-FIGURE.jpg)

8.26、绘制训练准确率

15）任选一张图片测试。程序代码如下

| 1 | #任选一张图片测试 |
| 2 | im = X test[100] |
| 3 | plt.imshow(im) |
| 4 | img = np.expand dims(im, axis=0) |
| 5 | out= model final.predict(img) |
| 6 |
| 7 | #显示预测结果 |
| 8 | if out[o]ToT > out[o][11: |
| 9 | print("有飞机"） |
| 10 | else: |
| 11 | print("没有飞机"） |

执行结果：如图8.27所示，图片有检测到飞机

执行结果：如图 8.28所 $\overline{{\Pi}}$ ，这张图片有检测到飞机，但其他部分图片并没有正确检测到。

$$
x-x+\Omega
$$

![](figures/456-2-FIGURE.jpg)

 $8. 2 7$ 选图测试结果冬

）测试所有文件中名为 $4$ 开头的文件 (16)

| 1 | 测试所有文件名为4开兴的文件 |
| 2 3 4 5 6 7 8 9 | =0 or e,i in enumerate(os.listdir(path)): if i.startswith("4"): Z+=1 img = cv2.imread(os.path.join(path,i)) #区域推荐 ss.setBaseImage(img) ss.switchToSelectiveSearchFast()  |
| 12 |  |
| LL 12 | 州日标检源 |
| 美 14 | T for p.recult in enumerate(csrecultc). |
| 一 16 | xv.w.h =result |
| 17 | 孑 timage= imout[v:v+h.x:x+wT |
| 18 | QL resized = cv2.resize(timage, (224,224), interpolation = cv2.INTER AREA) |
| 19 | 龙┅—口孑外孑≠孑白厂C img = np.expand dims(resized. axis=o)  |
| 20 | 口衣致致一蕲 out= model final.predict(img)  |
| 21 | 一 |
| 22 | #概率>0.65 才皇检测到飞机 |
| 23 | if out[0][01 > 0.65: |
| 24 | cv2.rectangle(imout, (x,y),(xtw, yth),(O,255,0),1,CV2.LINE AA |
| 26 | plt.imshow(imout) |

笔者并没有找到原发明人Ross $\mathbf{B}$ . Girshick的程序代码，故以上的范例并未使用Non-Maximum Suppression（NMS)
Bounding-box Regression，应该是作者之后又提出更好的算法 Faster R-CNN，所以R-CNN程序代码就被取代 $\overline{{\j}}$ ，后面我们会针对Faster R-CNN 再进行测试

(1）每张图经由区域推荐处理过后，各会产生出2000个候选框，然后每个框都需经过辨识，运行时间还是过长，而且区域推荐也不具备自我学习能力。

(2）接着再透过CNN模型提取4096个特征向量，合计有 2000x4096 = 8 192000个特征向量，内存消耗也很大

(3）每批数据都要经过 CNN、SVM、回归三个模型的训练与预测，过于复杂。

总体而论，目标检测不只追求准确率高，更要求能够实时检测，如自动驾驶汽车，不能等撞到障碍物后才侦测到。原作者虽然 $\biguplus$ Caffe $( \mathrm{C++} )$ 开发 $\mathrm{R-C N N}$ ，企图缩短侦测时间，但仍需要 $4 0$ 

![](figures/457-5-FIGURE.jpg)

8.28、测试所有文件图

R-CNN依然不尽理想的原因如下多秒才能侦测一张图像，因此引发 $7-$ 波算法的改良浪潮，参阅图 $8. 1 7_{\mathrm{c}}$ 

接下来，我们就来介绍各种改良算法的发想

首先 Kaiming He等学者提出 SPP-Net(Spatial Pyramid Pooing in Deep Convolutional Networks for Visual Recognition) 算法，针对R-CNN把每个候选框都视为单一图像并需经过辨识的缺点进行改良，作法如下。

$$
( 1 ) \ \mathrm{R-C N N}-\uparrow\pi\mp1 \pm\i\hbar\b\# \b)
$$
 $- \uparrow\mathrm{C N N}$ 模型，而
就一用掉
SPP-Net则是一张图的全部候选框都只用一个CNN。作者所提出的Spatia $\mathrm{P y}$ ramid Pooling(SPP）概念是不管图像尺寸大 $\rfloor$ ， $\overrightarrow{\mathrm{E}}$ 都能产生一个固定长度的输出，因 $\operatorname{l o b c}$ ，作者在最后一个卷积层上增加了一个SPP 层，这样就能接上一个可输入固定大小维度的
Dense.

## 8-5 R-CNN 改良

$$
( 2 ) \geq\pi\sharp\hbar\linebreak\hbar\ddag\hbar\ddag\equiv\mathrm{~ R-C N N-\hbar\neq\hbar~}
$$

R-CNN 与 SPP-Net的模型结构比较如图8.29所示

![](figures/459-5-FIGURE.jpg)

图 $8. 2 9$ R-ONN 与 SPP-Net的模型结构比较

SPP还是有以下缺点。 (1）虽然解决了CNN计算过多的状 $\gg_{\Pi}$ ，但没有支持向量机与回归过慢的问题。

详细处理流程可参阅Spatial Pyramid Pooling in Deep
Convolutional Networks for Visual Recognitiorlil一文，中文说明可参阅「SPP-Net论文详解」1例的内容。

接着Ross $\mathbf{B}$ . Girshick接续提出了Fast R-CNN、Faster R-CNN等算法

(1）将原始图像直接经白CNN转成特征向量，不用再借白个别候选框转换。

(2）透过候选框与原始图像的对照关系，换算出每个候选框的特征向量。

(2）特征向量大量占用内存空间。

 $\mathrm{F a s t \, R-C N N}$ 做法如 $\mathrm{F}$ 。
O

(3）之后的流程与 R-CNN一样。

Fast R-CNN模型结构如图8.30 所示

![](figures/460-9-FIGURE.jpg)

图 8.30 Fast R-CNN模型结构

(2）透过ROl pooling得到固定尺寸的特征后，只要连接一个 Dense 进行分类即可

(2）Ross B. Girshick决定放弃使用 selecive search，引进 $\overline{{\jmath}}$ RPN（Region Proposal Network）神经层，开发Faster R-CNN 模型，在训练的阶段挑选 $9$ 个尺寸的框，而这 $9$ 个框称为Anchor Box（见图8.31 ，然后再利用滑动窗口找出要比对的候选框。

$$
\ddag\ddag\Re\ddag\hbar\hbar\pi_{\circ}
$$

$$
( 1 ) \ \ \mathrm{C N N}
$$
模型只需训练原图就好，不用训练2000个候选
框。

其缺点如下。

1）用区域推荐算法找2000张候选框，耗时太久

![](figures/461-6-FIGURE.jpg)

$$
\boxed{\not\S} 8. 3 1 \quad\mathrm{A n c h o r} \not\Xi
$$

Faster R-CNN模型结构如图8.32所示

虽然Ross B. Girshick在 GitHub 放上了 Faster R-CNN 程序代码6，但安装不仅特别复杂，对执行环境的要求也很高
(Caffe/C++），所以建议大家直接使用Detectron库，目前已开发至第二版（Detectron 2），它使用 $\mathrm{P y}$ Torch框架，只能安装在 Linux/Mac环境，Windows 使用者可以在 Google Colaboratory上进行测试。

虽然Ross B. Girshick在 GitHub 放上了Faster R-CNN 程序代 $\mathbf{h} \mathbf{\Xi}^{[ 1 6 ]}$ ，但安装不仅特别复杂，对执行环境的要求也很高

请参阅程序：Detectron2 Tutorial.ipynb，用户需在 Google Colaboratory上执行，请上传程序至 Google 云端硬盘，接着再双击文件即可。记得要在选项「运行时间」选取GPU

(1）确认 $\mathrm{P y T o r c h} \,.$ gcc安装完成，且PyTorch 版本须为1. 或以上

![](figures/462-4-FIGURE.jpg)

 $8. 3 2$ Faster R-CNN 模型结构图

范例。使用Detectron2库进行目标检测

 $( 2 )$ 安装Detectron2库

(3) | 下载Detectron2 预先训练的模型。
自Model Zoo

(4）预测。程序代码如下：

执行结果：如图8.33所示，执行效果非常好，就连背景中旁观的人群都可以被正确检测。

(6）上传之前用ResNets0 检测结果失败的斑马照片，进行测试。程序代码如下

## (5）显示目标检测结果，如图8.33所示

![](figures/463-3-FIGURE.jpg)

冬8.33 显示目标检测结果

(7）读取文件，进行目标检测。程序代码如下：

#读取文件，进行目标检测
im = cv2.imread("./zebra. jpg") cv2_imshow(im)
predictor = DefaultPredictor(cfg) outputs predictor(im)
outputs

## 执行结果如下： 这个库功能很强，除了成功抓到所有对象之外，更是已经做到了实例分割（Instance Segmentation) 扫描到的对象不仅有框 (Bounding Box 还有准确的屏蔽（Mask)

$$
[ 4 6. 5 4 1 2, 9 4. 6 1 4 1, 2 3 4. 9 0 0 6, 2 5 8. 9 1 0 7 ],
$$

$$
[ 1 8 0. 8 2 4 5, 8 6. 8 5 0 8, 4 1 8. 6 1 4 2, 2 6 1. 7 7 4 0 ],
$$

$$
[ 3 4 2. 8 4 3 8, 1 0 3. 8 6 0 5, 5 6 3. 8 3 0 4, 2 6 6. 2 3 0 0 ]
$$

三个信
$$
{\bf\#} {\bf H} {\bf F} \colon{\bf[} 0. 9 9 9 2, ~ ~ 0. 9 9 8 6, ~ ~ 0. 9 9 8 3 {\bf]},
$$
概率都非常高

(s）显示目标检测结集。程序代码如下:

V = Visualizer(im[:, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2) $\begin{array} {l l l} {o u t} & {=} & {v.} \end{array}$ draw instance predictions(outputs["instances"].to("cpu"))
 $\operatorname{c v 2}$ imshow(out. get image()[:, ::-1])

：如图8.34斤示执行结果：

![](figures/464-8-FIGURE.jpg)

冬8.34 显示目标检测结果

文件后面还示范了以下功能。

(1）使用自定义的数据集，检测自己有兴趣的对象。在 Google Colaborato $\mathbf{r y}$ 上训练只需几分钟的时间就可以完成。 (3）全景视频的目标检测（笔者测试时有出现错误) O (2）人体骨架的检测。

白于R-CNN 属于两阶段（Two Stage）的算法，第一阶段先利用区域推荐找出侯选框，第二阶段才是进行目标检测，所以检测速度始终是一个瓶颈，难以满足实时检测的要求，后来有学者提出了一阶段（Single Shot）的算法，主要区分为两类：YOLO $\mathcal{R}$ 
SSD.

R-CNN 经过一连串的改良后，目标检测的速度比较如图8.35 所示。最新版速度比原版增快了250倍。

YOLO 发明人Joseph Redmon在 2016年的 CVPR研讨会 (You Only Look Once: Unified, Real-Time Object Detection） 中有两张幻灯片非常有意思：一辆轿车平均车身长约 $8$ 英 $\mathcal{R}$ ，假如使用Faster R-CNN 检测下一个路况的话，车子早已行驶了 $1 2$ 英 $\mathcal{R}$ , 也就是车子又开了1.5个车身的距离，相对地，如果使用YOLO检测 $\mathrm{F}$ 一个路况，车子则只行驶了 $2$ 英 $\mathcal{R}$ ，即 $1 / 4$ 个车身的距离，安全性是否会提高许多 $\ref{f i g : c c}$ 相信答案已不言而喻,非常有说服力，如 $8$ .36和图8.37所示。

## 8-6 YOLO 算法简介

|  | R-CNN | Fast R-CNN | Faster R-CNN |
| --- | --- | --- | --- |
| Test Time per Image | 50 Seconds | 2 Seconds | 0.2 Seconds |
| Speed U | lx | 25x | 250x |

图8.35 R-CNN各算法之目标检测的速度

YOLO （You Only Look Once）是现在最成熟的目标检测算法，于2016年由Joseph Redmon 提出，他本人开发至第三版，但因某些因素离开此研究领域，其他学者继续接手，直至2020年已开发到第五版了，如图8.38所示

| DPM v5 | Pascal 2007 mAP 33.7  | Speed | .07 FPS | 14 s/img |
| --- | --- | --- | --- | --- |
| R-CNN | 66.0 | .05 FPS | 20 s/img |
| FastR-CNN | 70.0 | .5FPS | 2 s/img |
| Faster R-CNN | 73.2 | 7 FPS | 140 ms/img |

![](figures/467-2-FIGURE.jpg)

图8.36 Faster R-CNN算法目标检测的速度

![](figures/467-4-FIGURE.jpg)

![](figures/467-5-FIGURE.jpg)

图8.37 YOLO算法目标检测的速度

V1:2016年5月·Joseph Redmon
You Only Look Once: Unified, Real-Time Obiect Detection V2:2017年12月，Joseph Redmon
YOLO9000: Better, Faster, Stronger
V3：2018年4月：Joseph Redmon
YOLOv3: An Incremental lmprovement
V4：2020年4月：Alexey Bochkovskiy
YOLOv4: Optimal Speed and Accuracy of Object Detection /5：2020年6月，Glenn Jocher
PyTorch based version of YOLOv5

YOLO各版本的平均准确度 $( \mathrm{m A P} )$ 与速度的比较如图8.39~ 图8.41所示。

(图片来源：YOLOv4: Optimal Speed and Accuracy of Object Detectiorl

图 8.38 YOLO 版本演进

![](figures/468-3-FIGURE.jpg)

图8.39 YOLO版本Vl-v3的比较

(图片来源：YOLO官网州

![](figures/468-6-FIGURE.jpg)

图8:40 YOL●版本 $\mathrm{V 4}$ 、v3的比较

YOLO的快速，部分是牺牲准确率所换来的，它的作法如下 (见图8.42)

(1）放弃区域推荐，以集群算法K-Means，从训练数据中找出最常见的 $\boldsymbol{\Lambda}$ 种尺寸的Anchor Box。

(2）直接将图像划分成 $( s, \, s )$ 个网格（Grid 每个网格 $\P$ 检查多种不同尺寸的 Anchor Box是否含有对象而已

(3)输 $\lambda$ CNN模型，计算每个Anchor Box含有对象的概率。

![](figures/469-4-FIGURE.jpg)

图8.41 YOLO版本v5的各模型比较

## (图片来源：YOLO5 GitHubl"

![](figures/469-7-FIGURE.jpg)

图8.42 YOLO的处理流程

(4）同时计算每一个网格可能含有各种对象的概率，假设每一网格最多只含一个对象。

YOLO为求速度快，程序代码采用 $\mathrm{C / C U D A}$ 开发，称为 Darknet架构，本书不剖析源代码，只聚焦下列重点：①环境搭建；②范例应用；③自定义数据集。

$$
( 5 ) \ \unni\# \# \exists\nexists\sharp\linebreak\mathbf{B} \linebreak\mathbf{B} \0 \linebreak\mathbf{B} \rbrack\6 \lines\mathbf{B} \rbrack\6 \lines\lines\mathbf{B} \rbrack\lines\lines\lines\lines\lines\lines\lines\lines\line4 \line\line4 \line\line4 \line\dag\bigoplus\C\line\mathbf{B} \rangle
$$
找出台格的候选框

(6）以NMS移除重叠BoundingBox

观察示意图8.43，有助于YOLO 的理解

![](figures/470-5-FIGURE.jpg)

图8.43 YOLO 处理流程的示意图

$$
( \not\S)+\not\S) / \bar{\pi} \colon\emph{Y o u \,} \, O n l y \, L o o k \, O n c e {:} \, U n i f i e d, \, D e t e c t i o n^{[ 2 0 ]} \, )
$$
. Real-Time Object

本书以YOLO v4为例来示范环境配置的步聚，其他版本也差不多。官网同时提供Linux与 Windows操作系统下的配置程序， 在Linux上用 GCG 编译程序配置比较简单，不过笔者习惯使用 Windows操作系统，因此，本文主要是介绍如何在 Windows系统下配置 Darknet.

我们先介绍比较简单的方式，再介绍官网所建议的方式，因为 Joseph Redmon 已经不再继续开发 $\overline{{\jmath}}$ ，后续有很多学者投入开发，所以YOLOv4 在GitHub上百花齐放，有非常多的版本，笔者就以Alexey Bochkovskiy的版本来说明配置步骤（见图 8.44)

）下载 OpenCV Windows版，要注 (https://opencv.org/releases/
意是 $\mathbf{C}$ 的版z $\propto, \ \pi\sharp\equiv\mathrm{O p e n C V-P y t}$ hon版本

(2）解压缩至CA或DA，以下假设安装在DA，如图8.45所示。

## 8-7 环境配置 YOLO

![](figures/471-5-FIGURE.jpg)

 $8$ 44 配置步骤图

(1） $\mathrm{O p e n C V}$ ：自OpenCV官网下载

htps:gihub.com/AexeyAB/darknet下载程序代码，并解压缩，以下假设安装在D:Adarknet-master，如图8.46 所示。

(4）安装Visual Studio 2017或 2019版本，以Visual Studlio 打开 DAdarknet-masterbuild darknetdarknet.sln 文件，就会出现升级窗口，单击「确定」按钮。提示：若无NVIDIA独立显卡，请改成开启darknet no gpu.sn.

![](figures/472-2-FIGURE.jpg)

 $8$ .45安装路径冬

## (3）下载YOLO4程序代码：自

![](figures/472-5-FIGURE.jpg)

图 $8$ .46 安装程序代码

(5）将项目组态（Configuration）改为x64（64位如图 8.47所示。

(6）修改项目属性，选择 $\Gamma\mathrm{V C}++$ 日录 >「Include目录加上：D:Aopencvbuildlinclude和
D:\opencvbuildtinclude\opencv2，如图 8.48和图 8.49所示。

(7）选择「连接器」>「输入 「其他相依性」，加上： D:openCVbuildx64vc1sliblopencv world430.ib，如图8.50和图 8.51所示

![](figures/473-3-FIGURE.jpg)

图 $8$ .47修改项目组态

![](figures/473-5-FIGURE.jpg)

 $8$ .48 编修目标1 图

![](figures/473-7-FIGURE.jpg)

 $8$ .49编修目标 $2$ 图

(8）在darknet 项目上右击，选择「重建」选项，若出现「创建成功」 即表示创建成功，执行文件放在D:darknet
master\build\darknetx64 日录 $\mathrm{T}$ 

(9）复制 D:openCAbuildx64vcl5liblopencv world430.lib 至 D:darknet-master buildtdarknetx64 目录下。留意若是使用 $\lor\mathrm{S}$ 201 $7$ ，目录应改为 $\mathrm{v c} 1 4_{\mathrm{c}}$ 

(10)依照htts:/github.com/AlexeyAB/darknet指 $\overline{{\Pi}}$ ，从 https://github.com/AlexeyAB/darknet/releases/download/darknet y olo $\mathbf{V 3}$ optimalyolov4.weights 下载 $\mathbf{y}$ olov4.weights，放入
D:darknet-master build darknetx64weights 目录中，如果不存在，可自行建立此目录。

![](figures/474-3-FIGURE.jpg)

图8.50编修其他相依性1

![](figures/474-5-FIGURE.jpg)

图 8.51编修其他相依性 $2$ 

11）执行下列指令进行测试：

dlarknet.exe detect .\ciglyolov4.cfg .weights\yolov4.weights .datadog.jp。执行结果：如图8.52所示，可以检测到自行车 (Bike）、狗（Dog) 货车（Truck) 盆栽植物（Potted plant) 概率分别为92 $\not\supset$ 、98 $\not\nabla_{O}$ 、92 $\not\sim$ 、33 $\not\nabla_{O}$ .

OD:\darknet-master\3rdpartyvothreadstbin'pthreacG2dll pthreadV02.dll

 $( 1 5 )$ 使用 $\mathrm{P y}$ thon调用yolo cp.ds.all，执行下列指令来测试：

![](figures/475-3-FIGURE.jpg)

 $8. 5 2$ 执行结果图

12）另外目录下还有许多.omd 脚本文件可以测试

(13）若要使用Python 直接调用Darknet $\mathrm{A P I}$ ，则需创建 yolo cop.dl.sin，项目属性不需任何修改。

 
(14）复制以下必要的函数库至当前目录（D·:darknet-masterbuild\darknetx64)下

$$
\widehat{2} \mathrm{D : l o p e n C V b u i l d b i n \Lambda^{*}. d I I_{o}}
$$

$$
\mathrm{\stackrel{~ ~ ~ \langle~ \stackrel{~ ~ ~ 3 ~} {~} D : ~ l o p e n C V b u i l d x 6 4 \sqrt{~ 5 ~ l i b l^{\star~}. l i b_{\circ} ~}}}
$$

Ddarknet.py 内含 performBatchDeteot国数，可一次测试多个文件。

笔者之前曾经正确执行过，目前应该是欠缺某些
 $\mathrm{y o l o}$ cpp dl.dll依赖的文件。我们会在下一节介绍TensorFlow使用YOLO权重文件，来排除此错误。

①修改darknet.vcxproj、yolo opp dllvcxproi，将所有的 CUDA 10.0 改为对应的版本，因为v10.0只支持 $\mathrm{V S} ~ 2 0 1 5$ 、VS2017 版本。

$$
\mathrm{p y t h o n \, d a r k n e t. p y}
$$

②执行结果会出现下列错误：

| rac |
| Fi |
| F1 |
| ile |

(16）使月C++调月用yolopp d.l可先编译 yolo console 然后再执行下列指令进行测试：
oll.sin,

yolo console dll.exe data/coco.names cig/yolov4.cfg weights/yolov4.weights dog.jpg

执行结果如下，包括对象名称/d框坐标和宽高/概率。

| bicycle O |
| dog - obi i |
| car - obji |
| truck 一 obj |

注意：假如使用 $\operatorname{V S} 2 0 1 9$ ，则须更改以下事项。

2复制「C:AProgram FilesNVIDIA GPU Computing
ToolkitICUDAv10.1\extrasvisual studio integration\MSBuildExten sions*.*」至 『C:Program Files (x86) Microsoft Visual
Studio\2019\CommunityMSBuidMicrosoftVC\v160\BuildCustomiz ations」目录内。

3修改项目属性，在 $\mathrm{C / C++>}$ 命令行，在其他选项加/FS，单击「套用」按钮，清除日项目后，再建置新的项目

④若出现「dropout layer kernels.ou rrcode 2」的错误， 则将「工具>选项>项目和方案 $\textgreater$ 建置」并执行菜单中的「平时项目组件的最大数目」项目修改为 $1_{\circ}$ 

第一种方法虽然很顺利地创建成功，但却在测试时发生了下列错误：

第二种方法较花时间，因为它会下载许多软件，包括NVIDIA SDK、FFMPEG等源代码，需重新创建，所以用户要耐心等候。 另外，官网介绍以下两种 Wincows版的配置方法。

(1）CMake：官网比较建议的方式。

(2）vcpkg：程序相对复杂

(1）前置作业须先安装以下软件

$$
\begin{array} {c} {\stackrel{\rightharpoonup} {2} \mathrm{V S} \, 2 0 1 7 \, \exists\sharp\, 2 0 1 9 :} \\ \end{array}
$$
须安装VCtoolset、English language
pack组件。

3NVIDIA CUDASDK：CUDA版本高于10.0，cuDNN 版本高于7.0。

(2）复制「C:AProgram FilesNVIDIA GPU Compuing
TookitCUDAlv10.1lextrasvisual studio integrationMSBuildExten sions*.* 至 『C:Program Files (x86) Microsoft Visual
Studio\2019\Community\MSBuildMicrosofVC\v160\BuildCustomiz ations」目录内。

(3）建 $\vec{\Sigma}-$ 个新目录，在「开始」菜单右击，开启Windows Powershell，执行以下指令：

$$
\mathbb{T O C M a k e} ~ ( \mathrm{h t t p s :} / \mathbb{I c m a k e. o r g / c}
$$
onload!) O

 $\bigoplus\mathrm{4 O p e n C V :}$ 版本须高于2.4

GGitforWindow:
$$
\mathrm{~ s ~ \ ( h t t p s : / / g i t-s c m. c o m / d o w n l o a d / w i n ) ~ \lrcorner~}
$$

$$
\mathrm{c d < \frac{i} {\hbar} \Pi\box\Pi\box\Pi>}
$$

git clone htps/gi.commicrosoivopkggit clone https://github.com/microsoft/vcpkg.

$$
\mathrm{c d \, v c p k g}
$$

S
$$
\S\! \mathrm{e n v \! :} \ \mathrm{V C P K G \_B O O T=\S P W D}
$$
.bootstrap-vcokg.bat

$$
\mathrm{\tt~ S e n v : ~ V C P K G \_~ R O O T=\S P W D}
$$

$$
\mathrm{. l b o o t s t r a p-v c p k g. b a t}
$$

.wopkg install darknet[opencv-base, cuda, cudnn]: x64-windows执行大约需要20 分钟

$$
\mathrm{p o w e r s h e l l-E x e c u t i o n P o l i c y}
$$
yBypass-Fie Abuild.ps1 执行大约
需要 $1 0$ 分钟

大功告成后，执行文件会放在darknetlbuild win release日录
自
https://gthub.com/AlexeyAB/darknet/releases/downloadidarknet y olo $\mathbf{V 3}$ optimalyolov4.weights 下载 ${\bf y}$ olov4.weights，放入
build $\mathrm{w i n}$ releaseweights 日录，执行下列程序

darknet.exe detect .cfgyolov
$$
4. \mathrm{c f g ~. ~ W e i g h t s} \mathrm{\, ~} \! \! \mathrm{l o v 4. w e i g h t s}
$$
..datadog.jpg

执行结果：如图8.53 所示，可以检测到自行车（Bike）、狗 (Dog）、货车（Truck） 盆栽植物（Potted plant) 概率分别为92 $\not\nabla_{O}$ 、98 $\not\nabla_{O}$ 、92 $\not\sim$ 、33%。

$$
\textbf{c d}...
$$

$$
\mathrm{g i t \, c l o n e \, h t t p s : / / g i t h u b. c o m / A l e x e}
$$
yAB/darknet

$$
\mathrm{c d ~ d a r k n e t}
$$

![](figures/480-0-FIGURE.jpg)

图8.3：检测执行结果

YOLO特别强调速度，因此，要实际应用于项日，应采用 C/C++创建辨识的模块，可依照darknet-
master\buildhdarknetyolo console dll.sln 方案来进行修改

日 $\mathrm{C / C++}$ 来开发程序，大部分的人可能都面有难色，所以
要使用
网络上有许多程序代码，让TensorFlow/Keras 也可以使用 YOLO 模型，主要的方式有以下两种。

(1）将YOL·权重文件转为TensorFlowKeras格式（.hs或 SaveModel)

tow to Perorm Objet Detecion With $Y O L O v 3 \: i n$ Keras'"l-文说明将YOL·权重文件转为 Keras格式文件（.hs）的步骤很简单，即用Keras 重建YOLO模型，并加载权重文件，变成完成训练的模型，之后再调用Save()即可

## 8-8 TensorFlow 实践 YOLO 模型以

## (2）直接使用YOLO权重文件

## 范例1。将YOLO权重文件转为Keras格式文件（.h5）

设计步骤如图8.34月所示。

![](figures/481-7-FIGURE.jpg)

 $8. 5 4$ 设计步骤图

请参阅程序：08 05 YOLO Keras Conversion.ipynb ## 1）加载相关库。程序代码如下

1#藏人库
2 import struct
3 import numpy as np
4 from tensorflow.keras.layers import Conv2D
5 from tensorflow.keras.layers import Input
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import LeakyReLU
8 from tensorflow.keras.layers import ZeroPadding2D
9 from tensorflow.keras.layers import UpSampling2D
10 from tensorflow.keras.layers import add, concatenate 11 from tensorflow.keras.models import Model

## (2）定义模型的卷积层。程序代码如下

![](figures/482-3-FIGURE.jpg)

(3）定义建立YOLOv3模型。程序代码如下：

![](figures/483-0-FIGURE.jpg)

(4）读取YOLO权重文件。程序代码如下：

(5）重建模型：结合模型结构和权重文件，并以Keras格式存盘。程序代码如下：

| 1 | #定义读取YOLO 权重文件的类别 |
| 2 C | mdjo, - Struct.umpdcKllywceautt)) 牛A 6C5 |
| 1 2 3 4 5 6 7 8 9 10 11 19 | #定义英取YOLO秋重文件的类别 class WeightReader: def init (self, weight file): with open(weight file, 'rb') as w f: major, = struct.unpack('i', w f.read(4)) minor, = struct.unpack('i', w f.read(4)) revision, = struct.unpack('i', w f.read(4)) if (major*10 + minor) >= 2 and major < 1000 and minor < 1000: w f.read(8) else: w f.read(4) +nAncRAA-An1000ANImInAn1000Y  |
| D 7 | mino, =struct.unpdcK(l,wT.red4)) 订8 |
| 0 | revision, FStruct.umpacK(-yw.fedu4) 上上限18101551000 |
| O a | Lmdjoiot mio- - dnd md o t ivoo cnd mliors ivve. WEAA0  |
| 2 10 |  w·IEdo A1  |
| ++ 12 |  WSA tnancnoca-maiany1000)ar minary 1000） |
| LL 12 | ciaspose-nmdjlogutmanriooo, hinAw-EA  |
| 12 1A |  uindy-wEdl A1CA50  |
| + 1C | cof11woichtc- nn framhuffan hinarw dtvno-'f1o2t22' |
| L2 16 | oc-rcl-wc-go-pmomoriciblicryy yPc-iioce |
| 美 17 | defraad hvtoclcalf-cizo. |
| 17 汽 | def read bytes(selt, size): |
| LO 19 | -OOLC return celf.all weichts[celf offset-sizo:self.offset1 |
| 一 20 | 京三中一三口二一二J |
| 20 汽 |  |
| Ll 22 | det load weignts(selt, mouel) foriinrange(106）.  |
| 23 | try: 82 |
| 24 | conv layer = model.get layer('conv ' + str(i)) |
| 25 n2 | print("loadnng weights ot convolution #r + str(l)) 3止：01654601  |
| 20 97 | 1T4n0c 1n[81,95,102° nAm1SwAR- mAd-1G+ 1SVAN/'hRAmILCtRIiY |
| 27 | norm layer - model.get layer("bnorm ' + str(i)) |
| 一 28 | BN- size=np.prod(norm laver.pet weiphts()rOl.shape) |
| 28 D0 | slze = np.prou(norm layer.get welgnts()io .snape) 上18481456  |
| 30 | gamma = self.read bytes(size) # scale |
| 一0 21 | Bama-srrtca ycooac/rocucc mean-celf nead hvtec(cizo#mean |
| 31 39 | mean = selt.read bytes(size) # mean 1844 |
| 32 | var- self.read bytes(size) # variance 话电9679 |
| 33 A |  weights = norm layer.set weights([gamma, beta, mean, van]) 上迎12  |
| 34 | if len(conv layer.get weights()) > 1: |
| 2L 32 33 34 35 36 37 38 |  medm-selt.ic yLesolze/ m wcu var- self.read bytes(size) # variance weights = norm layer.set weights([gamma, beta, mean, var]) if len(conv layer.get weights()) > 1: bias = self.read bytes(np.prod(conv layer.get weights()[1].shape)) kernel = self.read bytes(np.prod(conv layer.get weights()[O].shape)) kernel = kernel.reshape(list(reversed(conv layer.get weights()[O].shape) kernel = kernel.transpose([2,3,1,0])  |
| J2 36 | vido-ocrtrcau yceotp·pioulcov--cyci ·gcc welBicol/i- .oncpe , kornel -'celf read hvtec nn nrodlconw laver cot weichtc()ral chane)Y |
| 36 57 | kennel = selt.read bytes(np.prod(conv layer.get weights(Qlo].shape)) 上1151851158-6118 |
| 37 50 | kernel = kernel.reshape(list(reversed(conv layer.get weights()lo .shape)) 119407  |
| 20 20 | ASII--ASI-·CCPP55L---J/ rAnwlavon.cot- woichtc rLonnol hiac1 |
| 22 A0 | conv-dyeirogt we-buplLnenel, oldoJ, o1ce.  |
| 41 | 一一 kernels self.read bvtes(np.prod(conv laver.get weichts()Tol.shape)) |
| 42 | 公—-D-CDLJ kernel = kernel.reshape(list(reversed(conv laver.get weights()[ol.shape)) |
| 43 | kernel = kernel.transpose([2.3.1.01) 厂仙●诊纤纤厂——DL |
| 44 | conv laven.set weights([kernel)  |
| 45 |  一一 except ValueErron:  |
| 46 | print("no convolution #"+ str(i) |
| 47 | 〕 |
| 48 | def reset(self): |
| 49 | self.offset=0 |

1#建立模型
2 model = make yolov3 model(）
3#载人权重文件
4 weight reader = WeightReader('./YOLO weights/yolov3.weights') 5 weight reader.load weights(model)
6
7#转为TensorFlow/Keras格式文件
8 model.save("yolov3.h5')

## 范例2。测试Keras 格式文件（.h5) O (3）输出转换：使用NMS，移除重叠的Bounding Box。程序代码如下：

## 请参阅程序:08 06 YOLO Keras Test.ipynb

1）加载相关库。程序代码如下：

1#载人库
2 from yolo keras utils import *
3 from tensorflow.keras.layers import Conv2D
4 from tensorflow.keras.layers import Input
5 from tensorflow.keras.layers import BatchNormalization 6 from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.layers import ZeroPadding2D
8 from tensorflow.keras.layers import UpSampling2D
g from tensorflow.keras.layers import add, concatenate 10 from tensorflow.keras.models import Model

## (2）测试：预测的输出还须经过转换。程序代码如下：

![](figures/485-5-FIGURE.jpg)

执行结果：得到 $3$ 个对象信息如下

[ (1,13,13,
$$
2 5 5 ) \, \, \,, \, \, \, ( 1, 2 6, 2 6, 2 5 5 ) \, \, \,, \, \, \, ( 1, 5 2, 5 2, 2 5 5 )
$$

1#输出转换
2 #每个阵列内前两个值为grid宽/高，后四个为anchors 的坐标与尺寸
3 anchors = [[116,90, 156,198, 373,326], [30,61, 62,45, 59,119], [10,13, 16,30,33,23]] 4
5#设定对象检测的概率阈值
6 class threshold = 0.6
7
8#依 $a n c h o r s$ 的足寸及概率阈值筛选 $B o u n d i n g \ B o x$ 
9 boxes $= \mathsf{l i s t} ( \mathbb{)}$ 
lo for i in range(len(yhat)):
11 boxes t= decode netout(yhat[i][o], anchors[i], class threshold, input h, input w) 12
13#依原图、寸与缩放尺寸的比例，校正Bounding Box尽寸
l4 correct yolo boxes(boxes, image h, image w, input h, input w)
15
16 #使用non-maximal suppress，移除重叠的Bounding Box
17 do nms(boxes, 0.5)

(4）取得Bounding Box信息：坐标、类别、概率，并进行绘图。程序代码如下

请参阅程序：08 07 Tensorilow-Yolov4 Test.ioynb，此程序源自参考数据[22].

![](figures/486-2-FIGURE.jpg)

执行结果：如图8.55斤示

Bounding Box 个数：3
类别：zebra，概率：94.91060376167297 类别：zebra，概率：99.86329674720764 类别：zebra，概率：96.8708872795105

![](figures/486-5-FIGURE.jpg)

图 $8. 5 5$ 绘图结果

范例 $\textbf{3}$ .直接使用YOLO v4权重文件

(1）加载相关库。程序代码如下

1#载人库
2 import time
3 from absl import app, flags, logging
4 from absl.flags import FLAGS
5 import YOL04.core.utils as utils
6 from YOLO4.core.yolov4 import YOLOv4, YOLOv3, YOLOv3 tiny, decode 7 from PIL import Image
8 import cv2
9 import numpy as np
10 import tensorflow as tf

(2）设定YOLO模型参数。程序代码如下： ## (3）读取测诚影像：月Rite.jpg进行测试。程序代码如下：

![](figures/487-1-FIGURE.jpg)

执行结果：如图8.56所示

![](figures/487-3-FIGURE.jpg)

图 $8. 5 6$ 测试影像

(4）结合模型结构与权重文件，建立模型。程序代码如下： (5）预测并显示结果：成功标示出人和飞行伞。程序代码如下：

![](figures/488-1-FIGURE.jpg)

$$
\mathrm{F} \! :
$$

![](figures/488-3-FIGURE.jpg)

执行结果：如图8.57所示。

![](figures/488-5-FIGURE.jpg)

 $8. 5 7$ 预测并显示执行结果图

YOLO默认模型是采用COCO数据集’，共有80个类别，详见表 $8-1$ .

假若要检测的目标不在这 $8 0$ 三则需自行训练模型，大
类当中,
致步骤如图8.58所示

## 8-9 YOLO 模型训练

## 表8-1 COCO数据集

| 1 | 人 | 21 | 大象 | 41 | 红酒杯 | 61 | 餐桌 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 2 | 自行车 | 22 | 熊 | 42 | 杯子 | 62 | 厕所 |
| 3 | 汽车 | 23 | 斑马 | 43 | 叉子 | 63 | 电视 |
| 4 | 机车 | 24 | 长颈鹿 | 44 | 刀子 | 64 | 笔电 |
| 5 | 飞机 | 25 | 背包 | 45 | 汤匙 | 65 | 鼠标 |
| 6 | 公交车 | 26 | 伞 | 46 | 碗 | 66 | 遥控器 |
| 7 | 火车 | 27 | 手提包 | 47 | 香蕉 | 67 | 键盘 |
| 8 | 卡车 | 28 | 领带 | 48 | 苹果 | 68 | 手机 |
| 9 | 船 | 29 | 手提箱 | 49 | 三明治 | 69 | 微波炉 |
| 10 | 红绿灯 | 30 | 飞盘 | 50 | 橙子 | 70 | 烤箱 |
| 11 | 消防栓 | 31 | 双板滑雪板 | 51 | 花椰菜 | 71 | 烤面包机 |
| 12 | 停止标志 | 32 | 单板滑雪板 | 52 | 红萝卜 | 72 | 水槽 |
| 13 | 停车收费秒表 | 33 | 运动类用球 | 53 | 热狗 | 73 | 冰箱 |
| 14 | 长椅 | 34 | 风筝 | 54 | 披萨 | 74 | 书 |
| 15 | 鸟 | 35 | 棒球棒 | 55 | 甜甜圈 | 75 | 时钟 |
| 16 | 猫 | 36 | 棒球手套 | 56 | 蛋糕 | 76 | 花瓶 |
| 17 | 狗 | 37 | 滑板 | 57 | 椅子 | 77 | 剪刀 |
| 18 | 马 | 38 | 冲浪板 | 58 | 沙发 | 78 | 泰迪熊 |
| 19 | 羊 | 39 | 网球拍 | 59 | 植物盆栽 | 79 | 吹风机 |
| 20 | 牛 | 40 | 瓶子 | 60 | 床 | 80 | 牙刷 |

![](figures/489-5-FIGURE.jpg)

(1）准备数据集：若只是要测试处理影像，不想制作数据集的，则可以直接下载 COCO数据集，内含影像与标注文件
$$
\mathrm{( A n n o t a t i o n )}
$$
，接着遵循YOLO 步骤实践，但可能要训练好多
天，后续笔者使用 Open lmages Dataset，可以选择部分类别，缩短测试时间。

thtps://gihub.com/zutalin/labellmg) 产生YOLO格式的标注文件。Labellmg 安装步骤如下。Labellmg 标记工具如图 8.59所 $\overline{{\Pi}}$ 。

冬 $8. 5 8$ 自行训练模型步骤

(2）使用标记工具软件，Labellmg

$$
\mathbb{T} \mathrm{c o n d a ~ i n s t a l l ~ p y q t=5 ~_{o} ~}
$$

2conda install-c anaconda xmla

$$
\overline{{3}} \mathrm{p y r c c s-o ~ l i b s / r e s o u r c e s. p y ~ r e s}
$$
esources.arc.

$$
\mathbb{Q} \sharp\sharp\mathbb{I} \mathbf{J} \mathrm{~ L a b e l l m g : ~ p y t h o n ~ l a b e l l m g. p y_{\circ} ~}
$$

![](figures/490-8-FIGURE.jpg)

 $8. 5 9$ Labellmg 标记工具图

 (https:/gihub.com/AlexeyAB/darknethow-to-train-to-detect-your-custom-objects) 训练非常耗时，处理完成300张图 $\ H$ ，大约需要6小时。

范例.使用自定义数据集训练YOL·模型（内容参考Create your own dataset for YOLOv4 object detection in 5 minutes2il 及 $\mathbf{Y}$ OLO4 GitHub这两篇文章的做法)

(2）在ODv4 TooKt目录开启终端机 $( \mathrm{c m d} )$ ，并安装相关库：pio instal-r requirements.txt,

(htps://storage.googleapis.com/openimageswebindex.hitml) 下载训练数据，它包含350种类别可应用在实例分割（Instance Segmentation） $\pm$ ，我们只取三种类别来测试，避免训练太久。在 OIDv4 ToolKit日录，执行下列指令，下载训练数据：python
main.py downloader--classes Balloon Person Dog --type osv train --limit 200

(4）执行下列指令，下载测试数据：python main.py
downloader --classes Balloon Person Do --type -csv test --limim 200。

模型训练：参阅官网的教学步骤 (3)

(1）下载数据前置处理程序git clone
https
$$
1 / \mathrm{g i t h u b. c o m / t h e A l G u y s C o d e / O I D v 4 \_T H}
$$
okit.git.

(3）至 Open lmages Dataset 网站

注意：出现「missing 错误信息时，请输入 $\mathbf{y}$ .
filess

注意：出现「missing 错误信息时，请输入 $\mathbf{y}$ .
filess

(6）执行下列指令，产生YOLO标注文件，即每个图像文件都会有一个同名的标注文件（*.txt

类别ID>·标注框中心点 $\mathbf{X}$ 六 $> <$ 标注框中心点 $\mathsf{Y}$ 丛标、
丛标
标注框宽度>标注框高度 $>_{\circ}$ 

(7）移除OIDIDatasettirain、OIDDatasettest子目录下的 Label日录，包括以下日录。

(5）建立一个classes.x文件，内容如下 Balloon
Person
Dog

(5）建立一个classes.It文件，内容如下：

$$
\mathrm{B a l l o o n}
$$
$$
\mathrm{P e r s o n}
$$
Dog

$$
\mathrm{B a l l o o n}
$$

$$
\mathrm{P e r s o n}
$$

$$
\mathrm{D o g}
$$

$$
\mathrm{p y t h o n \, c o n v e r t \, a n n o t a t i o n s. p y}
$$

标注文件的内容为：

DOID\Datasettrain\BalloonLabel.
2OID\Datasettrain\Dog\Label. BOID\Datasettrain\PersonLabel.
GOID\DatasetltestBalloon\Label.
5OID\DatasettestnDog\Label. 6OID\Datasettest PersonLabelo

DOID\Datasettrain\Balloon\Labe

3OIDDatasettrainPerson\Label

$$
\mathbb{G O I D} \mathbb{D} \mathrm{a t a s e t \mathbb{V} t e s t \mathbb{P} e r s o n \mathbb{V} a b e l_{\circ}}
$$

(8）接着参考YOLO4 GitHub 的说明，切换到之前创建的 darknet-master\build darknetx64 日录

htos:/github.com/AlexeyAB/darknetreleases/download/darknet y olo $\mathbf{V 3}$ optimalyolov4.conv.137

(10）复制 ofgyolov4-ustom.ofg $\sharp\mathrm{y o l o-o b j. c f g}$ ，并将 $\mathrm{y o l o-}$ obi.cig 进行以下更改。

①修改第 $6$ 行的 batch-64，改为 $\mathrm{b a t c h=1 6}$ ，笔者GPU内存只有4GB，所以容易发生内存不足的错误，改为 $1 6$ 即可顺利执行，缺点是要花费更多的训练时间。

## (9）下载

②修改第 $7$ 行为 subdlvisons-16

③修改第 $2 0$ 行为「max batches $= 6 0 0 0 \rfloor$ 公式为类别数 (3）×2000=6000.

の修改第 $2 2$ 行为「steps-4800,5400」，为6000的 $8 0 \%$ 90%.

5修改第 $8$ 行为 width-416，即输入影像宽度

台修改第 $9$ 行为 eigh-416，即输入影像高度

D将Iyolo]段落的 classes-80改为 classes-3（第 $9 7 0$ 1058、1146行）

⑧将yolo]段落前一个[convolutional)的filtrs255改为 flters24(第 $9 6 3$ 、1051、1139列），公式为（类别数+5）×

(1）在 darknet-masterbuilcdarknet x64\datal目录建 $\b<-\uparrow$ obi.names文件，内密如 $\mathrm{T}$ :

(12）在 darknet-masterlbuilddarknetx64datal日录建 $\b<-\uparrow$ obj.data文件，内容如下：

(14）在 darknet-masterbuild darknet\x64\data\日录建 $\b<-\uparrow$ train.txt文件，内容如 $\mathrm{T}$ ，并将每个训练的图像文件按相对路径放入

$$
3=2 4_{\circ}
$$

$$
\begin{array} {c} {\mathrm{B a l l o o n}} \\ {\mathrm{P e r s o n}} \\ \end{array}
$$
Dog

 $\mathrm{c l a s s e s}=3$ 
train - data/train.txt
val $\textbf{i d}=$ data/test.txt
names - data/obj.names
$$
\mathrm{b a c k u p}=\mathrm{b a c k u p} /
$$

$$
\mathrm{c l a s s e s}=3
$$

(13）复制OID\Datasettrain、OID\Datasettest目录至 darknet-master build\darknetx64data\obj\日录下。

dabobitainalon006i571981a035jpg

(15）在 darknet-masterbuild\darknetx64datal日录建 $\exists-\uparrow$ test.txt文件，内容如 $\mathrm{F}$ ，将每个要训练的图像文件按相对路径放 $\lambda_{\circ}$ 

个在笔者的机器上大约执行 $\7 8 \jmath\mathrm{J n H J}$ ，如果读者要再自行标注影像的话，需有长期努力的准备

②若中途宕机，可指定backup目录下最大执行周期的文件继续执行训练：

3执行完成后，会产生backuplyolo-obi fina.weights 权重文件。

笔者写了一支程序gen arain.py，产生train.t文件：

$$
\mathrm{p y t h o n \, g e n \_t r a i n. p y}
$$

$$
\mathrm{d a t a / o b j / t e s t / B a l l o o n / 0 0 b 5 8 5 e 0 2}
$$
528755.jp9

执行程序gen train.py，产生tesi.xt文件：

$$
\mathrm{p y t h o n \, g e n \_t r a i n. p y \, t e s t}
$$

 $( 1 6 )$ 开启cmd，执行模型训练：

darknet.exe detector train data/obi.data yolo-obi.cfg yolov4.conv.137

darknet.exe detector train data/obi.data yolo-objofg backuplyolo-obj,5000.weights

④训练时会产生损失函数的变化，如图8.60所示。

输入D:1darknet
masterbuild\darknetx64\datalobltest\Balloon633dte8635d30dad. pg.

回执行效果不是很好，如图8.61所示。虽然有捕捉到人和气球，不过气球的概率（0.31）偏低，可能与第（10）步改成
 $\mathrm{b a t c h=1 6}$ 有关系，因此原作者建议值为 $6 4$ ，亦或者是
max batches - 6000 应该略微加大。

![](figures/496-2-FIGURE.jpg)

8.60 YOLO模型训练时损失函数的变化图

(17）自test目录下或网络任取一文件测试，执行下列指令

darknet.exe detector test data/obi.data $\mathbf{Y}$ olo-obi.cfg backupyolo-obj final.weights

![](figures/496-6-FIGURE.jpg)

以上只是笔者简单的实验，相关的配置文件放在
codeYOLO custom datasets 目录内，完整的目录文件过大， $\mathcal{F}_{\mathrm{U}}$ 法放 $\lambda$ ，请读者见谅。上述训练的步骤，在实际项目执行时，应该尚有一些改善空间，但最重要的还是要具有一台高性能的GPU机器，用金钱换时间。

图8.61 YOLO模型测试结果

与YOL0齐名，Single Shot MuliBox Detector（SSD）算法也属于一阶段的算法，在速度上比R-CNN 系列算法快，而在准确 ※（mAP）上比 YOLO $\mathrm{V 1}$ 高，后来YOLO 不断的升级改良，SSD 网络声量就变小了。几种算法的比较如图8.62所示

SSD比较特别的地方是它采用VGG 模型，并且在中间使用多个卷积层提取特征图（Feature map) 同时进行预测，如图8.63 所示。

详细说明可参阅「一文看尽目标检测算法SSD的核心架构与设计思想12

## 8-10 SSD算法

| System | VOC2007 test mAP | FPS (Titan X) | Number of Boxes | Input resolution |
| --- | --- | --- | --- | --- |
| Faster R-CNN (VGG16) | 73.2 | 7 | ~6000 | ~1000× 600 |
| YOLO(customized) | 63.4 | 45 | 98 | 448×448 |
| SSD300*(VGG16) | 77.2 | 46 | 8732 | 300×300 |
| SSD512*(VGG16) | 79.8 | 19 | 24564 | 512 ×512 |

8.62 R-CNN、YOLO、 比较表冬 SSD

## 数据来源:SSD官网

![](figures/498-7-FIGURE.jpg)

图8.63 YOLO模型和SSD

SSD也是用Caffe 架构开发的，SSD官网3并未说明在 Windows操作系统下要如何编译，不过，TensorFlow官方所提供的 TensorFlow Object Detection API内含 SSD 模型，可直接使用。

由于目标检测应用广泛，TensorFlow 与 PyTorch框架都特别提供了 $\mathrm{A P I}$ ，可直接呼叫相关模型，而TensorFlow Object
Detection AP 就是TensorFlow所支持的版本

## 8-11 TensorFlow Object Detection AP

API包含各式的算法，主要有CenterNet、EficientDet、 SSD、Faster R-CNN，具体如图8.64所示。

| Model name | Speed (ms) | COCOmAP | Outputs |
| --- | --- | --- | --- |
| CenterNet HourGlass104 512x512 | 70 | 41.9 | Boxes |
| CenterNet HourGlass104 Keypoints 512x512 | 76 | 40.0/61.4 | Boxes/Keypoints |
| CenterNet HourGlass104 1024x1024 | 197 | 44.5 | Boxes |
| CenterNet HourGlass104 Keypoints 1024x1024 | 211 | 42.8/64.5 | Boxes/Keypoints |
| CenterNet Resnet5O V1 FPN 512x512 | 27 | 31.2 | Boxes |
| CenterNet Resnet50 V1 FPN Keypoints 512x512 | 30 | 29.3/50.7 | Boxes/Keypoints |
| CenterNet Resnet101 V1 FPN 512x512 | 34 | 34.2 | Boxes |
| CenterNet Resnet50 V2 512x512 | 27 | 29.5 | Boxes |
| CenterNet Resnet50 V2 Keypoints 512x512 | 30 | 27.6/48.2 | Boxes/Keypoints |
| CenterNet MobileNetV2 FPN 512x512 | 6 | 23.4 | Boxes |
| CenterNet MobileNetV2 FPN Keypoints 512x512 | 6 | 41.7 | Keypoints |
| EfficientDet D0 512x512 | 39 | 33.6 | Boxes |
| EfficientDet D1 640x640 | 54 | 38.4 | Boxes |
| EfficientDet D2 768x768 | 67 | 41.8 | Boxes |
| EfficientDet D3 896x896 | 95 | 45.4 | Boxes |
| EfficientDet D4 1024x1024 | 133 | 48.5 | Boxes |
| EfficientDet D5 1280x1280 | 222 | 49.7 | Boxes |
| -GAG FfficientDet D6 1280x1280 | 5- 268 | 505 | WCA Boxes |
| EfficientDet D6 1280x1280 | 268 | 50.5 | Boxes |
| cicIentDetD/ 155ox1530 CCn-LI-N：0590590 | 325 10 | 31.4 509 | boxes D |
| SSD MobileNet v2 320x320 | 19 | 20.2 | Boxes |
| WVCICNGLG-CA CCD MAhil-Net V1 EDN SAOvSAO | A8 | 5.- 201 | DoAco Bavec |
| OYDICNCLYNUtAOT CCDMAHI-N-tVD EDNIit-290v220 | →0 22 | 5. 225 | DoAco BAvae |
| SSD MobileNet V2 FPNLite 320x320 | 22 | 22.2 | Boxes |
| W话CINCLGLCYCAGY CCD MAhil-Net V FPNlite SAOv6AO | 5- 20 | 56.- 282 | oAo Bavac |
| -YA CCD RecNet5O V1 EPN 64Ov640 (RetinsNet50) | A6 | 5 343 | WvAv Bavec |
| orncNcNtoAotcriaiyco CCD PecN-t5O V1 EDN 102Av102/ /P-tinsNet50) | T 87 | T 283 | DoAco Ravec |
| SSD ResNet101 V1 FPN 640x640 (RetinaNet101) | 57 | 35.6 | Boxes |
| SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101) | 104 | 39.5 | Boxes |
| SSD ResNet152 V1 FPN 640x640 (RetinaNet152 | 80 | 35.4 | Boxes |
| SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152) | 111 | 39.6 | Boxes |
| Faster R-CNN ResNet5O V1 1024x1024 | 65 | 31.0 | Boxes |
| Faster R-CNN ResNet5O V1 800x1333 | 65 | 31.6 | Boxes |
| Faster R-CNN ResNet101 V1 640x640 | 55 | 31.8 | Boxes |
| Faster R-CNN ResNet101 V1 1024x1024 | 72 | 37.1 | Boxes |
| Faster R-CNN ResNet101 V1 800x1333 | 77 | 36.6 | Boxes |
| Faster R-CNN ResNet152 V1 640x640 | 64 | 32.4 | Boxes |
| Faster R-CNN ResNet152 V1 1024x1024 | 85 | 37.6 | Boxes |
| Faster R-CNN ResNet152 V1 800x1333 | 101 | 37.4 | Boxes |
| Faster R-CNN Inception ResNet V2 640x640 | 206 | 37.7 | Boxes |
| Faster R-CNN Inception ResNet V2 1024x1024 | 236 | 38.7 | Boxes |
| Mask R-CNN Inception ResNet V2 1024x1024 | 301 | 39.0/34.6 | Boxes/Masks |
| ExtremeNet |  |  | Boxes |

图 8.64 TensorFlow Object Detection APl所支持的算法

安装环境需求请参考「Tensorflow Object Detecion API官网文件」12，目前如图8.65所示。

下面介绍APl在 Windows作业环境下的安装，步骤如图8.66 所示。

(3）安装GUDA：GPU不一定需要，如果要使月GPU，则需安装 CUDA 10.1/cuDNN v7.6.5，详细说明请参考「TensorFlow 数据来源：TensorFlow Detection Mode Z0P3

| Target Software versions | OS | Windows, Linux |
| --- | --- | --- |
| Python | 3.8 |
| TensorFlow | 2.2.0 |
| CUDA Toolkit | 10.1 |
| CuDNN | 7.6.5 |
| Anaconda | Python 3.7 (Optional) |

图 8.65 TensorFlow Objec Detection APl的安装环境需求

![](figures/502-6-FIGURE.jpg)

图8.66 API在Wndows环境下的安装步骤

 $( 1 )$ 安装 $\mathbf{A}$ naconda

(2）安装TensorFlow：需安装v2.2以上版本建立TensorFlow目录：在任意目录下建立 TensorFlow (4)
目录。

(5）下载模型：自TensorFlow Models GitHub
(https://github.com/tensorlow/models）下载整个项目
(Repository) 并解压缩至TensorFlow 次目录 $\mathrm{T}$ ，将models master日录改名为models。

htts:/github.com/protocolbuffers/protobuf/releases 下载最新版 protoc-.x.0-win64.zip，解压缩至特定目录，例如「CAProgram Files\Google Protobuf」 将其下bin 路径加到环境变量 Path中。

$$
\mathrm{g i t+h t t p s : / / g i t h u b. c o m / p h i l} \, \mathrm{t h o n A P l}
$$
terigreicocasigisubdirectory-Py

Object Detecion APl教学网站」，

## (6）下载pPocol Bufers自

(7）安装Protobuf：在第（4）步骤的
TensorFlowmodels\research 日录开启cmd，执行：

(7）安装Protobuf：在第（4）步骤的

Tensorlowmodelsresearch目录开启 omd，执行：

protoc object d
$$
\mathrm{e t e c t i o n} / \mathrm{p r o t o s} / \mathrm{^*. p r o t o-p y t h o n}_{-o u t=.}
$$

(8）安装执行下列指令：
COCO API

$$
\mathrm{p i p ~ i n s t a l l ~ c y t h o n}
$$

$$
\mathrm{p i p ~ i n s t a l l}
$$

(9）安装 Object Detection API：更改目前目录至 Tensorflow models research，复制

接下来，我们实践一个简单的范例调用TensorFlow Objec Detection AP，教学网站的范例写得有点复杂，笔者把一些函数拿掉，力求尽量简化程序。

范例1。使用TensorFlow Object Detection AP!进行目标检测。

object detectionpackages/it2/setup.py 至目前目录，执行 python-m oip install

bject 至目前目录，执行： delection/packages/t2/setup.py

$$
\mathrm{p y t h o n-m ~ p i p ~ i n s t a l l ~.}
$$

10）安装到此终于大功告成，执行测试指令

python object detection/builders/model builder t?2 test.py 测试结果如图 8.67所示。

![](figures/504-7-FIGURE.jpg)

8.67 TensorFlow Object Detection APl 测试成功信息冬

训练步骤如图8.68所示

(2）检测机器是否含有GPU：因速度比纯靠CPU快上数 $\imath\vec{\Xi}$ ，所以深度学习的模型训练非常仰赖GPU。一般而言，
TensorFlow 对内存的回收并不是很理想，时常会发生内存不足 (Out Of Memory, OOM）的状 $\gg\Pi$ ，所以我们通常会从这两种策略中择其一。

①设定成内存动态调整（
$$
\mathrm{D y n a m i c \, M e m o r y \, A l l o c a t i o n} )
$$
策
略：避免内存不足。程序代码如 $\mathrm{F}$ :

![](figures/505-2-FIGURE.jpg)

图 8.68训练步骤

请参阅程序：

08 08 Tensorflow Detection AP Test.ipynb
Object

(1）加载相关库。程序代码如下

| 1 | 2 载入库 |
| 2 | port os |
| 3 | port pathlib |
| 4 | port tensorflow as tf |
| 5 | port pathlib |

![](figures/505-8-FIGURE.jpg)

2设定成固定大小的内存，如2GB。程序代码如下

$$
\pi: \quad( 3 ) \quad\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\mp\pm\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\ -
$$
种模型，任选其中一种。程序代码如

执行结果：共花费160.80 秒，相当缓慢，后面（步骤（10） 会介绍另一种更快捷的方式。

$$
\mathrm{F} \! :
$$

| 1 | #下载模型，开解压纺 |
| 2 | def download model(model name, model date): |
| 3 | base url = 'http://download.tensorflow.org/models/object detection/tf2/ |
| 4 | model file = model name + '.tar.gz |
| 5 | #解乐缩 |
| 6 | model dir - tf.keras.utils.get file(fname=model name, |
| 7 | origin=base url t model date t '/' + model file, |
| 8 | untar=True) |
| 9 | return str(model dir) |
| 10 |  |
| 11 | MODEL DATE ='20200711" |
| 12 | MODEL NAME = 'centernet hg104 1024x1024 coco17 tpu-32" |
| 13 | PATH TO MODEL DIR = download model(MODEL NAME, MODEL DATE) |
| 14 | PATH TO MODEL DIR |

## (4）从下载的目录加载模型。程序代码如下

![](figures/506-5-FIGURE.jpg)

(5）下载Label文件。程序代码如下 ## 执行结果：文件会存储在C:AUsers<使用者登录账号：

kerasdatasetsmscoco label map.pbtxi

 $( 6 )$ 建立Label的对照表（代码与名称） O

## (7）任选一张图片进行目标检测。程序代码如下：

![](figures/507-4-FIGURE.jpg)

(8）目标加框：扫描Bounding Box，将图片的对象加框。程序代码如 $\mathrm{F}$ :

部分执行结果如下：

| 目标信息 | （候选框 | 展，类别， | 概率): |  |  |
| [0.2647 | 0.5269 | 0.6977 | 0.87491 | 24 | 98.77 |
| 0.2243 | 0.2899 | 0.6752 | 0.63571 I | 24 | 98.19 |
| [0.247 | 0.0723 | 0.6775 | 0.35461 I | 24 | 97.23 |
| 0.2958 | 0. | 0.4356 |  I 0.002111 | 24 | 3.25 |
| [0.2263 | 0. | 0.4017 | 0.0021 | 24 | 3.06 |
| [0.344 | 0.9967 | 0.478 | . ] | 24 | 2.85 |
| 0.3139 | 0.9975 | 0.6764 | . | 24 | 2.7 |
| [0.3315 | 0.9976 | .5939 | 0.99981 | 24 | 2.68 |
| [0.3326 | 0.9964 | 0.4505 | . ] | 16 | 2.3 |
| 10.2882 | 0.9967 | 0.4172 | . ] | 24 | 2.3 |

![](figures/508-3-FIGURE.jpg)

执行结果：不如预期，无法显示图 $\ H$ ，只能先存盘再显示

(9）显示处理过后的图片。程序代码如下：

![](figures/508-6-FIGURE.jpg)

执行结果：如图8.69所示

(10) ${\cal M}$ 下载的目录加载模型，执行速度大幅提
另一种方法：
升。程序代码如下：

执行结果：只花费0.5秒，与前一个方法相比执行速度明显提升。

![](figures/509-2-FIGURE.jpg)

 $8. 6 9$ 显示处理图 $\ H$ 图

![](figures/509-4-FIGURE.jpg)

（11）任选一张图片进行目标检测。程序代码如下： 范例 $2$ ，使用 Tensorflow Object Detection AP 进行视频测试。

![](figures/510-1-FIGURE.jpg)

（12）重复步骤（8） 执行结果相同
(9)

请参阅程序：

08 09 Tensorilow Object Detection APL Video.ipynba

由于前面 $1 0$ 个步骤均相同，因此这里只说明差异的部分。

(1）视频目标检测。程序代码如下：

(2）执行结果：影片中的车辆都可以检测到，辨识度极高， 也能够使用 Web Can，改为 $\mathrm{c v 2. V}$ ideoCapture (0) 0代表第一台摄影机。

![](figures/511-1-FIGURE.jpg)

使用另一段影片night.mp4进行测试，该影片来自「Python lmage Processing Cookbook GitHub  30，内容为高速 $\big<$ 路的夜景，辨识度也是相当好

如果要像YOLO 自定义数据集，检测其他对象的话，
TensorFlow Object Detection API的官网文体l有非常详尽的解 $\grave{\imath} \grave{\pi}$ ，读者可依指示自行测试。

目标检测的效果衡量指标是采平均精确度均值（mean
Average Precision, mAP) YOLO 官网展示的图表针对各种模型比较mAP，如图8.70所示

第 $4$ 章介绍过的 ROC/AUC效果衡量指标，是以预测概率为基准，计算各种阈值下的真阳率与伪阳率，以伪阳率为X轴，真阳率为 $\mathsf{Y}$ 轴，绘制出ROC曲线。而mAP也类似于 ROC/AUC,以IoU 为基准，计算各种阈值下的精确率（Precision）与召口率
(Recall），以召回率为 $\mathbf{X}$ 轴，精确率为 $\mathsf{Y}$ 轴，绘制出mAP 由线。

不过，目标检测模型通常是多分类，不是二分类，因此，采取计算各个种类的平均精确度，绘制后如图 $8 \!-\! 7 1$ 左图，通常会调整成右图的粗线，因为，在阈值低的精确率一定比阈值高的精确率更高，所以作此调整

## 8-12日标检测的效果衡量指标

![](figures/513-4-FIGURE.jpg)

图 $8. 7 0$ YOLO 与其他模型比较

## (图片来源：YOL·官网的

![](figures/514-0-FIGURE.jpg)

图 $8$ . $7 1$ mAP曲线

(左图是实际计算的结果，右图是调整后的结果

这一章我们认识了许多目标检测的算法，包括HOG、R-
CNN、YOLO、SSD，同时也实践了许多范例，像是传统的影像金字塔、R-CNN、PyTorch Detectron2、YOLO、TensorFlow Object Detection $\mathrm{A P I}$ ，还包含图像和视频检测，也可自定义数据集训练模型，证明我们的确有能力，将目标检测技术导入到项目中使用。

算法各有优劣，Faster R-CNN 虽然较慢，但准确度高，尽管 YOLO 早期为了提升执行速度牺牲了准确度，但经过几个版本升级 $\mathit{F}$ ，准确度也已大幅提高。所以建议读者在实际应用时，还是应该多方尝试，找出最适合的模型，如在边缘运算的领域使用轻量模型，不只要求辨识速度快，更要节省内存的使用。

现在许多学者开始研究动态目标检测，如姿态（Pose）侦测， 可用来辨识体育运动姿势是否标准，协助运动员提升成绩，另外还有手势检测和、体感游戏、制作皮影戏等，也很具有代表性。

## 8-13 总结除了目标检测之外，CNN还有许多影像方面的应用，例如： 个语义分割（Semantic segmentation）：②)风格转换 $( \mathrm{S t y l e}$ Transfer) 影像标题（Image Captioning 姿态辨识 (Pose Detection 或 Action Detection）；⑤生成对抗网络
 $( \mathrm{G A N} )$ 各式的应用；G深度伪造（Deep Fake)

本章将继续探讨这些应用领域，其中生成对抗网络的内容较多，会以专门章节来介绍。

## 第9 章

## 进阶的影像应用目标检测是以整个对象作为标记，而语义分割则以每个像素作为标记，区分对象涵盖的区域，如图9.1所示

经语义分割后如图9.2所示，各对象以不同颜色的像素表示

甚至更进一步，进行实例分割，相同类别的对象也以不同的颜色表 $\overline{{\Pi}}$ ，如图9.3所示。

## 9-1 语义分割介绍

![](figures/517-4-FIGURE.jpg)

图9.1，目标检测

![](figures/517-6-FIGURE.jpg)

图9.2，语义分割

(2）医疗诊断：断层扫描 $( \mathrm{C T} )$ 、核磁共振（MRI）的疾病区域标示。

语义分割的原理是先利用CNN进行特征提取，再运用提取的特征向量来重建影像，如图9.4所示

![](figures/518-2-FIGURE.jpg)

 $9. 3$ 实例分割冬

语义分割的应用非常广泛，举例如下

1）自动驾驶汽车的影像识别。

$$
( 3 ) ~ \b\supset\equiv\mathbb{R} \mathbb{H}_{\circ}
$$

(4）机器人的影像识别。

![](figures/518-8-FIGURE.jpg)

 $9. 4$ 语义分割的示意图图

(图片来源：SegNet: A Deep Convolutinal Encoder Decoder Architecture for tmage Segmentationi')

这种「原始影像→特征提取→重建影像」的做法，泛称为自动编码器（AutoEncoder，AE）架构，许多进阶的算法都以此架构为基础，因此比，我们先来探究 $\mathbf{A}$ utoEncoder 架构。

自动编码器透过特征提取得到训练数据的共同特征，一些噪声会被过滤掉，接着再依据特征向量重建影像，这样就可以达到「去噪声」（Denosing）的目的，此作法也可以扩展到语义分割、风格转换、U-net、生成对抗网络等各式各样的算法。

AutoEncoder E Encoder 与 Decoder组合而成，如图9.5所示。

接下来，我们实践 AutoEncoder，使用MNIST数据集，示范如何将噪声去除。

## 9-2 自动编码器

(1）编码器（Encoder) 即为提取特征的过程，类似于 CNN模型，但不含最后的分类层。

(2）译码器（Decoder）：根据提取的特征来重建影像。

![](figures/520-6-FIGURE.jpg)

图 $9. 5$ 自动编码器（AutoEncoder）示意图

范例1。实作 AutoEncoder，进行噪声去除。

9.6所示设计步骤如图

(3）取得MNIST训练数据，只取图像（X 不需要Label (Y) 因为程序只要进行特征提取，不用辨 $\grave{\imath} \P$ 。程序代码如下

![](figures/521-1-FIGURE.jpg)

冬9. $6$ 设计步骤

请参阅程序:09 01MNIST Autoencoder.ipynb

(1）加载相关库。程序代码如下

1#载人相关库
2 import numpy as np
3 import tensorflow as tf
4 import tensorflow.keras as K
5 import matplotlib.pyplot as plt
6 from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, UpSampling2D

## (2）超参数设定。程序代码如下

![](figures/521-7-FIGURE.jpg)

(4）在图像中加入噪声：以利于后续实验，观察 AutoEncoder是否能去除噪声。程序代码如下：

(6）建立译码器模型：使用卷积层和上采样层（Up-
sampling) 卷积层的输出个数与Encoder相反，代表把图像还原，上采样层与池化层相反，则是将图像放大。程序代码如下

![](figures/522-1-FIGURE.jpg)

(5）建立编码器模型：使用卷积层与池化层。程序代码如下:

$$
\mathrm{T} :
$$

![](figures/522-4-FIGURE.jpg)

(7）结合编码器、译码器，建立AutoEncoder模型。程序代码如下：

![](figures/523-1-FIGURE.jpg)

| 1 | I立 Autoencoder模型 |
| 2 | s Autoencoder(K.Model): |
| 3 | definit (self, filters): |
| 4 | super(Autoencoder, self). init (O |
| 5 | self.loss = [] |
| 6 | self.encoder = Encoder(filters) |
| 7 | self.decoder = Decoder(filters) |
| 8 |  |
| 9 | def call(self. input features): |
| 10 | #print(input features.shape) |
| 11 | encoded = self.encoder(input features) |
| 12 | #print encoded. shape) |
| 13 | reconstructed = self.decoder(encoded) |
| 14 | #print(reconstructed.shape) |
| 15 | return reconstructed |

## (8) 。程序代码如下训练模型。

| 1 model = Autoencoder(filters) |
| 2 |
| 8 model.compile(loss='binary crossentropy', optimizer='adam |
| 1 |
| loss = model.fit(x train noisy, 5  |
| 一一 x train, 5  |
| 7 validation data=(x test noisy, x test）. |
| epochs=max epochs, 3  |
| 0 batch size=batch size) |

(9）绘制损失函数。程序代码如下执行结果：如图9.7所示，损失随着训练次数越来越 $\imath\rfloor$ ，且趋于收敛。

(10）比较含噪声的图像与去除噪声后的图像。程序代码如下:

![](figures/524-2-FIGURE.jpg)

![](figures/524-3-FIGURE.jpg)

图9 $. 7$ 损失函数

$$
\mathrm{T} \! :
$$

![](figures/524-6-FIGURE.jpg)

执行结果：如图9.8所示，效果非常好，噪声被有效剔除

![](figures/524-8-FIGURE.jpg)

图 $9. 8$ 比较去除噪声前后图像

AutoEncoder属于非监督式学习算法，不需要标记
(（Labeling）。另外还有一个AutoEncoder的变形 $( \mathrm{V a r i a n t s} )$ 2 称为Variational AutoEncoders $( \mathrm{V A E} )$ ，数据编码不是一个输出常数，而是一个正态概率分布，译码时依据概率分布进行抽样，取得输 $\mathtt{H}$ ，利用此概念去除噪声会更稳健， $\mathrm{V A E}$ 常与生成对抗网络相提并论，可以用来生成影像，如图9.9所示。

注意：VAE的编码器输出不是特征向量，而是概率分布的母数 $\mu$ 和log（)

请参阅程序：09 02 MNIST VAE.ipynb，程序修改自keras-mnist-VAE GitHubl

![](figures/525-3-FIGURE.jpg)

冬 $9. 9$ Variational AutoEncoders $( \mathrm{V A E} )$ 的架构

范例 $2$ 。建立VA日模型，使用MNIST数据集，生成影像。 设计步骤如图9.10所示

范例 $2$ 。建立VAE模型，使用MNIST数据集，生成影像

设计步骤如图9.10所示

![](figures/525-8-FIGURE.jpg)

图9.10设计步骤

## (1）加载相关库。程序代码如下

1#载入相关库
2 import numpy as np
3 import matplotlib.pyplot as plt
import tensorflow as tf
5 from tensorflow.keras.datasets import mnist
6 from tensorflow.keras.layers import Input, Dense, Lambda 7 from tensorflow.keras.models import Model
8 from tensorflow.keras import backend as K
9 from scipy.stats import norm

## (2）取得MNIST训练数据。程序代码如下

1#取得 $M N I S T$ 训练数据
2 (x tr, y tr), (x te, y te) = mnist.load data()
3 xtr, x te = x tr.astype('float32')/255., x te.astype('float32')/255.
4 x tr, x te = x tr.reshape(x tr.shape $[ 0 ] \,, \;-1 )$ , x te.reshape(x te.shape[0], -1) 5 print(x tr.shape, x te.shape)

## (3）超参数设定。程序代码如下

## (4）定义编码器模型。程序代码如下

1# $e n c o d e r$ 
2 x = Input(shape=(x tr.shape[1:]))
3 x encoded = Dense(n hidden, activation='relu')(x)
4 x encoded = Dense(n hidden//2, activation="relu')(x encoded) 5
6 # encoder 后接 Dense，估算平均数 $m u$ 
7 mu = Dense(z dim)(x encoded)
8
9 # encoder 后接 Dense，估算 $l o g$ 变异数 $l o g \_v a r$ 
10 log_var = Dense(z_dim)(xencoded)

(5）定义抽样函数：根据平均数（mu）和Log变异数 (log var）取随机数。程序代码如下：

(7）以KL散度（Kullback-Leibler divergence, KL Loss）为损失函数（Loss) 2 MSE，主要用于衡量实际与理论概率
类似于
分布之差。程序代码如下：

## (6）定义译码器模型。程序代码如下

![](figures/527-2-FIGURE.jpg)

![](figures/527-3-FIGURE.jpg)

D编码器模型的汇总信息如下：

$$
\mathsf{M o d e l : \quad\operatorname{` ` m o d e l}^{n}}
$$

| Layer (type) | Output Shape | Param # | Connected to |
| --- | --- | --- | --- |
| input 1 (Inputlayer) | [(None, 784)] | 0 |  |
| dense (Dense) | (None,256) | 200960 | input 1[0][0] |
| dense 1（Dense) | (None,128) | 32896 | dense[0][0] |
| dense 2 (Dense) | (None,2) | 258 | dense 1[o][o] |
| dense 3(Dense) | (None,2) | 258 | dense 1[o][o] |
| lambda（Lambda) | (100,2) | 0 | dense 2[0][0] dense 3[o][0] |

## 2译码器模型的汇总信息如下

| dense 4（Dense) | (100，128) | 384 | lambda[0][0O] |
| --- | --- | --- | --- |
| dense_5(Dense) | (100，256) | 33024 | dense_4[0][0] |
| dense_6（Dense) | (100，784) | 201488 | dense 5[0][0] |
| tf.math.square (TFOpLambda) | (None，2) | 0 | dense_2[0][0] |
| tf.math.exp (TFOpLambda) | (None，2) | 0 | dense_3[0][0] |
| tf.operators_.add (TFOpLambd | (None，2) | 0 | tf.math.square[0][0] tf.math.exp[0][0] |
| tf.cast （TFOpLambda) | (None,784) | 0 | input 1[0][0] |
| tf.convert to tensor (TFOpLambd | (100，784) | 0 | dense 6[0][0] |
| tf.math.subtract (TFOpLambda) | (None，2) | 0 | tf.operators._.add[0][0] dense 3[0][0]  |
| tf.keras.backend.binary crossen | (100,784) | 0 | tf.cast[0][0] tf.convert to tensor[o][o] |
| tf.math.subtract 1 (TFOpLambda） | (None，2) | 0 | tf.math.subtract[o][0] |
| tf.math.reduce mean (TFOpLambda | (100,) | 0 | tf.keras.backend.binary crossentr |

## (8) 。程序代码如下训练模型。

| 1 | #7 训练模型 |
| --- | --- |
| 2 | vae.fit(x tr, |
| 3 | shuffle=True, |
| 4 | epochs=n epoch, |
| 5 | batch size=batch size, |
| 6 | validation data=(x te,N None）, verbose=1 |

## (9）取得编码器的输出。程序代码如下执行结果：如图9.11所示，显示0~9图像的分布，大致呈现分离，表示辨识度还不错

）测试数据预测：以译码器的输出来生成图像。程序代码 (12)
如下：

## (10）测试数据预测：以编码器的输出绘图。程序代码如下：

![](figures/529-3-FIGURE.jpg)

![](figures/529-4-FIGURE.jpg)

图9.11 显示0-9图像的分布

11）取得译码器的输出。程序代码如下

![](figures/529-7-FIGURE.jpg)

![](figures/530-0-FIGURE.jpg)

执行结果：如图9.12所示，生成的样本无噪声且正确。

![](figures/530-2-FIGURE.jpg)

 $9. 1 2$ 以译码器输出生成图像图

语义分割或称影像分割（Image Segmentation) 它将每个像素作为标记，为避免抽样造成像素信息遗关，模型不使用池化层 $( \mathrm{P o o l i n g} )$ 学者提出了许多算法。

 $( 1 ) \ \textup{S e g N e t}^{[ 2 ]},$ 全名为影像分割的Encoder-Decoder架构 (Deep Convolutional Encoder-Decoder Architecture for Image $\mathbf{S}$ egmentation) 使用反卷积放大特征向量，还原图像。

 $( 2 ) \textit{D e e p l a b}^{[ 3 ]}$ ：以卷积作用在多种尺寸的图像，得到 Score Map $\sqrt{\Xi}$ ，再利用Score Map 与 Conditional Random Field (CRF）算法，以内插法（Interpolate）的方式还原图像，如图 9.13 所示。详细处理流程可参阅原文。

(3）RefiNet l4：反卷积需要占用海量存储器，尤其是高分辨率的图像，所以RefiNet提出了一种节省内存的方法。

$$
( 4 ) \ \mathrm{P S P n e t}^{[ 5 ]}
$$
：使用多种尺寸的池化层，称为金字塔时尚
(Pyramid Fashion），金字塔掌握影像各个部分的图像数据，利

## 9-3 语义分割实践

![](figures/531-6-FIGURE.jpg)

冬 $9. 1 3$ DeepLab的处理流程

$$
( 5 ) \mathrm{\ u-N e t}^{[ 6 ]} :
$$
广泛应用于生物医学的影像分割，这个模型经
常被提到，且有许多的变形，所以，我们就来认识这个模型。

U-Net是AutoEncoder的变形，口于它的模型结构为U形而得名，如图 9.14所示。

传统 AutoEncoder的问题点发生在前半段的编码器，口于它提取特征的过程，会使输出的尺寸（Size）越变越小，接着译码器再透过这些变小的特征，重建出一个与原图同样大小的新图像，因此原图的很多信息，如前文所说的噪声，就没办法传递到译码器 $\overline{{\jmath}}$ 。 这个特点应用在去除噪声上是一分恰当的，但如果二标是要检测异常点（如检测黄斑部病变）的话，那就很不恰当 $\overline{{\j}}$ ，因为经过模型过滤后，异常点通通都不见 $\overline{{\jmath}}$ 。

用此金字塔还原图像。

![](figures/532-4-FIGURE.jpg)

图 $9$ .14U-Net模型

(图片来源：U-Net: Convolutional Networks for Biomedica lmage Segmentation o

所以，U-Net 在原有编码器与译码器的联系 $\L$ ，增加了一些链接，每一段编码器的输出都与其对面的译码器相连接，使得编码器每一层的信息，都会额外输入到一样尺寸的译码器，如图9.14中间横跨 $\mathbf{U}$ 形两侧的长箭头，这样在重建的过程中就不会遗失重要信息。

请参阅程序： 09 03 mage segmentation.ipynb，修改自 Keras 官网的范例7

(2）取得原图与目标图屏蔽（Mask）的文件路径，自以下网址下载数据

范例.以U-Net 实践语义分割。

9.15所示设计步骤如图

![](figures/533-5-FIGURE.jpg)

图 $9. 1 5$ 设计步骤

(1）加载相关库。程序代码如下

![](figures/533-8-FIGURE.jpg)

htp:/w.rboso.ac.uk-ugdatalpetsdaiannotations.tar.gz

①原图
htp:/ww.robots.ox.ac.uk/vgg/dataipetsdataimages.tar.gz.

$$
\oplus\mathtt{F} \boxplus\Xi\colon
$$

p:/www.robots.o.ac.uk-vgg/data/pets/datadimagestar.gz.

$$
\textcircled{2} \pm\pm\i\pm:
$$

## 程序代码如下：

![](figures/534-6-FIGURE.jpg)

执行结果：样本数：7390

）检查： 显示任一张图。程序代码如下： (3)

1#显示第10张图
2 print(input img paths[9])
3 display(Image(filename=input img paths[9]))
多#调整对始，将最深的颜色当作黑色(（），最法的颜色当作自(255
5 nintrstsdespsthdceon
img = PIL.ImageOps.autocontrast(load img(target img paths[9])) 8 display(img)

(4）建立lterator：lterator一次传回一批图像，不必一次全部加载至内存。程序代码如下：

②模型 output 设为 $4$ ：以程序中的 num classes参数设定， 一般 Fiter都设置为 $4$ 的倍数，作者利用后续的display mask函数取最大值，判断屏蔽的每一个像素是黑或是白。也有学者直接将其执行结果：原图和屏蔽图如图9.16所示

![](figures/535-3-FIGURE.jpg)

 $9. 1 6$ 原图和屏蔽图图

![](figures/535-5-FIGURE.jpg)

## (5）建立U-Net模型

$$
\mathrm{( {S e p a r a b l e C o n v 2 D} ~ ) \# [ {\frac{1 7} {2}} \equiv\Delta\pm( x ) \pm( x ) \pm\pi^{\prime} )}
$$
通道分别进行卷积。

## 设为1，详情可参阅Understanding Semantic Segmentation wit UNETl8.

$$
\star\Xi\equiv\pm\mp\exists\pm\pm\Gamma:
$$

![](figures/536-2-FIGURE.jpg)

## (6 。程序代码如下建立模型。

1#释放内存，以防执行多灾造成内存的占用
2 keras.backend.clear session()
3
4#建立模型
5 model = get model(img size, num classes) 6 model.summary()

(7）绘制模型结构：无法画出U形结构，绘图程序没有那么聪明，请参阅程序输出。程序代码如下

## (8）将数据切割为训练数据和验证数据。程序代码如下：

![](figures/537-2-FIGURE.jpg)

## (9) 。程序代码如下训练模型。

1 #设定优亿器(optimizer）、损失函数(Loss）、效果衡量指标(metrics）的类别
2 model.compile(optimizer="rmsprop", loss="sparse categorical crossentropy")
3
4 #设定检查点callbacks，模型文件
5 callbacks = [
7 ] keras.callbacks.ModelCheckpoint("oxford segmentation.h5", save best only=True) 6
8
9#训练15 周期(epoch）
10 epochs = 15
11 model.fit(train gen, epochs=epochs, validation data=val gen, callbacks=callbacks)

(10）预测并显示图像。程序代码如下：

![](figures/538-0-FIGURE.jpg)

执行结果：如图9.17所示，与图9.16右图比较，效果还不错。

![](figures/538-2-FIGURE.jpg)

图9.17预测并显示图像

(a）原因 (b）屏蔽图: (c）预测结果上一节的语义分割，同类别的对象只能够以相同颜色呈现，如要做到同类别的对象以不同颜色呈现，就会轮到实例分割
(Instance Segmentation）上场。

而实例分割所使用的 Mask R-CNN 算法系白Facebook A Research 在2018年所发表。Mask R-CNN为Faster R-CNN的延伸，不只会框住对象，更能产生屏蔽（Mask），如图9.18所 $\overline{{\Pi}}$ 。

(2）移除特殊的对象：将检测到的对象移除后，根据周遭的颜色填补移除的区域。如在观光景点拍照时，最困扰的就是有陌生 $\lambda-$ 起入镜，这时即可利用此技术将之移除，PhotoShop就提供了类似的功能。

## 9-4 实例分割

![](figures/539-4-FIGURE.jpg)

图9.18 MaskR-CNN模型

(图片来源：Mask R-CNN 9

除了辨识对象之外，实例分割还有以下延伸的应用

(1）去背：检测到对象后，将对象以外的背景全部去除。 我们直接来看一个实例，使用akTwelve Mask R-CNN 函数库 10，安装程序如 $\mathrm{F}$ 。

(1）自 htps://gihub.om/akTwelve/Mask RCNN 下载整个项目，并解压缩

htips:/github.com/matterport/Mask RCNN/releases/download/v2. 0/mask renn coco.hs.

(2）切换至该项目的根目录，执行下列指令

$$
\mathrm{p i p ~ i n s t a l l ~-r ~ r e q u i r e m e n t s. t x t}
$$

$$
\mathrm{p y t h o n \, s e t u p. p y \, i n s t a l l}
$$

(3）执行下列指令，确认安装成功：

 $\mathrm{p i p ~ s h o w}$ mask-ronn

如下画面即表示安装成功。

Name: mask-rcnn
Mersion: 2.1
Summary: Mask R-CNN for object detection and instance segmentation Home-page: https://github.com/matterport/Mask RCNN
Author: Matterport
Author-email: waleed.abdulla@gmail.com
License: MIT
Location: c:\anaconda3\liblsite-packages\mask rcnn-2.1-py3.8.egg Requires:
Required-by:

## (4）下载权重文件：

范例.,使用 Mask R-CNN进行实例分割。

设计步骤如图9.19所示 (3）定义检测的组态文件：Mask R-CN 须指定各项参数程序代码如下：

![](figures/541-1-FIGURE.jpg)

图 $9. 1 9$ 设计步骤

请参阅程序: 09 04 Mask R-CNN Test.ipynb，修改自 THow to Use Mask R-CNN in Keras for Object Detection in Photographs」所描述的例子中

1）加载相关库。程序代码如下

| 1 | 人相关库 |
| 2 | tensorflow.keras.preprocessing.image import load img |
| 3 | tensorflow.keras.preprocessing. image import img to array |
| 4 | mrcnn.config 天2 import Config |
| 5 | rt matplotlib.pyplot as plt |
| 6 | matplotlib.patches import Rectangle |
| 7 | mrcnn.model import MaskRCNN |

## (2）定义在图像上加框的函数。程序代码如下

| 1 | #定义函数，在图像加框 |
| 2 | def draw image with boxes(filename, boxes list): |
| 3 | #读取图文件 |
| 4 | data = plt.imread(filename) |
| 5 | #易示图像 |
| 6 | plt.imshow(data) |
| 7 |  |
| 8 | #加框 |
| 9 | ax= plt.gcaO |
| 10 | O for box in boxes list: |
| 11 | #左石坐标 |
| 12 | V1,x1,2,x2= box |
| 13 | #计草框的电高  |
| 14 | width, height=x2- x1,y2-y1 |
| 15 | #画框 |
| 16 | rect = Rectangle((xl, y1), width, height, fill=False, color='red') |
| 17 | 万诊豆豆褰—踮 ax.add patch(nect)  |
| 18 |  |
| 19 | #绘图 |
| 20 | plt.show() |

## (4）建立模型，加载权重，进行测试。程序代码如下

1#建立模型
2 rcnn = MaskRCNN(mode='inference', model dir='./', config=TestConfig()) 3
4#载人权重文件
5 rcnn.load weights('./MaskRCNN weights/mask rcnn coco.h5', by name=True) 6
7 $^\sharp$ 载人图片文件
8 img = load img('./images test/elephant.jpg')
9 img = img to array(img）# 影像转阵列
10
11 #预测
12 results = rcnn.detect([img], verbose=0)
13#加框、绘图
14 draw image with boxes('./images test/elephant.jpg', results[0]['rois'])

执行结果：如图9.20所示

![](figures/542-3-FIGURE.jpg)

图 $9$ 20 测试结果

(5）定义类别名称。程序代码如下执行结果：观察图9.21发现整条尾巴都没被屏蔽到，所以要达到精准去背，应该需要更多的数据与训练周期，甚至要改善算法才可能达到目的。

![](figures/543-1-FIGURE.jpg)

## (6）显示屏蔽。程序代码如下：

![](figures/543-3-FIGURE.jpg)

![](figures/543-4-FIGURE.jpg)

 $9$ 21显 $\overline{{\Pi}}$ 屏蔽图

(7）测试含多个对象的图片文件执行结果：如图9.22所示三只斑马均可被屏蔽

![](figures/544-1-FIGURE.jpg)

9.22 测试多个对象文件图

接着来认识另一个有趣的 AutoEncoder变形，称为风格转换 (Neural Style Transter），把一张照片转换成某一幅画的风格，如图9.23 所示。读者可以在手机下载Prisma $\mathrm{A p p}$ 来玩玩，它能够在拍照后，将照片风格实时转换，内建近二十种的大师画风可供选择，只是转换速度有点慢。

之前有一则关于美图影像实验室 $( \mathbf{M} \mathrm{T l a b} )$ 的新闻，「催生全球首 $\vec{\imath} \vec{\jmath}$ AI绘师Andy，美图抢攻人工智能却面临一大挑战」均，该 $\big< \lambda$ 司号称投资了约合1.99亿元人民币，研发团队超过60 $\mathcal{A}$ ，将风格转换速度缩短到了 $3$ 秒钟，开发出了美图秀秀 $\mathrm{A p p}$ ，大受欢迎， 之后更趁势推出专属手机，狂销100 多万台，算得上少数成功的 $\mathrm{A I}$ 商业模式。

风格转换算法中Leon $\mathbf{A}$ . Gatys等学者于2015年提出14，主要做法是重新定义损失函数，分为内容损失（Content Loss）与风格损失（Style Loss) 并利用AutoEncoder的译码器合成图像，随

## 9-5 风格转换一人人都可以是毕加索

![](figures/545-4-FIGURE.jpg)

图9.23风格转换（Slye Transfer

(原图＋风格图像一生成图像，图片来源：ast-style-transfer
GitHub l12]

(原图+ $\overline{{\chi}}$ 格图像一生成图像，图片来源：fast-styetransfer 着训练周期，损失逐渐变 $\imath\rfloor\omicron$ ，即生成的图像会越接近于原图与风格图的合成。

内容损失函数比较单纯，即原图与生成图像的像素差异平方和，定 $\grave{\chi}$ 为

式中： $n_{\mathrm{H}}$ 、 $n_{\mathrm{W}}$ 1 高： $n_{\mathrm{C}}$ 为色彩通道数； $\boldsymbol{a}^{~ ( C )}$ 
分别为原图的宽、了
为原图的像素；a为生成图像的像素

风格损失函数为该算法的重点，如何量化抽象的画风是一大挑战，Ga $\mathrm{t y s}$ 等学者想到的方法是，先定义 Gram 矩阵（Matrix) $\sqrt{\Xi}$ ，再利用Gram 矩阵来定义风格损失。

Gram Matrix：两个特征向量进行点积运算，代表特征的关联性，显现那些特征是同时出现的，即风格。因此，风格损失就是要最小化风格图像与生成图像的 Gram 差异平方和，即

上式只是单一神经层的风格损失，结合所有神经层的风格损失，定 $\AA$ 为

$$
J_{\mathrm{c o n t e n t}} ( C, G )=\frac{1} {4 \! \times\! n_{\mathrm{H}} \! \times\! n_{\mathrm{w}} \! \times\! n_{\mathrm{C}}} \sum( a^{( C )} \!-\! a^{( G )} )^{2}
$$

$$
J_{\mathrm{s t y l e}} ( S, G ) \!=\! {\frac{1} {4 \! \times\! n_{\mathrm{C}}^{2} \! \times\! \left( n_{\mathrm{H}} \! \times\! n_{\mathrm{w}} \right)^{2}}} \! \sum_{i=1}^{n_{\mathrm{c}}} \sum_{j=1}^{n_{\mathrm{c}}} ( G^{( S )}-G^{( G )} )^{2}
$$

式中： $\textbf{G}^{( S )}$ 为风格图像的 Gram； $G^{\emph{\Sigma}^{( G )}}$ 为生成图像的 Grama

$$
J_{\mathrm{s t y l e}} ( S, G )=\sum_{l} \lambda^{( l )} J_{\mathrm{s t y l e}}^{( l )} ( S, G )
$$

式中： $\lambda$ 为每一层的权重式中： $\alpha$ 、6为控制内容与风格的比重，可以控制生成图像要偏重风格的比例。

范例1。使用风格转换算法进行图文件的转换。提醒一下，范例中的内容图即是原图的意思。

请参阅程序：09 05 Neural Style Transfer.ipynb，修改自 TensorFlow 官网提供的范例「Neural Style Transter 157

$$
\boxed{\Xi}+\boxed{\Xi} \pm\boxed{\Xi} \parallel\emptyset:
$$

$$
J \left( \textbf{G} \right)=\! \alpha J_{\mathrm{c o m t e n t}} \left( \textbf{C}, \textbf{G} \right)=\! \beta\! J_{\mathrm{s t y l e}} \left( \textbf{S}, \textbf{G} \right)
$$

接下来，我们就来进行实践

设计步骤如图924所示

![](figures/547-7-FIGURE.jpg)

冬 $9. 2 4$ 设计步骤

1）加载相关库。程序代码如下 (2）载入内容图片文件。程序代码如下

![](figures/548-1-FIGURE.jpg)

执行结果：如图9.25所示。

![](figures/548-3-FIGURE.jpg)

图9.25、载入内容图片文件

(3）载入风格图片文件。程序代码如下：

1#载入风格图片文件
2 style path = "./style transfer/wave.jpg" 3 style image - load img(style path)
plt.imshow(style image)
5 plt.axis $( {\mathrm{" o f f}}^{1} )$ 
plt.show()

执行结果：如图9.26所示

![](figures/548-8-FIGURE.jpg)

图9.26载入风格图片文件

(4）定义图像前置处理的函数。程序代码如下

(7）定义模型：加载 $\mathsf{V}$ GG19模型，不含辨识层，加上自定义的输出。程序代码如下：

| 1 | #载人图像并进行前置处理 |
| 2 | def load and process img(path to img): |
| 3 | img = load img(path to img) |
| 4 | img = img to array(img)  |
| 5 | img = np.expand dims(img, axis=0) |
| 6 | img = tf.keras.applications.vgg19.preprocess input(img) |
| 7 | # print(img.shape) |
| 8 |  |
| 9 | #回传影像阵列 |
| 10 | return img |

## (5）定义由数组还原成图像的函数。程序代码如下

![](figures/549-3-FIGURE.jpg)

(6）定义内容图和风格图输出的神经层名称。程序代码如下：

## (6）定义内容图和风格图输出的神经层名称。程序代码如

$$
\mathrm{T} \! :
$$

![](figures/549-7-FIGURE.jpg)

(8）定义内容损失函数：为内容图与生成图特征向量之差的平方和，也可以采用上述理论的 $\parallel$ 式。程序代码如 $\mathrm{F}$ :

(9）定义风格损失函数：先定义Gram Matix计算函数，再定义风格损失函数。程序代码如下

![](figures/550-2-FIGURE.jpg)

![](figures/550-3-FIGURE.jpg)

![](figures/550-4-FIGURE.jpg)

10）定义图像的特征向量计算函数。程序代码如下

(12）定义所有层的损失计算函数：按照内容图与风格图的权重比例，计算总损失。程序代码如下

![](figures/551-1-FIGURE.jpg)

## 11）定义梯度计算函数。程序代码如下：

| #计算梯度 |
| --- |
| 2 def compute grads(cfg): |
| with tf.GradientTape() )as tape: |
| #累计损失 |
| all loss = compute loss(**cfg) |
|  |
| #取得梯度 |
| 3 total loss= all loss[o] |
| 1 # cfg['init image']:内容图影像阵列 |
| return tape.gradient(total loss, cfg['init image']), all loss |

![](figures/551-4-FIGURE.jpg)

(13）定义执行训练的函数：这是程序的核 $\grave{\imath} \grave{\mathrm{L}}$ ，在训练的过程一，在固定周期生成图像，可以看到图像逐渐转变的过程。程序代码如下：

| 1 2 9 | 执行训练的函数 mport IPython.display |
| --- | --- |
| 5 4 | ef run style transfer(content path, style path, num iterations=1000, |
| 5 6 | content weight-le3, style weight=le-2): 优取得模型  |
| 7 | model = get model() |
| 8 | 欢海会南数司区网边司行地公一6 |
| 9 10 | 丹成府告国农宏型设件工公形山 ctvofatuacAntant fatinac-aotfatuno nannacantatinnc madel cAntantnathctwlonath |
| L0 11 | styie-ieatunes, conteit-iedtuires - Bet-'eature- epiesemtdtions (mouel, conteit pat, ocyie pacn aram ctwlo foatinac - raram matnivlctvle featunel far ctvle foatine in ctvlo fostunacT  |
| 一一 12 | 8--55L5--7-— |
| 13 | #载人内容图 |
| 14 | init image = load and process img(content path) |
| 15 | init image = tf.Variable(init image, dtype=tf.float32) |
| 16 |  |
| L/ 1O | #店上孔10证 A++E+554m/151+551-0006：1-1611 |
| 19 |  |
| 20 4 | #初始化变重 上 |
| 21 | iter count= 1 |
| 一一 22 | 一— best loss, best img = float('inf'), None |
| 23 | 5-963 loss weights = (style weight, content weight) |
| 24 - | cfg={#组态 111 |
| 25 | 'model': model, |
| 26 | 'loss weights': loss weights, |
| 0 27 | 1055--80:058-83 'init image': init image.  |
| 一 28 | 一一—5 'gram style features': gram style features, |
| 一 29 | 石—2-—92 'content features': content features  |
| 81 |  |
| 32 33 | #参数设足 numrows：2#输出山图以列易示 |
| 34 | num cols=5#输出小图以5 行显示 A口07电 |
| 86 | — display interval = num iterations/(num rows*num cols) |
| 87 | start time = time.time() 厂一一一一 #计时  |
| 38 20 | global start = time.time() |
| 10 | #RGB二色中心值，输人图像以中心值为作转换 |
| 40 1 | #RGB二B中心值·期人图缘以中心值为作转类 5话109604177040901 |
| +1 10 | norm means - mp.drrdy([105.25, 110:779, 122.08]7 minwalc- -norm meanc  |
| tL 13 | mauvdls- -morm means max vals=255-norm means |
| 43 1A | max vals = 255 - norm means |
| 14 | 出T小 |
| 45 C | #术石训练 51 |
| F0 17 | 08-LJ for iin range(num iterations): |
| 4/ 10 | tor i in range(num iterations): nsdc5111Acc--AmntacnsdcI-fe |
| 48 19 | Braus, dllloss = compute grdustcrg) loss, style score, content score = all loss |
| 50 | 一一 opt.apply gradients([(grads, init image)]) |
| 52 63 | init image.assign(clipped) end time = time.time()  |
| 04 55 | #记录最小损失时的图像 |
| 05 56 | #地水取热大时化窗款 if loss < best loss: |
| 58 -A | 电小城 best img = deprocess img(init image.numpy()）#生成图像 |
| 59 50 | 社N公周斯教生成图像 |
| 50 51 | #N同别数士成康 if i%displav interval== o: |
| L 62 | lT-ouiSpidy mtervdl-u. start time= time.time() |
| 52 2 | start time - time.time() |
| 63 | A374 |
| 54 | #生成图象 |
| 56 | 上一口—D plot img = deprocess img(plot img) |
| 57 | imgs.append(plot img) |
| 8 |  |
| 9 | # IPython.display.clear output(wait-True） # 可清除之前的显示 |
| 0 | print(f'周期数：{i}'） |
| 1 | elapsed time - time.time() - start time |
| 2 | print(f'总损失：{loss:.2e}，风格损失：{style score:.2e},'+ |
| 3 | f'内容损失：{content score:.2e}，耗时：{elapsed time:.2f}s') |
| 4 | IPython.display.display png(Image.fromarray(plot img)) |
| 5 |  |

执行结果：执行500个周期，刚开始画百变化很大，之后转换逐渐减 $\varlimparallel$ ，表示损失函数逐步收敛，如图9.27所示。

![](figures/554-1-FIGURE.jpg)

## 14）调用上述函数，执行模型训练。程序代码如下：

1#执行训练
2 model = get model()
3 best, best loss = run style transfer(content path,
stylepath, num iterations=500)

周期数：0
总损失： $1. 7 0 e+0. 8$ ，风格损失： $1. 7 0 e+0. 8,$ 内容损失： $0. 0 0 e+0 0$ ，耗时： $8. 0 2 5$ 

![](figures/554-4-FIGURE.jpg)

周期数：50
总损失：9.72e+06，风格损失：7.52e+06,内容损失：2.21e+06，耗时：0.03

![](figures/554-6-FIGURE.jpg)

## 图9.27图文件风格转换

周期数：100
总损失：5.80e+06，风格损失：3.94e+06,内容损失：1.87e+06，耗时：0.035

![](figures/555-1-FIGURE.jpg)

周期数:200
总损失：2.71e+06，风格损失：1.42e+06,内容损失：1.30e+06，耗时：0.025

![](figures/555-3-FIGURE.jpg)

周期数:300
总损失： $1. 7 0 e+0 6$ ，风格损失：7.33e+05,内容损失： $9. 6 2 e+8 5$ ，耗时：0.02s

![](figures/555-5-FIGURE.jpg)

周期数：450
总损失：1.13e+06，风格损失：4.21e+05,内容损失：7.08e+05，耗时：0.03s

![](figures/555-7-FIGURE.jpg)

 $9. 2 7$ 图文件风格转换续图

总耗时： $2 4 7. 4 5 5$ 

![](figures/556-1-FIGURE.jpg)

续图 $9. 2 7$ 图文件风格转换

## （15）显示最佳的图像

![](figures/556-4-FIGURE.jpg)

1#显示最佳的图像
$$
2 : \mathrm{I m a g e. \, f r o m a r r a y ( b e s t )}
$$

执行结果：如图9.28所示

![](figures/556-7-FIGURE.jpg)

图9.28、显示最佳图像

 $( 1 6 )$ 比较内容图与生成图。程序代码如下

![](figures/557-0-FIGURE.jpg)

## (17）用另一张内容图来测试。程序代码如下：

1#以另一张图测试
2 content path = './style transfer/Green Sea Turtle grazing seagrass.jpg'
3 style path = './style transfer/wave.jpg'
4
5 best starry night, best loss = run style transfer(content path, style path) 6 show results(best starry night, content path, style path

执行结果： 9.29所示
如图

![](figures/557-4-FIGURE.jpg)

图9.29、另一张图执行结果

TensorFlow Hub 里面有许多预先训练好的模型可以直接套用，包括风格转换模型，下面的范例 $2$ 使用Fast Style Transter算法预先训练好的模型，可快速产生新图像，但该模型每次产生的图像均不相同。

范例 $2$ .使用TensorFlow Hub 的 Fast Style Transfer 预先训练模型完成风格转换。

请参阅程序：请参闭阅09 05 Neural Style Transter.ipynb的下半段。

(2）定义加载图像并进行前置处理的函数：与前一个范例的处理方式相同。程序代码如下：

设计步骤如图9.30所示

 $\begin{matrix} \pm\pm\lambda\left[ \lambda\right] \pm\left[ \pm\right] \times\\ \left[ \times\pm\right] \left[ \pm\right] ] \times\\ \left[ \times\pm\left[ \pm\right] \right] \end{matrix}$ 图像前置处理还原图像函数

自TensorFlowHub 生成图像还原图像下载压缩的模型数据数组

图 $9. 3 0$ 设计步骤

(1）下载图像：可从网络下载图像。程序代码如下：

![](figures/558-7-FIGURE.jpg)

![](figures/559-0-FIGURE.jpg)

## (3）定义显示图像的函数：与前一个范例的处理方式相同

## 程序代码如下：

![](figures/559-3-FIGURE.jpg)

## (4）加载图像并显示。程序代码如下：

![](figures/559-5-FIGURE.jpg)

## 执行结果： 9.31所示。 如图

![](figures/559-7-FIGURE.jpg)

$$
\boxed{H} \boxed{g}
$$

 $9. 3 1$ 加载像并显示冬

TensorFlow 白网提供的范例「Neural Stye Transfer」 15，提及了一些改善的措施。例如，基本的风格转换算法所生成的图像经 (5）定义还原图像的函数。程序代码如下

1#定义还原图像的函数
2 def tensor to image(tensor):
3 tensor = tensor*255
4 tensor = np.array(tensor, dtype=np.uint8) 5 if np.ndim(tensor)>3:
6 assert tensor.shape $[ \varnothing] \ =\pm\ 1$ 
7 tensor = tensor[o]
8 return Image.fromarray(tensor)

## (6）自 TensorFlow Hub下载压缩的模型。程序代码如下：

## (7）生成新的图像。程序代码如下

1 $^\sharp$ 生成图象
2 stylized image = hub model(tf.constant(content image), tf.constant(style image))[o] 3 tensor to image(stylized image)

执行结果：如图9.32所示

![](figures/560-7-FIGURE.jpg)

9.32风格转换结果

常会偏重高频（High Frequency) 而高频会造成边缘特别明显， 可使用正则化（Regularization）矫正损失函数，改善此现象。

上述程序生成一张图像需要用时290秒，在如今的社群媒体时代，就算这种酷炫的效果吸引了大众的眼球，也难以流行，因此在网络上有许多的研究，讨论如何提升算法速度，有兴趣的读者可搜导「Fast Style Transter」。另 $\flat\Gamma$ ，也有同学问到，如果用同一张风格图，对另一张新的内容图进行风格转换，也要重新训练吗？答案是不一定，这是一个值得研究的课题。开发美图秀秀的公司斥资近 $2$ 亿人民币，才将速度缩短至 $3$ 秒，可见技术难度颇高，所以，速度绝对是商业模式重要的考虑因素。

风格转换是一个非常有趣的应用，除了转换成名画风格之外， 也可以将照片卡通化，或是针对脸部美肌，凡此种种都值得一试。 当然，不只有风格转换算法可以实现，其他像GAN或 OpenCV图像处理也都能做到类似的功能，大家可以自行探讨。

脸部辨识（Facial Recognition）的应用面向非常广 $\grave{\imath} \overline{{\jmath}}$ ，国内厂
,
商不论是系统厂商、PC厂商、NAS厂商，甚至是电信业者，都已涉猎比领域，推出了各种五花 $\l\setminus$ 门的相关产品，已经有以下这些应用类型。

(1）智慧保全：结合门禁系统，运用在家庭、学校、员工宿舍、饭店、机场登机检查、出入境比对、黑名单/罪犯/失踪人口比对等方面。

(3）商店实时监控：实时辨识VIP和黑名单客户的进出，进行客户关怀、发送折扣码或记录停留时间，作为商品陈列与改善经营效果的参考依据。

(5）人流统计：针对有人数容量限制的公共场所，如百货公司、游乐园、体育场馆，通过脸部辨识，进行人数管控

(6）情绪分析：辨识脸部情绪，发生意外时能迅速通报救援或进行满意度调查

## 9-6 脸部辨识

 $( 2 )$ 考勤系统：上下班脸部刷卡取代卡片。

(4）快速结账：以脸部辨识取代刷卡结账

 $( 7 )$ 社群软件上传照片的辨识：标注朋友姓名等

依据技术类别可细分为以下类型

(1）脸部检测（Face Detection）：与目标检测类似，因此运用目标检测技术即可做到比功能，检测图像中有哪些脸部和其位置。

(2）脸部特征点检测（Facial Landmarks Detection 检测脸部的特征点，用来比对两张脸是否属于同一人

(3）脸部追踪（Face Tracking 在视频中追踪移动中的脸部，可辨识人移动的轨迹。

个脸部识别（Face dentification 从N个人中找出最相似的人。

②脸部验证（Face Verification 验证脸部是否相符，如出入境检查旅客是否与其护照上的大头照相符合。

(4）脸部辨识，分为以下两种

各项脸部辨识技术及支持的库如图9.33所示。

![](figures/563-7-FIGURE.jpg)

图9.3脸部辨识的技术类别与相关库支持

接下来，我们就逐一实作这些相关功能

OpenCV 使用Haar Cascades算法来进行各种对象的检测它会将各种对象的特征记录在 XML文件，称为级联分类器
(Cascade File），可在 OpenCV或 OpenCV-Python 安装目录内找到（haarcascade *.xml) ，笔者已把相关文件复制到范例程序目录 $\mathrm{F}$ 。

Haar Cascades 技术发展较早，辨识速度快，能够做到实时检测，缺点是准确度较差，容易造成伪阳性，即误认脸部特征。它的架构类似于卷积，如图9.34所示，以各种滤波器（Filters）扫描图像，像是眼部比脸颊暗、鼻梁比脸颊亮等。

## 9-6-1 脸部检测

![](figures/564-3-FIGURE.jpg)

3 9.34 Haar Cascades 以滤波器扫描图像

## 范例. 日 OpenCV 进行脸部检测使用

9.35 所示设计步骤如图

![](figures/564-7-FIGURE.jpg)

1）加载相关库，包含OpenCV-Python。程序代码如下：

(2）载入脸部的级联分类器（Face Cascade File 程序代码如下：

## 图9.35设计步骤

请参阅程序：09 06脸部检测_opencv.ipynb

![](figures/565-4-FIGURE.jpg)

1#藏人相关库
2 import cv2
3 from cv2 import CascadeClassifier from cv2 import rectangle
5 import matplotlib.pyplot as plt 6 from cv2 import imread

1#载人脸部级联分类器(face cascade file)
2 face cascade = './cascade files/haarcascade frontalface alt.xml 3 classifier = cv2.CascadeClassifier(face cascade)

(3）载入测试图文件。程序代码如下

1#载入图文件
2 image file = "./images face/teammates.jpg"
3 image = imread(image file)
4
5 # OpenCV预设为 $B G R$ 色桑 $-$ 转为 $R G B$ 色桑
6 im rgb = cv2.cvtColor(image, cv2.COLOR BGR2RGB) 7
8#显示图像
9 plt.imshow(im rgb)
10 plt.axis('off')
11 plt.show()

：如图9.36所示执行结果：

![](figures/565-10-FIGURE.jpg)

 $9. 3 6$ 载 $\lambda$ 测试图像冬

(4）检测脸部并显示图像。程序代码如下

![](figures/566-1-FIGURE.jpg)

执行结果：如图9.37所示，全部人的脸都被正确检测到了

![](figures/566-3-FIGURE.jpg)

图 $9$ .37检测险部

(5）载入另一图文件。程序代码如下：

1#载人图文件
2 image file = "./images face/classmates.jpg"
3 image = imread(image file)
4
5 # OpenCV 预设为 $B G R$ 色桑，转为 RGB 色桑
6 im rgb = cv2.cvtColor(image, cv2.COLOR BGR2RGB) 7
8#显示图像
9 plt.imshow(im rgb)
10 plt.axis('off')
11 plt.show()

执行结果：如9.38月所示

执行结果：如图9.39所示，就算图像中的脸部占据画面较大， 检测结果依然正确

![](figures/567-1-FIGURE.jpg)

图 $9$ .28载 $\lambda$ 图文件

）检测脸部并显示图像。程序代码如下： (6)

![](figures/567-4-FIGURE.jpg)

![](figures/567-5-FIGURE.jpg)

图 $9$ .39检测脸部

(7）同时载入眼睛与微笑的级联分类器。程序代码如下： 执行结果：如图9.40所示，左边人脸的眼睛少抓 $7-\uparrow$ ，嘴巴误抓好几个。这是笔者多次调整detectMutiScale参数后，所能得到的较佳结果。

## (8）检测脸部并显示图像。程序代码如下:

![](figures/568-2-FIGURE.jpg)

![](figures/568-3-FIGURE.jpg)

冬 $9$ .40 检测脸部

$$
( 1 ) ~ \mathrm{s c a l e F a c t o r} :
$$
设定每次扫描窗口缩小的尺寸比例，设定
较 $\rfloor\rfloor\eta$ 直，会检测到较多合格的窗口。

(2)minNeighbors：每一个被选中的窗口至少要有邻近且合格的窗口数，设定较大值，会让伪阳性降低，但会使伪阴性提高

$$
( 3 ) \ \mathrm{m i n S i z e :} \^{\prime\downarrow1 \mp i z} \, h \! h \circ
$$
个设定值，会被过滤掉，格式为（w,

$$
( 4 ) \ \mathrm{m a x S i z e :} \ \lambda\mp i h \, h ) \ \circ
$$
这个设定值，会被过滤掉，格式为（W,

deteciMlisale 相关参数的介绍如下 Haar Cascades技术发展较早，使用很简单，但是要能准确检测，必须根据图像的色泽、光线、对象大小来调整参数，并不容易。因此，近几年发展改用深度学习算法进行脸部检测，较知名的算法MTCNN系白 Kaipeng Zhang等学者于 2016年/oint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks发表。

MTCNN的架构是运用影像金字塔加上三个神经网络，如图 9.41所示，四个部分的功能如下。

(2）建议网络（Proposal Network or $\mathrm{P-N e t} )$ 类似区域推荐，找出候选的区域。

输出网络（Output Network or O-Net) 找出脸部特征 (4）

## 9-6-2 MTCNN 算法

(1）影像金字塔（Image Pyamid）：抽取不同尺寸的脸部

(3）强化网络（Refine Network or R-Net 找出合格框 (bounding boxes

乍一看，会不会觉得有些熟悉？其实 MTCNN的做法与目标检测算法Faster R-CNN类似。

原作者使用Caffe/C 开发"，许多人将以Python改写，安装指令为：pip install mtonn.

![](figures/571-2-FIGURE.jpg)

图9.41 MTCNN使用影像金字塔加上三个神经网络

范例。使用 MTCNN进行脸部检测

9.42所示设计步骤如图

![](figures/571-6-FIGURE.jpg)

冬 $9. 4 2$ 设计步骤

请参阅程序：09 07脸部检测_mtenn.pynb

(1）加载相关模块，包含 MTCNN。程序代码如下： ## (2）加载并显示图文件。程序代码如下：

1#藏人图文件
2 image file = "./images face/classmates.jpg" 3 image = plt.imread(image file)
4
5#显示图像
6 plt.imshow(image)
7 plt.axis('off')
8 plt.show()

执行结果：如图9.43所示

![](figures/572-3-FIGURE.jpg)

图9.43：加载显示图文件

建立MTCNN对象，检测脸部。程序代码如下 (3)

![](figures/572-6-FIGURE.jpg)

(4）脸部增加框与特征点，并显示图像。程序代码如下

执行结果：如图9.44所示，特征点包括眼睛、鼻子、嘴角，详细内容请参阅程序。

![](figures/573-1-FIGURE.jpg)

![](figures/573-2-FIGURE.jpg)

 $9$ 44增加特征点图

(5）将每张脸个别显示出来。程序代码如下：

![](figures/573-5-FIGURE.jpg)

执行结果：如图9.45所示

![](figures/574-0-FIGURE.jpg)

图9.45、个别显示

注意：face-recognition 是以 lib 为基础的库，所以上述指令就是先安装dllb库，在Windows作业环境 $\mathrm{T}$ ，必须备妥以下工具。

## 9-6-3：脸部追踪

脸部追踪可在影片中追踪特定人的脸部，这里使用的库是 face-recognition，安装指令为： pip install face-recognition

(1）Microsoft Visual Studio 2017/2019

(2）CMake for Windows：安装后将bin 路径（如 CAProgram Files\CMakelbin）加入环境变量Path中

范例1、使用Face-Recognition库进行脸部检测

9.46 所示。 设计步骤如图

![](figures/575-7-FIGURE.jpg)

图 $9. 4 6$ 设计步骤

请参阅程序： 09 08脸部检测Face Recognition.ipynb

1）加载相关库，包含Face-Recognion。程序代码如下：

(4）脸部加框，显示图像。注意：框的坐标所代表的方向依序为上/左/下/右（逆时钟程序代码如 $\mathrm{F}$ :

## (2）加载并显示图文件。程序代码如下：

1#载人图文件
2 image file = "./images face/classmates.jpg" 3 image = plt.imread(image file)
4
5#显示图像
6 plt.imshow(image)
7 plt.axis('off')
8 plt.show()

执行结果：如图9.47所示

![](figures/576-4-FIGURE.jpg)

图9.47加载并显示图文件

(3）调用 tace ocaions函数检测脸部。程序代码如下：

1#检测脸部
2 faces = facegrecognition.face locations(image) 1#脸部加框
2 ax = plt.gca()
3 for result in faces:
4 #取得框的坐标
5 yl, xl, y2, x2 = result
7 withrgieht xa - xl, $y^{2}-y 1$ 
6
8 rect = Rectangle((x1, y1), width, height, fill=False, color='red') 9 ax.add patch(rect）
10
11#显示图象
12 plt.imshow(image)
13 plt.axis('off')
14 plt.show()

执行结果：如图9.48所示

![](figures/577-2-FIGURE.jpg)

 $9$ .48 脸部加框图

(5）检测脸部特征点并显 $\overline{{\Pi}}$ 。程序代码如下：

![](figures/577-5-FIGURE.jpg)

执行结果如下。 范例 $2$ 。使用Face-Recognition 库进行视频脸部追踪，程序修改自face-recognition GitHub 的范例

## 1）五官特征点的坐标如下

![](figures/578-2-FIGURE.jpg)

(2）脸部轮廓画线。如图9.49月所示

![](figures/578-4-FIGURE.jpg)

图9.49 脸部轮廓画线

设计步骤如图9.50所示

![](figures/578-7-FIGURE.jpg)

图 $9. 5 0$ 设计步骤

请参阅程序：09 09脸部追踪Face Recogntion.ipynb

(3）指定输出文件名。注意，影片分辨率设为
$$
( 6 4 0, 3 6 0 )
$$
故输入的影片不得低于此分辨率，否则输出文件将无法播放。程序代码如 $\mathrm{T}$ :

(4）加载要辨识的图像，范例设定两 $\uparrow\uparrow$ : Lin-Manue
Miranda（美国歌手）与Barack Obama（美国总统) 需先编码为向量，以利于脸部比对。程序代码如 $\mathrm{T}$ :

1）加载相关库。程序代码如下：

## (2）载入影片文件。程序代码如下

1#载入影片文件
2 input movie = cv2.VideoCapture("./images face/short hamilton clip.mp4" 3 length = int(input movie.get(cv2.CAP PROP FRAME COUNT)）
4 print(f'影片帧数：{length}')

执行结果：影片总顺数为 $2 7 5$ 

![](figures/579-6-FIGURE.jpg)

![](figures/580-0-FIGURE.jpg)

## (s）变量初始化。程序代码如下

| 1 | #变量初始化 |  |
| 2 | face locations = 门 12 # | 门 12 # 脸部位置 |
| 3 | face encodings S= [] # | [] # 脸部编码 |
| 4 | face names = 「1 # | # 脸部名称 |
| 5 | frame number= o # | # 帧数 |

(6）比对脸部并存档。程序代码如下

![](figures/581-0-FIGURE.jpg)

执行结果如下：

(7）输出的影片为images face/output.avi文件：观看影片后发现，检测速度较慢，且并未检测到Obama，因图片文件是正面照，而图像文件则是侧面的画面；但瑕不掩瑜，大致上仍可以追踪主夏影像的动态。

范例3。改用WebCam 进行脸部实时追踪，程序修改自face recognition GitHub的范例间

由于步骤重叠的部分较多，所以只说明与范例3有差异的程序代码。

| Writing | frame | 4 | / | 275 |
| writing | frame | 5 | / | 275 |
| writing | frame | 6 | / | 275 |
| Writing | frame | 7 | / | 275 |
| writing | frame | 8 3 | / | 275 |
| writing | frame | 9 | / | 275 |
| writing | frame | 10 | 0A | / 275 |
| writing | frame | 11 | L | / 275 |
| writing | frame | 12 | 2A | / 275 |
| writing | frame | 13 | B | / 275 |
| writing | frame | 1 14 | l 1 | / 275 |
| writing | frame | 6 15 | 5 | / 275 |
| writing | frame | 5 16 | 5 6A | / 275 |
| writing | frame | 17 | 7 | / 275 |
| writing | frame | 18 | 3 | / 275 |
| writing | frame | 19 | 9A | / 275 |
| writing | frame | 20 | 0 | / 275 |
| writing , | frame | 21 | L | / 275 |

请参阅程序：0910脸部追踪_webcam.ipynb

(1）以读取WebCam取代加载影片文件。程序代码如下：

1#指定第一台webcam
2 video_capture = cv2.VideoCapture(0)

(2）读取 WebCam 一帧影像：第4行。程序代码如下：

$$
( 3 ) \uparrow\Delta\uparrow\downarrow\uparrow\uparrow\exists\exists\uparrow\downarrow\uparrow\downarrow\uparrow\downarrow\uparrow\uparrow\downarrow\uparrow\uparrow\uparrow\uparrow\uparrow\uparrow\uparrow\uparrow\uparrow\uparrow\uparrow\uparrow
$$
相同，但存盘改成实时显 $\overline{{\Pi}}$ 。程序代

原作者也示范 $\exists-\uparrow[ 5 ] \ddag$ ，可在RaspberryPI执行，实时进行脸部追踪

| 46 | # 显示每一帧影像 |
| 47 | cv2. imshow('Video 3 frame) |

(4）按 $\mathbf{q}$ 即可跳出循环。程序代码如下：

| 49 | # 按 2 q 即可跳出循环 |
| 50 | if cv2.waitKey(1) & OxFF == ord('q'): |
| 51 | break |

检测脸部特征点可使用Face-Recognition、dlib 或者 OpenCV 库，它们都可以检测到 68个特征点，如图9.51所示

Face-Recognition 库检测脸部特征点已在09 08脸部检测 Face Recognition.ipynb 实践过，不再多做介绍。dlilb是另一套包含了机器学习、数值分析、计算器视觉、图像处理等功能的区数库，它使用G++开发，只要安装Face-Recognition 库就会自动安装dib，如果要单独安装 dlib，可参考笔者撰写的博客文dlb安装心得 -Windows 环境」

## 9-6-4 脸部特征点检测

![](figures/584-3-FIGURE.jpg)

图9.1脸部68个特征点的位置

## 范例1. 实践脸部特征点的检测使用dlib

设计步骤如图9.52所示 (1）加载相关库，imutils库是一个简易的图像处理函数库安装指令为：

![](figures/585-1-FIGURE.jpg)

 $9. 5 2$ 设计步骤冬

请参阅程序：0911脸部特征点检测.ipynb

pip install ils。 程字代码如下：

(2）加载并显示图文件。程序代码如下：

1#载人图文件
2 image file = "./images face/classmates.jpg" 3 image = plt.imread(image file)
4
5#显示图像
6 plt.imshow(image)
7 plt.axis("off')
8 plt.show()

执行结果：如图9.53所示

![](figures/585-8-FIGURE.jpg)

Ddlib 特征点模型文件为
shape predictor 68 face andmarks.dat，可检测68个点，如果只需要检测 $5$ 个点就好，可加载
shape predictor 5 face landmarks.dat

图9.3：加载并显示图文件

）检测脸部特征点并显示 (3)

Bdib.get rontal ica detector检测脸部

Bdlib.shape prcitor:检测脸部特征点。

$$
\star\Xi\equiv\pm\pm\exists\pm\pm\pm\pm\pm
$$

![](figures/586-6-FIGURE.jpg)

执行结果： 9.54所示
如图

(4）检测视频文件也没问题，按Esc 键即可提前结京。程序代码如下：

(1）FacemarkLBF: Shaocing Ren等学者于 2014年发表 TFace Alignment at 3000 FPS via Regressing Local Binary Features」所提出的

(2) FacemarkAAM: Georgios Tzimiropoulos 等学者于 2013 年发表「Optimization problemsforfast AAMfitting in-the-wildu所检测到2张脸部.

![](figures/587-3-FIGURE.jpg)

图9.54$检测显示特征点

![](figures/587-5-FIGURE.jpg)

OpenCV针对脸部特征点的检测，提供了以下三种算法。

(3）FacemarkKamezi: $\mathsf{V}$ Kazemi和·.Slian 于 2014年发表「One Millisecond Face Alignment with an Ensemble of
Regression Trees」所提出的。

$$
\ddag\equiv\pm\pm\uplus^{[ 2 1 ]}.
$$
D

我们分别实验一下，着看有什么差异。

范例 $2$ 。使用OpenCV库进行脸部特征点检测

9.55 所示设计步骤如图

![](figures/588-5-FIGURE.jpg)

 $9. 5 5$ 设计步骤冬

请参阅程序：09 12 脸部特征点检测_OpenCV.ipynb

(1）加载相关库：注意，只有OpenCV扩充版提供相关 API，所以，须执行下列指令，改安装 OpenCV扩充版。

D卸载：pip uninstall openc-python opencv-contrib-python.

②安装库:
$$
\mathrm{o i p ~ i n s t a l l ~ o p e n c v-c o n t r i b-p y t h o n_{o} ~}
$$

$$
\star\equiv\lfloor\pm\lfloor\exists\pm\rfloor\rceil\top\colon
$$

(2）加载并显示图文件：使用Lena图像测试。程序代码如

## (2）加载并显示图文件：使用Lena 图像测试。程序代码如

$$
\mathrm{T} \! :
$$

![](figures/589-3-FIGURE.jpg)

执行结果：如图9.56所示

![](figures/589-5-FIGURE.jpg)

图9.56加载并显示图文件

(3）使用FacemarkLBF 检测脸部特征点。程序代码如下：

![](figures/589-8-FIGURE.jpg)

执行结果： 显示脸部和特征点的坐标如下执行结果：结果准确， $\P$ 是无法检测到左上角被帽子遮蔽的部 $\acute{\jmath}$ ，如图9.57所示

(5）改用FacemarkAM 来检测脸部特征点。程序代码如下:

faces [[225 205 152 152]]
landmarks LBF True [array([[[201.31314, 268.08807],
[201.5153 , 293.1106 ]，
[204.91422, 317.07196],
[210.71988, 340.4278 ],
[222.97098, 360.37122],
[240.34521, 375.51422]，
[260.10678, 386.35587],
[280.64197, 392.04227],
[298.6573 , 390.89835],
[311.434 , 384.88406],
[318.37827, 371.235381，
[324.82266, 357.113 ],
[331.87363, 342.1786 ]，
[339.7072 , 327.1501 ],
[346.04462, 311.9719 ]，
[349.2847 , 296.59448],
[348.95883, 280.12585],
[236.43172, 252.06743],

(4）绘制特征点并显示图像。程序代码如下：

![](figures/590-4-FIGURE.jpg)

![](figures/590-5-FIGURE.jpg)

至9.57绘制显示特征点

(6）绘制特征点并显示图像：过程与前面的程序代码相同程序代码如下：

执行结果：如图9.58所示，左上角反而多出一些错误的特征点。

(7) Facemarkkamezi检测脸部特征点。程序代码如换用
下：

![](figures/591-3-FIGURE.jpg)

![](figures/591-4-FIGURE.jpg)

![](figures/591-5-FIGURE.jpg)

图9.8 FacemarkAAM 检测结果

(8）绘制特征点并显示图像：过程与前面的程序代码相同程序代码如下：

执行结果：如图9.59所示，左上角也是多出一些错误的特征点。

![](figures/592-2-FIGURE.jpg)

![](figures/592-3-FIGURE.jpg)

图9.59 FacemarkKamezi检测结果

检测完脸部特征点后，利用线性代数的法向量比较多张脸的特征点，就能找出哪一张脸最相似，使用Face-Recognition或dlib 库均可。

范例。使用Face-Recognition 或dlilb库，比对哪一张脸最相似。

$$
\pi: \qquad( 1 ) \pm\pm\pm\pm\mp\mp\mp\mp\mp\mp
$$
l Face-Recognition 库。程序代码如

## 9-6-5 脸部验证

设计步骤如图9.60所示

![](figures/593-5-FIGURE.jpg)

图 $9. 6 0$ 设计步骤

请参阅程序：09 13脸部验证.ipynb

![](figures/593-8-FIGURE.jpg)

(2）载入所有要比对的图文件。程序代码如下：

(3）图像编码：使用 face recognition.face encodings 函数编码。程序代码如下

(4）使用face recognition.compare faces 进行比对。程序代码如下：

执行结果：如图9.61所示

![](figures/594-3-FIGURE.jpg)

图9.61载 $\lambda$ 图文件

![](figures/594-5-FIGURE.jpg)

![](figures/594-6-FIGURE.jpg)

执行结果：[True， True， True, False]，前三笔符合，完全正确

(6）加载模型：包括特征点检测、编码、脸部检测。程序代码如下：

 $( 7 )$ 定义脸部编码与比对的函数：由于dilb无相关现成的欧数，必须自行撰写。程序代码如下

(5）加载相关库：改月dib库。程序代码如下：
(

![](figures/595-4-FIGURE.jpg)

![](figures/595-5-FIGURE.jpg)

![](figures/595-6-FIGURE.jpg)

(8）加载图文件并显示。程序代码如下： ## 执行结果：显示两张脸的法向量距离，数字越小表示越相似。 执行结果如下：

![](figures/596-1-FIGURE.jpg)

## (9）图像编码。程序代码如下

![](figures/596-3-FIGURE.jpg)

## 10）比对。程序代码如下

![](figures/596-5-FIGURE.jpg)

## 执行结果： □ 数字越小表示越相似。 显示两张脸的法向量距离

比较两张脸的法向量距离：[0.3998327850880958, 0.4104153798439364, 0.3913189516694114,0.9053701677487068] 排序:（0.3913189516694114,0.3998327850880958, 0.4104153798439364,0.9053701677487068)
依相似度排序：（"jared_3.jpg jared 1.jpg', 'jared 2.jpg" obama.jPg')

除了前面的介绍，另外还有很多其他类型的影像应用，举例如下

(2）影像修复（Image Inpainting 用周围的影像将部分影像做修复，可用于抹除照片中不喜欢的对象。

利用深度学习开发影像相关的应用系统，也是种类繁多，举例如下

光学文字辨识，是把图像中的印刷字辨识为文字，以节省大量的输入时间或抄写错误，可应用于支票号码/金额辨识、车牌辨识

$$
\mathrm{( A u t o m a t i c \, N u m b e r \, P l a t e \, R e t )}
$$
cognitionAPR）等，但也有人用于

## 9-7 光学文字辨识

(1）光学文字辨识（Optical Character Recognition. OCR)

(3) 影像的建构与辨识。
3D

(1）防疫：是否有戴口罩的检测、社交距离的计算

(2）交通：道路拥塞状况的检测、车速计算、车辆的违规 (越线、闯红灯)等。

(3）智能制造：机器人与机器手臂的视觉辅助

(4）企业运用：考勤、安全监控破解登录用的图形码验证（Captcha 我们就来看看如何实践 OCR辨识。

Tesseract OCR 是目前很盛行的 OCR 软件，HP $\big< \g$ 司于 2005 年开放源代码（Open Source），使用C++ 开发而成的，可白源代码建置，或直接安装已建置好的程序，在这里我们采取后者，自 https://github.com/UB-Mannheim/tesseract/wiki下载最新版.exe $\grave{\chi}$ 件，直接执行即可。安装完成后，将安装路径下的 bin 子目录放 $\lambda$ 环境变量path内。若要以Python 调用Tesseract OCR，需额外安装pytesseract库，指令为pip install pytesseract.

例如，辨识一张发票文件（.images ocr/receipt.ong，见图 9.62）的指令为

tesseract images ocr/receipt.ong .images ocrresult.ixi-l eng --psm 6

其中：-leng为辨识英文：--psm 6为单一区块 $\mathrm{( a \ s i n g l e}$ uniform block of text) 相关的参数请参考 Tesseract OCR官网 (https://github.com/tesseract-
ocr/tesseract/blob/master/doc/tesseract.1.asc)

最简单的测试指令如下：

tesseract图片文件><辨识结果文件： 执行结果： $\prod$ 乎全部正确，只有特殊符号误判，如图9.63所示。

范例。以Python 调用 Tesseract OCR API，辨识中、英文请参阅程序：09 14 OCR.ipynb.

范例。以Python 调用Tesseract OCR API，辨识中、英文

![](figures/599-3-FIGURE.jpg)

 $9$ .62发票图

(图片来源:Acomprehensive guide to OCR with Tesseract, OpenCV and Python lasly

![](figures/599-6-FIGURE.jpg)

图9.63、辨识发票执行结果

请参阅程序：09 14 OCR.jpynb

(1）加载相关库。程序代码如下 (4）只辨识数字：参数设定加ouiputbase digits。.程序代码如下:

## (2）加载并显示图文件。程序代码如下：

1#载人图文件
2 image = cv2.imread('./images ocr/receipt.png')
3
4 #显示图文件
5 image RGB = cv2.cvtColor(image, cv2.COLOR BGR2RGB) 6 plt.figure(figsize=(10,6))
7 plt.imshow(image RGB)
8 plt.axis('off')
9 plt.show()

(3）OCR辨识：调用image to string函数。程序代码如

## (3） OCR辨识：调用image tostring函数。程序代码如

$$
\mathrm{T \! :}
$$

执行结果：与直接下指令的辨识结果大致相同

$$
\mathrm{T :}
$$

1#参数设定，只辨识数字
2 custom conf: ${\tt i g} ~=~ {\Gamma}^{\prime}-{\tt p s m}$  $6$ outputbase digits
3#OCR辨识
4 print(pytesseract.image to string(image, config-custom config))

## 执行结果如下：

0001 122011
$$
\begin{matrix} 4 3 3 8. \\ 7 1 ~ ~ 2 \\ 2 9. 9 5 1 9. 9 0 \\ 1 ~ ~ 3. 7 9 \\ 1 ~ ~ 4. 5 0 \\ 2 8. 1 9 \end{matrix}
$$
0 30.69
30.69

## (5）只辨识有限字符。程序代码如下

1#参数设定白名单，只辨识有限字符
2 custom config = r'-c tessedit char whitelist-abcdefghijklmnopqrstuvwxyz --psm 6 3#OCR $\frac{1} {2} y=2 \sqrt{1}$ 
4 print(pytesseract.image to string(image, config=custom config))

## 执行结果如下：

elcometoels
heck
erverdeshf
able uests
eefurgrea
efries
udight
ud
ubtotal
alesfax
a
alanceue
$$
\mathrm{h a n k y o u f o r y o u r p a t r o n a g e ~ a t}
$$

$$
\mathrm{e l c o m e t o e l s}
$$
heck
$$
e r v e r d e s h f
$$
$$
\mathrm{a b l e ~ u e s t s}
$$
$$
\begin{array} {l} {{\mathrm{e e f u r g r e a}}} \\ {{\mathrm{e f r i e s}}} \\ \end{array}
$$

## (6）设定黑名单：只辨识有限字符。程序代码如下：

1#参数设定黑名单，只辨识有限字符
2 custom config = r'-c tessedit char blacklist-abcdefghijklmnopqrstuvwxyz --psm $6$ 3 $\# \ O C R$ 辨识
4 print(pytesseract.image to string(image, config=custom config))

## 执行结果如下：

W M]'
 $\mathrm{c}$ #: 0001 12/20/11 $\mathrm{s}$ : J F 4:38 PM
$$
\begin{array} {l} {{\texttt{I \! :} 7 / 1 ~ 6 \mathtt{:} ~ 2}} \\ {{2 ~ B ~ B ~ ( 6 9. 9 5 / ) ~ 1 9, 9 0}} \\ {{\texttt{S I D E :} ~ F}} \end{array}
$$
1BL3.79
1 B 4.50
S-] 28.19
S T 2.50
TOTAL 30.69
BI D 30.69
T é!

W M]'
 $\mathrm{c}$ #: 0001 12/20/11 $\mathsf{s}$ :J F 4:38 PM
T: 7/1 G：2
2 B B(E9.95/) 19,90 SIDE:F

$$
\begin{array} {l} {1 \textsc{B} \bot3. 7 9} \\ {1 \textsc{B} 4. 5 0} \\ \end{array}
$$
28.19
$$
{\mathsf{S T}} \, 2. 5 0
$$
$$
\mathsf{T O T A L} \, 3 0. 6 9
$$
BI D 30.69

$$
\mathtt{T} \quad\mathtt{e l} \quad\mathfrak{c}
$$

## (7）辨识多国文字：先加载并显示图文件。程序代码如下：

(8）辨识多国文字：先自htps://gihub.com/tesseract
ocr/tessdata best下载各国字库，放 $\lambda$ 安装目录的 tessdata子日录内（C: Program Files Tesseract-OCRtessdata 程序代码如
下：

1#载入图文件
2 image = cv2.imread('./images ocr/chinese s.png') 3
4#显示图文件
5 image RGB = cv2.cvtColor(image, cv2.COLOR BGR2RGB) 6 plt.figure(figsize=(10,6))
7 plt.imshow(image RGB)
8 plt.axis('off')
9 plt.show()

## 图文件如下

Tesseract OCR 是目前很盛行的OCR 软件，HP公司于2005年开放源代码(Open Source)，以C++开发而成的，可由源代码创建，或直接安装已创建好的程序， 在这里我们采取后者，自https://github.com/UB-Manmheim/tesseract/wiki下载最新版exe文件，直接执行即可。安装完成后，将安装路径下的bin 子目录放入环境变量path内。若要以Python 调用Tesseract OCR，需额外安装 pytesseract库，
VVY
指令如下： $\cdot$ 
pip install pytesseract-

![](figures/602-4-FIGURE.jpg)

## 执行结果： 9.64所示如图

Tesseract OCR 是目前很盛行的OCR 软件，HP 公司于2005年开放源代码（Open
Sourcej，以C++开发而成的，可由源代码创建，或直接安装已创建好的程序爱品安, 座厦拔行,阳 安餐架股: 省将服安物1路区gAN牛 康发
执行即可·安装完成后，将安装路径下的bin子目录放入环德期th内·若要以Python调用Tesseract ocR，需额外安装Pytesseract库
指令如下:

$$
{\tt0 i d i n s t a l l p y t e s s e r a c t} \circ
$$

![](figures/603-0-FIGURE.jpg)

9.64 Tesseract $4$ 支持的语言

(图片来源：Tesseract官网的语言列表）

车牌辨识（Automatic Number Plate Recognition, ANPR)系统已被应用多年 $\overline{{\jmath}}$ ，早期用像素逐点辨识，或将数字细线化后，再比对线条，但最近几年改为采用深度学习进行辨识，它已被应用到许多领域。

(2）停车场：当车辆进场时，系统会先辨识车牌并记录，要出场时会辨识车牌，自动扣款。

请参阅程序：09 15车牌辨识。 $\mathbf{i p y n b}$ ，程序修改自CarLicense Plate Recognition using Raspberry Pi and OpenCV/23l

## 9-8 车牌辨识

）机车检验：检验单位的计算机会先进行车牌辨识。 (1)

## 范例，以OpenCV 及Tesseract OCR进行车牌辨识。

设计步骤如图9.65 所示。

![](figures/604-7-FIGURE.jpg)

冬 $9. 6 5$ 设计步骤

1）加载相关库。程序代码如下 (3）先转为灰阶，会比较容易辨识，再提取轮廓。程序代码如下：

1#载人相库
 $2$ import cv2
3 import imutils
4 import numpy as np
5 import matplotlib.pyplot as plt 6 import pytesseract
 $7$ from PIL import Image

## (2）加载并显示图文件。程序代码如下：

1#载人图文件
2 image = cv2.imread('./images ocr/1.jpg',cv2.IMREAD COLOR) 3
4 #显示图文件
5 image RGB = cv2.cvtColor(image, cv2.COLOR BGR2RGB)
6 plt.imshow(image RGB)
7 plt.axis $( {\mathrm{\Lambda o f f}}^{1} )$ 
8 plt.show()

执行结果：如图9.66所示，此测试图来自于原程序

![](figures/605-5-FIGURE.jpg)

图9.66加载并显示图文件

![](figures/605-7-FIGURE.jpg)

执行结果：如图9.67所示 (4）取得等高线区域，并排序，取前10个区域。程序代码如下:

(5）找第一个含四个点的等高线区域：将等高线区域转为近似多边形，接着导找四边形的等高线区域。程序代码如下：

![](figures/606-2-FIGURE.jpg)

图9.67转为灰阶并提取轮廓

$$
\mathrm{T} :
$$

![](figures/606-5-FIGURE.jpg)

![](figures/606-6-FIGURE.jpg)

(6）在原图上绘制多边形，框住车牌。程序代码如下：

| #在原图上绘制多边形，框住车牌 |
| 2 if screenCnt is None: |
| 3 detected= o |
| 1 print("No contour detected") |
| 6 else: |
| 5 detected= 1 |
|  7 |
| 3 if detected = 1: |
| cv2.drawContours(image, [screenCnt], -1,(0,255,0)，3） |
| print(f'车牌坐标=\n{screenCnt}') |

(7）去除车牌以外的图像，找出车牌的上下左右的坐标，计算车牌宽高。程序代码如 $\mathbf{F}$ :

(8）仿射（Affine Transformation），将车牌转为矩形：仿射可将偏斜的梯形转为矩形，笔者发现等高线区域的各点坐标都是以逆时针排列，因此，当要找出第一点坐标在哪个方向时，通常它会位在上方或左方，所以不需考虑右下角。程序代码如下：

执行结果：/HR.26 BR 9044，有误认一些非字母或数字的符 $\frac{\Xi} {\Xi}$ ，可直接去除，这样车牌辨识的结果就完全正确。

![](figures/607-3-FIGURE.jpg)

![](figures/607-4-FIGURE.jpg)

## (9）车牌号码OCR辨识。程序代码如下

1#车牌号码 $O C R$ 辨识
2 text = pytesseract.image to string(Cropped, config='--psm 11') 3 print("车牌号码：",text)）

(10）显示原图和车牌。程序代码如下：

(H）再使月images ocr/l.ipg测试，车牌为 NAX-6683，辨识为NAY-6683,将 $\mathbf{X}$ 误认为 $\mathbf{Y}$ ，有可能是中国台湾省车牌的字形不同，可使用中国台湾省车牌字形供Tesseract OCR 使用。执行结果如图9.69所示。

![](figures/608-1-FIGURE.jpg)

执行结果：如图9.68月所示

![](figures/608-3-FIGURE.jpg)

图9.68 显示原图和车牌

![](figures/608-5-FIGURE.jpg)

图9.696更换图片执行结果

另外，笔者实验发现，若镜头拉远或拉近，而导致车牌过大或过小的话，都有可能辨识错误，所以，实际进行时，镜头最好与车牌距离能固定，会比较容易辨识。假如图像的画面太杂乱，则取到的车牌区域也有可能是错的，而这个问题相对容易处理，当OCR 辨识不到字或者字数不足时，就再找其他的等高线区域，即可解决。

从这个范例可以得知，通常一个实际的案例，并不会像内建的数据集一样，可以直接套用，常常都需要进行前置处理，如灰阶化、提取轮廓、找等高线区域、仿射等，数据清理（Data Clean) 完才可导入模型加以训练，而且为了适应环境变化，这些工作还必须反复进行。所以有人统计，仅仅收集数据、整理数据、特征工程等工作就占项目85%的时间，把最烦琐的工作处理好，才是项目成功的关键因素，这与参加Kaggle竞赛是截然不同的感受，成功总是藏在细节里。

CNN的应用领域那么多 $\overline{{\pi}}$ ，相当实用，但是它仍存在以下缺点。

(1）卷积不考虑特征在图像的所在位置，只针对局部窗口进行特征辨识，因此，图9.70所示的两张图，辨识结果是相同的，这种现象称为「位置无差异性」 (Position Invariant

(2）图像中的对象如果经过旋转或倾斜，CNN就无法辨识 $\overline{{\jmath}}$ ，如图9.71所示。

## 9-9卷积神经网络的缺点

![](figures/610-4-FIGURE.jpg)

图 $9. 7 0$ 正常的人脸和五官移位

(两者对 CNN来说是无差异的。图片来源：Disadvantages of
CNNmodelsas

(3) ，人眼可以辨识不同的对象特征，但CNN
图像坐标转换，
却难以辨 $\grave{\i} \mathbb{R}$ ，如图9.72所示。

因此，Geofrey Hinton等学者就提出了胶囊算法
(Capsules) 用来改良CNN的问题，有兴趣的读者可以进一步研究 Capsules.

![](figures/611-2-FIGURE.jpg)

图 $9. 7 1$ 右图为左图侧转近180度，CNN无法辨识

(图片来源：Disadvantages of CN models l

![](figures/611-5-FIGURE.jpg)

 $9. 9 2$ 右图为上下颠倒的左图，CNN无法辨识图

(图片来源：Disadvantages of ONN modls B

因此，Gofirey Hinto $\mathbf{n}$ 等学者就提出了胶囊算法

水能载舟，亦能覆舟。Al虽然给人类带来了许多便利，但也带来不小的危害。近几年泛滥的深度伪造（Deepfake）就是一例， $\overrightarrow{\mathrm{E}}$ 利用 $\mathbf{A} \mathbf{I}$ 技术伪造政治人物与明星的视频，能够做到真假难辨，一旦在网络上散播开来，就会产生莫大的社会危害。根据统计，名 $\mathcal{A}$ 色情片八成都是伪造的。深度伪造的基础算法就是生成对抗网络

(Generative Adversarial Network, GAN) 本章就来介绍这一课题。

Facebook人工智能研究院 Yann LeCun 在接受 Quora 专访时说到：「GAN及其变形是近十年最有趣的想法。」（This, and the variations that are now being proposed is the most interesting idea in the last 10 years in ML, in my opinion.）。一句话导致GAN一炮而红，其作者lan Goodfelow 也成为各界竞相邀请演讲的对象。

另外，2018年10月纽约佳士得艺术拍卖会，也卖出了第一幅利用GAN 算法绘制的肖像画，最后得标价为432500美金。有趣的是，画作右下角还列出GAN的损失函数，如图10.1所示。相关报导可参见 $\ll$ 全球首次！AI创作肖像画10月佳士得拍卖》及Is arificial intellicence set to become art's next mediumtl.

## 第10章

## 生成对抗网络此后有人统计，每28分钟就有一篇与GAN相关的论文发表

![](figures/613-1-FIGURE.jpg)

图10.1 Edmond de Belamy肖像画

## (图片来源：佳土得网站问

关于生成对抗网络有一个很生动的比喻：它是由两个神经网络所组成，一个网络扮演伪钞制造者（Counterteiter），一直制造假钞；另一个网络则扮演警察，不断从伪造者那边拿到假钞，并判断真假，然后，伪造者就根据警察判断结果的反馈，不停改良，直到最后假钞变成真假难辨，这就是 GAN的概念。

「伪钞制造者」称为「生成模型」（Generative model)
「警察」则是「判别模型 (Discriminative model 简单的架构如图10.2所示。

(1）先训练判别神经网络：从训练数据中抽取样本，导入判别神经网络，期望预测概率D $( x ) \approx\! 1$ ，相反地，判断来自生成网络的伪造图片，期望预测概率D $( G \ ( z ) \ ) \approx0 \circ$ 

(2）训练生成网络：刚开始以正态分布或均匀分布产生噪声 (z) 导入生成神经网络，生成伪造图片。

## 10-1生成对抗网络介绍

![](figures/614-5-FIGURE.jpg)

图 $1 0. 2$ 生成对抗网络的架构

 $\sharp$ 理流程如下。

(3）透过判别网络的反向传导（Backpropagation） 更新生成网络的权重，即改良伪造图片的准确度（技术），反复训练，直到产生精准的图片为止，如图10.3所示。

(1）判别神经网络的损失函数：前半段为真实数据的判别， 后半段为伪造数据的判别。即

recognize generated images better

式中：x为训练数据，故预测概率 $\boldsymbol{D}$ （x）越大越好； $\not E$ 为期望值，因为训练数据并不完全相同，故预测概率有高有低:z为伪造数据，预测概率 $\boldsymbol{D}$  $( G ~ \langle z \rangle$ ）越 $\imath\rfloor\omicron$ 越好，调整为 $1-D ~ ~ ( G ~ ~ ( z )$ )
） 变成越大越好。

D取log：并不会影响最大化求解，通常概率相乘会导致多次方，不睿易求解，故取log，变成一次方函数。

(2）生成神经网络的损失函数：即判别神经网络损失函数的右边多项式，生成神经网络期望伪造数据被分类为真的概率越大越

![](figures/615-5-FIGURE.jpg)

10.3 判别神经网络的反向传导冬

GAN根据以上流程重新定义损失函数。

$$
\operatorname* {m a x}_{D} V ( D )=\sharp E_{\scriptscriptstyle x \sim P_{\mathrm{d e a}}} ( x ) [ \operatorname{l o g} D ( x ) ]+\sharp E_{\scriptscriptstyle z \sim P_{z} ( z )} [ \operatorname{l o g} ( 1-D ( G ( z ) ) ) ]
$$

recognize real images better

D两者相加当然是越大越好。

因为函数左边的多项式与生成神经网络的损失函数无关，故加上亦无碍。整个算法的伪码如图10.4所示，使用小批量梯度下降法，最小化损失函数

生成网络希望生成出来的图片越来越逼真，能通过判别网络的检验，而判别网络则希望将生成网络所制造的图片都判定为假数据，两者目标相 $\o$ ，互相对抗，故称为生成对抗网络。

好，故差距越小越好。

$$
\operatorname* {m i n} V ( G )=\sharp\! \! \! E_{z \sim P z ( z )} [ \operatorname{l o g} ( 1-D ( G ( z ) ) ) ]
$$

## (3）两个网络损失函数合而为一的表示法为

$$
\operatorname* {m i n} \operatorname* {m a x}_{G \to P_{z \to\infty}} V ( D, G )=\not{\! \! E}_{x \sim P_{\mathrm{d i m}}} [ \operatorname{l o g} D ( x ) ]+\not{\! \! E}_{z \sim P_{z} ( z )} [ \operatorname{l o g} ( 1-D ( G ( z ) ) ) ]
$$

![](figures/616-6-FIGURE.jpg)

$$
\nabla_{\theta_{g}} \frac{1} {m} \sum_{i=1}^{m} \operatorname{l o g} \left( 1-D \left( G \left( z^{( i )} \right) \right) \right).
$$

图10.4 GAN算法的伪码

GAN 不是只有一种模型，其变形非常多，读者可以参阅「The GAN Zoo」14，有上百种模型，其功能各有不同，具体如下

$$
( 2 ) \mathrm{\ A C G A N} \! :
$$
参阅Towards the Automatic Anime Characters
Creation with Generative Acversarial Networks'的，可生成不同的动漫人物，如图10.6所示。

## 10-2生成对抗网络种类

(1） CGAN：参阅Pose Guided Person image $G e n e r a t i o n^{[ 5 ]}$ ，可生成不同的姿势，如图10.5所示

![](figures/617-4-FIGURE.jpg)

图10.5 CGAN算法的姿势生成

![](figures/617-6-FIGURE.jpg)

图10.6 ACGAN算法（从左边的动漫角色，生成为右边的新
角色）

作者附有一个展示的网站，可利用不同的模型与参数，生成各种动漫人物，如图10.7所示。

$$
( 3 ) \ \mathrm{C y c l e G A N}
$$
：风格转换，作者也附上 $7-$ 个展示的网站
:
$$
\mathrm{( h t t p s : ) / m i l-t o k y o. g i t h u b. i o / w e t}
$$
sbdnn），可选择不同的风格图，生
成各式风格的照片或视频，如图10.8所示。

![](figures/618-2-FIGURE.jpg)

10.7 ACGAN展示网站冬

![](figures/618-4-FIGURE.jpg)

图10.8CyoleGAN的展示网站

(4）StarGAN：参阅StarGAN:Uniied Generative Adversarial Networks for Multi-Domain Image-to-image $\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\lfloor\medskip\tabularnt^{1}$ ，生成不同的脸部表情，转换肤色、发色或是性别程序代码在htips://github.com/yunjey/stargan，展示如图10.9所 $\overline{{\pi}}$ 。

$$
( 6 ) ~ \mathrm{\ S t y l e G A N 2 :}
$$
功能与语义分割（Image Segmentation)
相 $\sqrt{\sum}$ ，它是从语义分割图渲染成实景图，参阅 $A n a l y z i n g \, a n d$ 

![](figures/619-2-FIGURE.jpg)

图10.9 StarGAN展示（将左边的脸转换肤色、发色、性别或
表情)

 
(5）SRGAN：可以生成高分辨率的图像，参阅Photo-Realistic Single image Super-Resolution Using a Generative Adversarial Network3，如图10.10 所示

![](figures/619-5-FIGURE.jpg)

图10.10SRGAN展示（由左而右从低分辨率的图像生成为高
分辨率的图像)

限于篇幅，这里仅介绍一小部分的算法，读者如有兴趣可参阅 IGA-- Sone col applications of GAN W文，其有更多种算法的介绍。

只要修改原创者GAN的损失函数，即可产生不同的效果，所 $\operatorname{l o g}$ ，根据「The GAN Zoo」 以的统计，GAN 相关的论文数量呈现爆炸性的成长趋势，如图10.12所示。

improving the inag Qaityof StyieGA"，程序代码在 https:/github.com/NVlabs/stylegan2，如图10.11 所示。

![](figures/620-3-FIGURE.jpg)

图10.11 StyleGAN2展示（从右边的图像生成为左边的图
像)

![](figures/621-0-FIGURE.jpg)

 GAN有关的论文数量呈现爆炸性的成长图10.12与

范例1. $\mathbf{D C G A N}$ 算法，产生手写阿拉伯以MNIST数据实践
数字

加载相关库：需先安装imageio 库，以利于产生GIF动
(1)
画。程序代码如下：

## 10-3 DCGAN

我们先来实践DCGAN（Deep Convolutional Generative Adversarial Network）算法。

10.13月所示设计步骤如图

![](figures/622-5-FIGURE.jpg)

图 $1 0. 1 3$ 设计步骤

请参阅程序
$$
: ~ 1 0 \_0 1 \_\mathrm{D C G A N \_M N I S T. i p y n b_{c}}
$$

训练数据集： $\mathrm{M N I S T}_{\circ}$ 

$$
\pi: \qquad\exists x \pm1 \pm\pm\pm\pm\pm\mp\pm
$$
为TensorFlow Dataset。程序代码如

Duse bias-False：训练不产生偏差项，因为要生成的影像尽量是像素所构成的。

PActivation Function 采用LeakyReLU：尽可能不要产生0的值，以免生成的影像有太多空白。

3Conv2DTranspose：反卷积层，进行上采样，由小图插补为大图，strides- $( 2, 2 )$ 表示宽、高各增大2倍。

![](figures/623-4-FIGURE.jpg)

1#载入相关库
2 import glob
3 import imageio
import matplotlib.pyplot as plt
import numpy as np
6import os
import PIL
8 import tensorflow as tf
9 from tensorflow.keras import layers $1 0$ import time
11 from IPython import display

$$
\mathrm{T} :
$$

![](figures/623-7-FIGURE.jpg)

## (3）定义生成神经网络

0最后产生竞、高为 $( 2 8, 2 8 )$ 的单色向量

$$
\pi_{\pm}^{\Pi} \equiv1 \pm h \pm\pm\Pi\Gamma:
$$

(5）定义判别神经网络：类似于一般的CNN判别模型，但要去除池化层，避免信息损失。程序代码如 $\mathrm{F}$ :

![](figures/624-1-FIGURE.jpg)

(4）测试生成神经网络。程序代码如下

1#测试生成神经网络
2 generator = make generator model()
4#测试产生的噪声
5 noise = tf.random.normal([1, 100])
6 generated image = generator(noise, training-False) 8#显示噪声生成的图象
9 plt.imshow(generated image[0, :, :, 0], cmap='gray')

执行结果： 10.14所示
如图

![](figures/624-4-FIGURE.jpg)

图10.14测试生成神经网络

(6）测试判别神经网络：真实影像的预测值会是较大的值， 生成影像的预测值则会是较小的值，因为无明显的线条特征。程序代码如下：

(8）定义损失函数为二分类交叉熵（BinaryCrossentropy） 优化器为 Adam.

判别神经网络的损失函数为真实影像加上生成影像的损失函数和，因为判别神经网络会同时接收真实影像和生成影像。程序代码

![](figures/625-3-FIGURE.jpg)

![](figures/625-4-FIGURE.jpg)

执行结果：预测值 $=-0. 0 0 2 0 8 4 5 6$ ，为很小的值。

(7）测试真实的影像。程序代码如下

![](figures/625-7-FIGURE.jpg)

执行结果：预测结果如下，为绝对值较大的值。

| 预测值=「 0.053187441 |  |
| 1-0.10278386 |  |
| 0.062099617 |  |
| 0.048261871 |  |
| 0.01407145]1 |  |

(9）设定检查点：在检查点将模型存盘，要花很长的时间训练，万一中途断掉，还可以从上次的断点继续训练。

还原 $( \mathrm{r e s t o r e} )$ 指令为：checkpoint.restore
(firain.latest checkpoint (checkpoint dir) 程序代码如下：

(11）定义梯度下降函数：同时对生成的神经网络和判别神经网络进行训练，噪声与真实图像也一起导入判别神经网络，计算损失及梯度，并更新权重。

$$
\flat[ \mathrm{T} :
$$

![](figures/626-4-FIGURE.jpg)

![](figures/626-5-FIGURE.jpg)

## 10）参数设定。程序代码如下

![](figures/626-7-FIGURE.jpg)

@tfitunotion：会产生运算图，使函数指令周期加快。程序代码如下：

$$
\pi: ( 1 2 ) \ \ \Xi\times i \Vert\Vert\pm\Vert\Sigma\Vert\pm\emptyset\rangle
$$
引时间产生图像并存盘。程序代码如

## 如下：

![](figures/627-3-FIGURE.jpg)

$$
\mathrm{T} :
$$

![](figures/627-5-FIGURE.jpg)

 $( 1 3 )$ 定义产生图像的函数。程序代码如下： 执行结果：训练了50个周期，数字已隐约成形，如图10.15所示。

(15）将训练过程中的存盘图像转为GIF文件，并显示GF文件。程序代码如下

![](figures/628-2-FIGURE.jpg)

## (14）训练模型：训练过程会产生动画效果。程序代码如下：

1 train(train dataset, EPOCHS)

![](figures/628-5-FIGURE.jpg)

图10.15、训练结果

![](figures/628-7-FIGURE.jpg)

执行结果：注意GIF文件会不断循环播放，所以也可以打开 「文件管理单击「大图显示」来检视，如图10.16所示

接着以名人脸部数据集，生成近似真实的图像，程序修改自 Keras 官网「DCGAN to generate face images lW

![](figures/629-2-FIGURE.jpg)

图10.16文件转换及显示

范例12.以名人脸部数据集实践DCGAN算法。

请参阅程序
$$
: ~ 1 0 \_0 2 \_\mathrm{D C G A N \_F a c e. i p y n b_{o}}
$$

训练数据集：名人脸部，约1.3GB.

1）加载相关库。程序代码如下

(2）下载解压缩至celeba gan目
img align celeba.zip,
录，产生数据集（Dataset) 图像缩放为 $( 6 4, 6 4 )$ 程序代码如下:

执行结果：共有202599个图文件，同属一个类别（以子目录为类别名称）

(4）定义判别神经网络：使用一般的 CNN结构，包含卷积层与完全连接层，用来辨识输入的影像，但为了避免影像信息减损， 故不包含池化层。程序代码如下：

![](figures/630-3-FIGURE.jpg)

（3）加载并显示第一个图文件。程序代码如下：

1#载人并显示第一个图文件
2 image = next(iter(dataset))
3 plt.axis("off")
4 plt.imshow((image.numpy() * 255).astype("int32")[0])

执行结果： 10.17所示
如图

![](figures/630-7-FIGURE.jpg)

图10.17加载并显示图文件

执行结果： $( 6 4, 6 4, 3 )$ 宽高各为64， RGB
输入图像维度为
三颜色

(5）定义生成神经网络：使用一般的译码器结构，含完全连接层、反卷积层（Conv2DTranspose）与卷积层，可输出特征向量。程序代码如 $\mathrm{T}$ :

执行结果：输入随机向量大小为 $( 8 \star8 \star1 2 8 )$ 最后网络输出为 $( 6 4, 6 4, 3 )$ 与判别神经网络输入的维度一致。程序代码如下：

![](figures/631-3-FIGURE.jpg)

![](figures/631-4-FIGURE.jpg)

$$
\mathrm{M o d e l :} \mathrm{` ` g e n e r a t o r "}
$$

| Output Shape | Param # |
| --- | --- |
| (None,8192) | 1056768 |
| (None,8,8,128) | 0 |
| （None,16,16,128) | 262272 |
| (None,16,16,128) | 0 |
| （None,32,32,256) | 524544 |
| (None,32,32,256 | 0 |
| （None,64,64,512) | 2097664 |
| (None,64,64,512) | 0 |
| (None,64,64,3)  | 38403 |
| 飞 |  |

## (6) 组合两个网络。程序代码如下： 定义GAN:

1#定义GAN，组台两个网络
2 class GAN(keras.Model):
3 def init (self, discriminator, generator, latent dim): 4 super(GAN, self). init()
g self.generator = generator
self.discriminator = discriminator
7 self.latent dim = latent dim
8
9 #编译·定义损失函数
10 def compile(self, d optimizer, g optimizer, loss fn):
11 super(GAN, self).compile()

(7）自定义 Callback：在训练过程中存储图像。程序代码如下:

| 12 49 | self.d optimizer = d optimizer 上 |
| 13 14 | self.g optimizer = g optimizer se1f.loss fn= loss fn  |
| 15 | self.d loss metric = keras.metrics.Mean(name="d loss") |
| 上 16 | oc-romc-ncco lg-co.ccgic-ooo c-1fdaccmatnir- karac motnirc MaanInamo-"g 1acc"Y |
| 一 17 | -8--55 |
| 一 18 | #效能指标·判别神经网络、牛成神经网络 |
| 中 19 | aproperty |
| 20 | def metrics(self): |
| 21 | return [self.d loss metric, self.g loss metricT |
| 22 | 匕一一一一 |
| 23 | #训练 |
| 一 24 | def train step(self. real images): |
| 25 | 一一 #随机抽样 batch size笔，维度大小:Latent dim(128) |
| 26 | batch size = tf.shape(real images)[o] |
| 27 A0 | random latent vectors = tt.random.normal(shape=(batch size, selt.latent dim)) |
| 28 A0 | 热小7临 |
| 29 | #生成图象 |
| 50 一4 | generatea images = selt.generator(random latent vectors) |
| Dl 20 | 丹告训金数护生 |
| 52 53 | #司川双热级口 A550506656-0 |
| 22 2A | comoineu lmdges - t.coicatlgeierdte lmdgesy edl lmdgesy axls-u |
| 34 一 | 江一来海记台一进建司名—476 |
| 55 26 | #川然火店物立教上二花国门物企教/0 1ahalc-tfrAncatl  |
| 一 37 | 一 [tf.ones((batch size, 1)), tf.zeros((batch size, 1))], axis=0 |
| 37 A | [tt.ones((batch size, l)), tt.zeros((batch size, i)), axis=o |
| 38 |  |
| 39 |  |
| 40 | #将标签加入噪声，此步练养常重要 |
| 41 | labels t= 0.05 * tf.random.uniform(tf.shape(labels)) |
| + 42 | +话08-85507-0/ |
| 42 A | 本机业明为饭网位 |
| 43 | #训练判别神经网络 |
| 44 | TLO with tf GradientTane() as tane. |
| 44 汽一 | with tt.Gradientlape() as tape:  |
| 45 | predictions = self.discriminator(combined images) |
| 46 | d loss = self.loss fn(labels, predictions) |
| 47 AO | grads - tape.gradient(d loss, selt.discriminator.trainable weights) |
| 48 | self.d optimizer.apply gradients( ●电06696 |
| 49 | zip(grads, self.discriminator.trainable weights) |
| 50 一 |  |
| 51 |  |
| →L 52 | #随机样batch size笔，维度大小：Latent dim(128) |
| DL 53 | # 股0- 01528老址及·408764m(120 random latent vectors = tf.random.normal(shape=(batch size, self.latent dim)) |
| 53 54 | random latent vectors = tt.random.normal( snape=(oatcn size, seiT.latent aim)) |
| 54 一一 | 北司6集 |
| 55 C6 | 开士桃图康/物企教0 micloading lahelc - tf zarocl(hatrh cizo1)Y |
| Do 57 | misleduing ldteis - tr.zeros((batcnsize, 1)) |
| 57 58 | 世训练生成被经网络，注音，不可更新物别袖经网络的权善，具更新生成被经网络的权善 |
| 58 CO | #川新水种经网给社总个可史机判种绘网纺砂伙里，家史利生影神会网纺的伙里 S+5+EC4：A+TAA+An.  |
| 22 60 | wLcnc·drdiciticpe/ astcpe. nnedi-tions - celf diccriminaton celf conerator(random latont vertonc)Y |
| 60 | predictions - self.discriminator(self.generator(random latent vectors)) |
| 一 62 |  口一一口 grads - tape.gradient(g loss, self.generator.trainable weights) |
| 63 | C— self.g optimizer.apply gradients(zip(grads, self.generator.trainable weights) |
| 花——一一 64  |
| 65 | #计算效果指标 |
| 66 | self.d loss metric.update state(d loss) |
| 67 | self.g loss metric.update state(g loss) |
| 68 | return { |
| 69 | "d loss": self.d loss metric.result(), |
| 70 | "g loss": self.g loss metric.result(), |
| 71 | } |

## '7) Calback：在训练过程中存储图像。程序代码如自定义

$$
\mathrm{T} \! :
$$

(8）训练模型：在每个执行周期结束时产生10张图像，依据原程序的批注，训练周期正常需要100 $\gtharpoondown$ ，才能产生令人惊艳的图 $\Ha{H}_{\circ}$ 程序代码如下：

个执行结果：笔者的PC执行1周期就需 $2 \sim\! 3 \eta\g$ 时，如果单纯使用CPU，时间会更长，但毕竞实际项目会有完成时间的压力，这时候GPU卡就显得格外重要。

3执行30个周期的结果：如图10.19 所示，虽然有改善，但仍然合不符预期效果。

![](figures/634-3-FIGURE.jpg)

![](figures/634-4-FIGURE.jpg)

2执行1周期结果：还非常模糊，如图10.18斤示

![](figures/634-6-FIGURE.jpg)

冬 $1 0. 1 8$ 训练周期结果

![](figures/635-0-FIGURE.jpg)

图10.19、执行30个周期结果

Progressive GAN 也称为Progressive Growing GAN 或
PGGAN，它是NVIDIA 201 $7$ 年发表的一篇文章Progressive
Giroming of GAt to imoroved aiy siaity, and Variation l 中提到的算法，它可以生成高画质且稳定的图像， $\imath\rfloor$ 图像透过层层的神经层不断扩大，直到所要求的尺寸为止，大部分是针对人脸的生成，如图 10.20所示。

它的厉害之处是算法生成的尺寸可以大于训练数据集的任何图像，这称为「超分辨率」（Super Resolution 网络架构如图 10.21所示。

## 10-4 Progressive GAN

![](figures/636-3-FIGURE.jpg)

图 10.20 e GAN的示意图

(图片来源：Progressive Growing of GANs or Improved Quality, Stability, and Variation 12

生成网络（G）使用类似于Residual的神经层，一边输入原图像，另一边输 $\lambda$ 为反卷积层，使用权重 $\alpha$ ，设定两个输入层的比例。判别网络（D）与生成网络（G）做反向操作，进行辨 $\grave{\i} \mathbb{R}$ 。使用名人脸部数据集进行模型训练，根据论文估计，使用 $8$ 颗Tesla $\mathsf{V}$ 100 GPU，大约要训练 $4$ 天，才可以得到不错的效果。还好， TensorFlow Hub 提供了预先训练好的模型，我们马上来测试 $- \mathrm{T}$ 吧。

![](figures/637-1-FIGURE.jpg)

冬 10.21 eGAN的网络架构

## 范例。再拿名人脸部数据集来实践Progressive GAN算法

(1）使用随机向量， 10.22所示
流程如图

![](figures/637-5-FIGURE.jpg)

冬 $1 0. 2 2$ 使用随机向量流程

(2）若使用自定义的图像，流程如图10.23所示 (1）加载相关库：需先安装scikit-image、imageio库。程序代码如下：

(2）定义显示图像的函数以及转为动画的函数。程序代码如下:

![](figures/638-2-FIGURE.jpg)

图10.23使用自定义图像流程

请参阅程序：1003 GAN ®
Face.ipynb

训练数据集：名人脸部，约1.3GB

![](figures/638-6-FIGURE.jpg)

1 #生成网络的输人向量尺寸
2 latent dim = 512
3
4#定义显示图像的函数
5 def display image(image):
6 image = tf.constant(image)
7 image = tf.image.convert image dtype(image, tf.uint8)
8 return PIL.Image.fromarray(image.numpy())
9
10#定义显示一序列图像转为动画的函数
11 from tensorflow docs.vis import embed
12 def animate(images):
13 images = np.array(images)
14 converted images = np.clip(images * 255, 0, 255).astype(np.uint8) 15 imageio.mimsave('./animation.gif', converted images)
16 return embed.embed file('./animation.gif')
17
18 #工作记录文件只记录错误等级以上的信息
19 logging.set verbosity(logging.ERROR)

(3）插补两个向量，产生动画：采用非线性插衣 $\vdash$ ，在超球面上插补两个向量，这样才能有较为平滑的转换，随机抽取两张图像，由第一张图像开始，渐变成第二张图像。程序代码如 $\mathrm{F}$ :

(4）使用随机数或自定义的图像：由第 $5$ 行的
image from nodule space 变量来调控，设为True 表示使用随机数，False 表示使用自定义的图像。程序代码如下：

![](figures/639-2-FIGURE.jpg)

执行结果：如图10.24所示，由左图渐变为右图。

![](figures/639-4-FIGURE.jpg)

图10.24插补向量产生动画

![](figures/640-0-FIGURE.jpg)

执行结果：目标图像如图10.25所示

![](figures/640-2-FIGURE.jpg)

图10.25目标图像

(5）以PGGAN处理的图像作为起始图像。程序代码如下：

1#以PGGAN 必理的图像作为起始图像
2 initial vector = tf.random.normal([1, latent dim]) 3
4#显示图像
5 display image(progan(initial vector)['default'][0])

执行结果： 10.26所示。 起始图像如图

![](figures/640-7-FIGURE.jpg)

(6）找到最接近的特征向量，渐变成第二张图像：模型训练的过程中，每 $5$ 个步骤就会产生一个图像。程序代码如下

## 图10.26起始图像

![](figures/641-2-FIGURE.jpg)

(7）显示训练过程的损失函数。程序代码如下

1#显示训练过程的损失函数
3 $2$ plt.plot(loss)
$$
\mathrm{\sf~ p l t. y l i m} ( \, [ \, \emptyset\,, \mathrm{\sf~ m a x} ( \, p \, ] \, t \,. \, y \, ] \, \mathrm{\sf~ i m} ( \, ) \, ) \, ] \, )
$$

## (8 显示动画。程序代码如下

执行结果：如图10.27所示，由左图渐变为右图

①修改步骤 $4$ 第 $5$ 行的image from module space 变量控制，更改为False，使用自定义的图像，脸部特写必须一致，效果才会比较好。

![](figures/642-1-FIGURE.jpg)

图 10.27 显示动画

2 10.28 所示起始图像如图

![](figures/642-4-FIGURE.jpg)

10.28起始图像冬

③自定义的图像如图10.29-斤示，为目标图像。

![](figures/642-7-FIGURE.jpg)

冬 $1 0. 2 9$ 目标图像

④执行结果：如图10.30所示

![](figures/643-0-FIGURE.jpg)

图 10.30 显示动画执行结果

DCGAN生成的图片是随机的，以MNIST数据集而言，生成图像的确会是数字，但无法控制要生成哪一个数字。Conditional GAN 增加 $\overline{{\varsigma}} \underline{{\varrho}}$ 个条件（Gondition），即目标变量y，用来控制生成的数字。

Conditional GAN 也称为CGAN，它是Mehdi Mirz等学者于 201 $4$ 年发表的一篇文章「Conditional Generative Adversaria Nets」Il中所提出的算法，它修改GAN损失函数为

范例。以Fashion MNIST数据集实践Conditional GAN算法

## 10-5 Conditional GAN

$$
\operatorname* {m i n} \operatorname* {m a x}_{G \ \ D} V ( D, G )=\varkappa_{\stackrel{x \sim P_{\mathrm{d i m}}} {}} ( x ) [ \operatorname{l o g} D ( x \mid y ) ]+\varkappa_{\stackrel{x \sim P_{z}} {} ( z )} [ \operatorname{l o g} ( 1-D ( G ( x \mid y ) ) ) ]
$$

它将单纯的 $\boldsymbol{D}$ （x）改为条件概率D（xy） O

设计流程如图10.31所示

![](figures/644-7-FIGURE.jpg)

图 $1 0. 3 1$ 设计流程

请参阅程序: 10 04 CGAN FashionMNIST.ipynb

(2）定义判别神经网络：把Fashion MNIS丁数据集的标记也视为特征变量（X 因为我们要指定生成图像的类别，透过嵌入层（Embedding）转成50个向量。程序代码如下：

1）加载相关库。程序代码如下

1#载人相库
2 from numpy import zeros, ones, expand dims
3 from numpy.random import randn
4 from numpy.random import randint
5 from tensorflow.keras.datasets.fashion mnist import load data
6 from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import Model
8 from tensorflow.keras.layers import Input, Dense, Reshape, Flatten
9 from tensorflow.keras.layers import Dropout, Embedding, Concatenate, LeakyReLU 10 from tensorflow.keras.layers import Conv2D, Conv2DTranspose

![](figures/645-3-FIGURE.jpg)

(3）定义生成神经网络：同样需要标记。程序代码如下：

(4）定义CGAN神经网络：结合判别神经网络和生成神经网络。程序代码如下：

(5）定义GAN的真实训练数据集：真实数据的标记均为1。 程序代码如下：

![](figures/646-2-FIGURE.jpg)

![](figures/646-3-FIGURE.jpg)

## (5）定义GAN的真实训练数据集：真实数据的标记均为1。

(7）训练模型：训练GAN模型，内含判别神经网络、生成神经网络。程序代码如下

![](figures/647-1-FIGURE.jpg)

## (6) GAN的生成数据。程序代码如下： 定义

![](figures/647-3-FIGURE.jpg)

![](figures/648-0-FIGURE.jpg)

执行结果：模型训练需要几个小时的时间

8）定义噪声生成、显示图像的函数。程序代码如下：

执行结果：如图10.32所示，结果非常理想，我们指定标记就可以生成该类别的图像。

![](figures/649-1-FIGURE.jpg)

## (9）预测并显示结果。程序代码如下：

| #载人模型 |
| 2 model = load model('cgan generator.h5') |
| 3 #生成100批数据 |
| - latent points, labels = generate latent points(100, 100) |
| #标记~9 |
| labels = asarray([x for in range(10) for x in range(10)]) |
| 匕一 |
| 3 # 预测开显示结果 |
| 1 X = model.predict([latent points, labels) |
| #将缘素范围由「-1,17转换为0,17  |
| X=(X+1)/2.0  |
| 2 #绘图 |
| save plot(X,10) |

![](figures/649-4-FIGURE.jpg)

图10.32 显示结果

上例是运用Conditional GAN很简单的例子，只是把标记一并当作 ${\bf X}$ ，输入模型中训练，作为条件或限制条件（Constraint） 另外还有很多延伸的做法，例如ColorGAN，它把前置处理的轮廓冬作为条件，与噪声一并当作 $\mathbf{X}$ ，就可以生成与原图相似的图像， 并且可以为灰阶图上色，相关细节可参阅Colorization Using
ConvNet and GAN"或「End-to-End Conditional GAN-baseo Architectures for Image Colourisation 1Sl,如图10.33所示。

![](figures/650-1-FIGURE.jpg)

$$
\boxed{\S} ~ 1 0. 3 3 ~ ~ ~ \mathrm{C o l o r G A N}
$$

(图片来源：Cot
$$
\, p r i z a t i o n \, U s i n g \, C o n v N e t \, a n d \, G A N^{[ 1 4 ]} \, )
$$

Pix2Pix为 Conditional GAN算法的应用，出自Philip lsola等学者在 2016年发表的Image-to-image Translation with Conditiona Adversarial Networks $[ 1 6 ]$ 它能够将影像进行像素的转换，故称为 Pix2Pix，可应用于以下领域。

## 10-6 Pix2Pix

将语义分割的街景图转换为真实图像，如图10.34所示 (1)

![](figures/651-3-FIGURE.jpg)

图10.34将语义分割的街景图转换为真实图像

(以下图片均来自于mage-to-image Transiation wit Conditional Adversarial Networks'i)

(2）将语义分割的建筑外观转换为真实图像。

(3）将卫星照转换为地图，反之亦可，如图10.35 所示

![](figures/651-8-FIGURE.jpg)

生成网络采用的U-net结构，引进了 Skip-connect的技巧，即每 $- \pi$ 反卷积层的输入都是前一层的输出加与该层对称的卷积层的输 $\uplus$ ，译码时可从对称的编码器得到对应的信息，使得生成的图像保有原图像的特征。

判别网络额外考虑输 $\lambda$ 图像的判别，将真实图像、生成图像与输入图像合而为一，作为判别网络的输 $\lambda$ ，进行辨识。原生的 GAN在预测像素时，以真实数据对应的单一像素进行辨 $\grave{\imath} \exists$ ，然而 Pix2Pix 则引用PatchGAN的思维，利用卷积将图像切成多个较小的区域，每个像素与对应的区域进行辨识，计算最大可能的输出。 10.35 将卫星照转换为地图图

(4）将白天图像转换为夜晚图像，如图10.36所示。

![](figures/652-4-FIGURE.jpg)

图10.36将白天图像转换为夜晚图像

(5）将轮廓图转为实物图像，如图1.37所示

![](figures/652-7-FIGURE.jpg)

图10.37将轮廓图转为实物图像

PatchGAN可
$$
\begin{array} {l} {\n5} \\ \end{array} ] \exists\, l m a g e-t o-l m a g e \ T r a n s l a t i o
$$
n with Conditiona

范例。以CMP Facade Database数据集实践Pix2Pix GAN 算法。CMP Facade Database"共有12类的建筑物局部外形，如外观（fagade）、造型（molding）、屋檐（cornice）、柱子 (pillar）、窗户（window）、门（door）等。数据集自
httos ${\tt=} / /$ people.eecs.berkeley.edu/-tinghuiz/orojects/oix2pix/da tasets/facades.tar.gz下载.

请参阅程序：10 05 Pix2Pix.ipynb。注意：执行此范例，耗时较长，原文估计使用单片V100 GPU，训练一个周期约需15秒。 设计流程如图10.38所示

![](figures/653-4-FIGURE.jpg)

冬 $1 0. 3 8$ 设计流程

(1）加载相关库。程序代码如下 ## (2）参数设定并定义图像处理的函数。程序代码如下：

![](figures/654-1-FIGURE.jpg)

(3 显示任一张训练图片。程序代码如下： 执行结果：如图10.39所示，左为语义分割图，右为实景图

(4）随机转换测试：也可作为数据增补。程序代码如下：

![](figures/655-2-FIGURE.jpg)

图 $1 0. 3 9$ 显示训练图 $\ H$ 

1#随机转换测试
2 plt.figure(figsize=(6, 6))
3 for i in range(4):
4 rj inp, rj re = random jitter(inp, re) 5 plt.subplot(2, $2$ ,i+1)
6 plt.imshow(rj inp/255.0)
7 plt.axis('off')
8 plt.show()

执行结果：如图10.40所示

![](figures/655-6-FIGURE.jpg)

$$
\begin{array} {c c} {( 5 ) \; \; \hbar[ \pm\lambda) ] \vert\vert\hbar\neq\pm\pm\pm\pm\pm\pm\pm} \\ {\hbar[ \pm\lambda] \vert\hbar\downarrow=\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\} \\ \end{array}
$$
换为TensorFlow Dataset。程序代码

图10.40随机转换测试

![](figures/656-2-FIGURE.jpg)

(6）定义采样、上采样函数，并做简单测试

D定义采样函数。程序代码如下

![](figures/656-5-FIGURE.jpg)

执行结果：（1,128,128,3 O 2定义上采样函数。程序代码如下：

![](figures/657-1-FIGURE.jpg)

执行结果： $( 1, 2 5 6, 2 5 6, 3 )$ 。

(7）定义生成神经网络：U-Net结构。程序代码如下 1 def Generator():
2 inputs = tf.keras.layers.Input(shape=[256, 256, 3]）
3
4 down stack= [
5 downsample(64, 4, apply batchnorm=False), #(bs, 128, 128, 64)
6 downsample(128, 4), #(bs, 64, 64, 128)
7 downsample(256, 4), #（bs, 32, 32, 256）
8 downsample(512, 4), #(bs, 16, 16, 512)
9 downsample(512, 4), #(bs, 8, 8, 512)
10 downsample(512, 4), #(bs, 4, 4, 512)
11 downsample(512, 4), #（bs, 2, 2, 512)
12 downsample(512, 4), #(bs, 1, 1, 512)
13 ]
14
15 up stack = [
16 upsample(512, 4, apply dropout=True), #(bs, 2, 2, 1024)
17 upsample(512, 4, apply dropout=True), #（bs, 4, 4, 1024)
18 upsample(512, 4, apply dropout=True), #(bs, 8, 8, 1024)
19 upsample(512, 4), #（bs, 16, 16, 1024)
20 upsample(256, 4)， #（bs, 32, 32, 512)
21 upsample(128, 4), #(bs, 64, 64, 256)
22 upsample(64, 4), #(bs, 128, 128, 128)
23 ]
24
25 initializer = tf.random normal initializer(o., 0.02)
26 # (bs, 256, 256, 3）
27 last = tf.keras.layers.Conv2DTranspose(OUTPUT CHANNELS, $4$ , strides=2,
28 padding='same', kernel initializer=initializer, activation='tanh') 29
30 x = inputs
31
32 # Downsampling through the model
33 $s k i p s \,=\, \left[ \, \right]$ 
34 for down in down stack:
35 ${\bf x} \,=\, \mathrm{d o w n} \, ( {\bf x} )$ 
36 skips.append(x)
37
38 skips = reversed(skips[:-1])
39
40 # Upsampling and establishing the skip connections
41 for up, skip in zip(up stack, skips):
42 ${\textbf{x}}=\operatorname{u p} ( {\textbf{x}} )$ 
43 x = tf.keras.layers.Concatenate()([x, skip])
44
45 ${\bf x} \,=\, {\tt l a s t} \, ( {\bf x} )$ 
46
47 return tf.keras.Model(inputs=inputs, outputs=x)

## t8：绘制生成神经网络模型。程序代码如下

| 1 | #文建立生成神经网络 |
| 2 | generator = Generator() |
| 3 |  |
| 4 | # 绘制生成神经网络模型 |
| 5 | tf.keras.utils.plot model(generator, show shapes=True, dpi=64) |

执行结果：为U型结构

(9）测试生成神经网络：取第一批数据。程序代码如下： 执行结果：如图10.41所示，只有隐约的形状。

![](figures/659-1-FIGURE.jpg)

图10.41测试神经网络

10）定义判别神经网络。程序代码如下

![](figures/659-4-FIGURE.jpg)

## 11）建立判别神经网络，并绘制模型。程序代码如下：

![](figures/660-1-FIGURE.jpg)

## 执行结果：如图10.42所示，主要为卷积层

![](figures/660-3-FIGURE.jpg)

图10.42判别神经网络及模型

(12）测试判别神经网络：取第一批数据。程序代码如下：

![](figures/660-6-FIGURE.jpg)

执行结果：如图10.43所示 （5）定义检查点：可设定以下函数，直接生成图像。程序代码如下：

![](figures/661-1-FIGURE.jpg)

10.43 测试判别神经网络冬

13）定义损失函数。程序代码如下

![](figures/661-4-FIGURE.jpg)

## 14）定义优化器。程序代码如下

![](figures/661-6-FIGURE.jpg)

| 1 | #定义检查点 |
| 2 | checkpoint dir = ./Pix2Pix training checkpoints |
| 3 | checkpoint prefix = os.path.join(checkpoint dir, ckpt") |
| 4 | checkpoint = tf.train.Checkpoint(generator optimizer-generator optimizer, |
| 5 | discriminator optimizer=discriminator optimizer |
| 6 | generator=generator, |
| 7 | discriminator=discriminator) |

## 16）生成图像，并显示。程序代码如下：

![](figures/662-2-FIGURE.jpg)

## 执行结果：如图10.44-所示，为还未训练的结果。

![](figures/662-4-FIGURE.jpg)

图10.44生成图像并显示

 $( 1 7 )$ 定义训练模型的函数。程序代码如下

(18）训练模型：可先启动Tensorboard查看工作记录，在 Notebook中启动如下。程序代码如下

![](figures/663-1-FIGURE.jpg)

1#启动Tensor Board
2 %load ext tensorboard
3 %tensorboard --logdir {log dir}
4#训练模型
5 fit(train dataset, EPOCHS, test dataset)

## 19）取5批数据测试。程序代码如下： 执行结果：如图10.45所示，以下仅截图两批数据，经过训练 $\F_{\Xi}$ ，预测的图像已没有树木或栏杆了

![](figures/664-1-FIGURE.jpg)

图10:45取 $5$ 批数据测试

前面 GAN算法处理的都是成对转换数据，而CycleGAN则是针对非成对的数据生成图像。成对的意思是一张原始图像对应一张日标图像，图10.46 右方表示多对多的数据，也就是给予不同的场域（Domain) 原始图像就可以合成指定场景的图像。

CycleGAN 或称 Cycle-Consistent GAN，是Jun-Yan Zhu等学者于 2017年发表的一篇文章Unpairedlimage-to-image Translation using Cycle-Consistent Adversariat Networks is中提出的算法，概念如图10.47所示

## 10-7 CycleGAN

![](figures/665-3-FIGURE.jpg)

图10.46 成对的数据（左方）&非成对的数据（右方）

(以下图片来源均来自Unpairad imae-to-image Translation using Cyole-Consistent Adversarial Ntworks il

![](figures/665-6-FIGURE.jpg)

（1）图10.47（a）：有两个生成网络，G将图像白X场域生成 Y场域的图像，F网络则是相反功能，由Y场域生成X场域的图像。

(2）图10.47 （b) 引进cycle consistency losses 概念，可以做 $( \exists) x \to G \; \; ( x ) \; \to F \; ( G )$ (0） $) ~ \approx x$ ，即x经过 $\boldsymbol{G}$ 、F转换，可得到近似于x的图像，称为Forward $\mathrm{c y c l}$ e-consistency loss

(3）图 10.47 (0) 从另一场域y开始，也可以做到 $y \to F$ 
$$
( y ) ~ \to~ G ~ ~ ( F ~ ( y ) ~ ~ ) ~ \approx~ y,
$$
称为 Backward cycle-consistency
loss.

(4）整个模型类似两个GAN网络的组合，具备循环机制，因此，损失函数为

这种机制可应用到影像增强（Photo Enhancement 影像彩色化（mage Colorization) 风格转换（Style Transfer）等功能，如图10.48 所示，帮一般的马匹涂上斑马纹。

$$
\boxed{\xi} 1 0. 4 7 \quad\mathrm{C y c l e G A N} \overline{{| x |}} \4 \xi\Xi\Xi\pm\pi\pi\pi\pi
$$

$$
L \ \ ( G, F, D_{X}, D_{Y} ) \ =L_{G A N} \ \ ( G, D_{Y}, \ \ X, Y ) \, X ) \ +\lambda L_{\mathrm{c y c}} \ \ ( G, F )
$$
+Lom{fFPZx`Y

$$
\equiv\# \#
$$

$$
\operatorname{\mathcal{L c y c}} ( G, F )=\operatorname{\textit{E}}_{x \sim P_{\mathrm{d i m}}} ( x ) [ \operatorname{\| F ( G ( x ) )-X \left\| 1 \right\|}+\operatorname{\textit{E}}_{y \sim P_{\mathrm{d i m}} ( y )} [ \operatorname{\| G ( F ( y ) )-y \left\| 1 \right\|} ]
$$

式中：入控制 $\boldsymbol{G}$ 、F损失函数的相对重要性。

![](figures/666-10-FIGURE.jpg)

范例。以 horse2zebra数据集实践 CycleGAN算法（流程与上一节大致相同，所以不再复制/粘贴，节省篇幅)

(2）加载训练数据：tfds.load 会加载内建数据集，相关资料可参考TensorFlow官网有关 $\mathrm{C y c l e G A N}$ 的描述9。程序代码如下：

图10.48CycdeGAN的功能展示

请参阅程序: 10 06_CycleGAN.ipynb.

(1）加载相关库。程序代码如下

![](figures/667-5-FIGURE.jpg)

![](figures/667-6-FIGURE.jpg)

(3）定义图像处理的函数：与上一节相同。程序代码如下 (4）定义数据前置处理的函数，设定数据集属性。程序代码如下：

![](figures/668-1-FIGURE.jpg)

## 如下：

![](figures/668-3-FIGURE.jpg)

如图 10.49所示，左图为原图，右图为随机转换的执行结果：
图

(6）定义CycleGAN神经网络：借用上一节的P'x2Px网络结构。程序代码如下：

数据测试。程序代码如下： (5)

![](figures/669-3-FIGURE.jpg)

![](figures/669-4-FIGURE.jpg)

图10.49、数据测试

执行结果：如图10.50所示，左图为原图，右图为未训练前的结果。

![](figures/670-1-FIGURE.jpg)

![](figures/670-2-FIGURE.jpg)

图 $1 0. 5 0$ 定义CyoleGAN神经网络并显示

执行结果：如图 10.51所示，未训练前的结果，左图为X判别网络的结果，右为Y判别网络的结果。

(8）定义损失函数为二分类交叉熵：包括判别网络、生成网络、循环损失与Identit $\mathbf{y}$ 损失函数。Identty 损失函数为输入Y与 $Y$ 生成网络的差异，X亦同，正常来说，差异应为 $\mathbf{O}$ 。程序代码如下：

(7）判别网络图像测试。程序代码如下

![](figures/671-3-FIGURE.jpg)

![](figures/671-4-FIGURE.jpg)

冬 $1 0. 5 1$ 判别网络图像测试

![](figures/672-0-FIGURE.jpg)

## (9）定义优化器：采月Adam。程序代码如下：

| 1 | #定义优化器 |
| 2 | generator g optimizer= t tf.keras.optimizers.Adam(2e-4, beta 1=0.5 |
| 3 | generator f optimizer= t tf.keras.optimizers.Adam(2e-4, beta 1=0.5) |
| 4 |  |
| 5 | discriminator x optimizer= t tf.keras.optimizers.Adam(2e-4, beta 1=0.5) |
| 6 | tf.keras.optimizers.Adam(2e-4, beta 1=0.5) |

## (10）定义检查点：将以上网络函数放入。程序代码如下：

| 1 | 定义检查点 eckpoint path = "./CycleGAN checkpoints/train" |
| --- | --- |
| 2 |
| 3 |
| 4 | pt = tf.train.Checkpoint(generator g-generator g, |
| 5 | generator f=generator f, |
| 6 | discriminator x=discriminator x, |
| 7 | discriminator y=discriminator y, |
| 8 | generator g optimizer-generator g optimizer, |
| 9 | 二 generator f optimizer-generator f optimizer, |
| 10 | 一一一一 discriminator x optimizer=discriminator x optimizer, |
| 11 | 一—— discriminator y optimizer=discriminator y optimizer) |
| 12 | 一 |
| 13 | pt manager - tf.train.CheckpointManager(ckpt, checkpoint path, max to keep=5) |
| 14 |
| 15 | 如果检查点存在，回复至最后一个检查点 |
| 16 | ckpt manager.latest checkpoint: |
| 17 ckpt.restore(ckpt manager.latest checkpoint) |
| 18 | print ('Latest checkpoint restored!!') |

(11）定义生成图像与显示的函数。程序代码如下：

| 1 | #定义生成图像及显示的时数 |
| 2 | def generate images(model, test input): |
| 3 | prediction = model(test input) |
| 4 |  |
| 5 | plt.figure(figsize=(12, 12) |
| 6 |
| 7 | display list - [test input[o], predictionfo1T |
| 8 | 一一 title = r'Input Image', 'Predicted Image'T  |
| 9 |  |
| 10 | for i in range(2): |
| 11 | plt.subplot(1,2, i+1) |
| 12 | plt.title(title[i]) |
| 13 | # getting the pixel values between [o, 1] to plot it. |
| 14 | plt.imshow(display list[i] * 0.5 + 0.5) |
| 15 | plt.axis('off') |
| 16 | plt.show() |

 $( 1 2 )$ 定义训练模型的函数。程序代码如下

![](figures/674-0-FIGURE.jpg)

13) 。程序代码如下训练模型。

执行结果：训练中的结果如图10.52所示。可以看出有逐步的转变，第一排右图为第 $9$ 周期的结果，第二排右图为第10周期的结果，图像已有明显改善，第三排右图为第 $1 2$ 周期的结果，已集中在马匹的处理上。

![](figures/675-1-FIGURE.jpg)

训练的最终结果：如图10.53所示，效果很好，马匹已加上了斑马纹。

![](figures/676-1-FIGURE.jpg)

Time taken for epoch 9 is 1200.320835351944 sec

![](figures/676-3-FIGURE.jpg)

Saving checkpoint for epoch 10 at ./CycleGAN checkpoints/train\ckpt-2 Time taken for epoch 10 is 1213.892056465149 sec

![](figures/676-5-FIGURE.jpg)

Time taken for epoch 12 is 1216.7267162799835 sec

图10.52 训练模型

每个训练周期均执行约1200秒，即20分钟，全部执行40个周期，笔者大概执行了15个小时

执行结果：如图10.54所示，以下仅截图两批数据，左侧为原图，右侧为预测的图像，效果比训练样本差，应该是因为训练的执行周期不足，原文作者执行了200个周期，如果真的照做，预测耗时可达三天两夜。

![](figures/677-2-FIGURE.jpg)

冬 $1 0. 5 3$ 训练模型结果

14）取 $5$ 批数据测试。程序代码如下

![](figures/677-5-FIGURE.jpg)

![](figures/678-0-FIGURE.jpg)

图10.54取两批数据测试

这一章我们认识了许多种不同的 GAN算法，由于大部分是由同一组学者发表的，因此可以看到演化的脉络。原生GAN加上条件 $\sqrt{\Xi}$ ，变成了Conditional GAN，再将生成网络改成对称型的U-Net $\sqrt{\Xi}$ ，就变成了Pix2Pix GAN，接着再设定两个Pix2Pix循环的网络，就衍生出了CycleGAN。除此之外，许多的算法也会修改损失函数的定 $\grave{\chi}$ ，来产生各种意想不到的效果。本书介绍的算法 $\P$ 是沧海一粟，更多的内容可参考李宏毅老师的PPT「Introduction of Generative Adversarial Network $( \mathrm{G A N} ) ~ \rfloor~^{[ 2 0 ]} \circ$ 

另 $\ --$ 方面，GAN不仅可以应用在图像上，还可以结合自然语言处理（NLP）、强化学习（RL）等技术，扩大应用范围，像是高解析图像生成、虚拟人物的生成、数据压缩、文字转语音（Text To Speech,TTS）、医疗、天文、物理、游戏等领域，可以参阅
lTutorial on Deep Generaive Models Pll文。

(1）生成的图像模糊：因为神经网络是根据训练数据求取回归，类似求取每个样本在不同范围的平均值，所以生成的图像会是相似点的平均，导致图像模糊。必须有非常大量的训练数据，加上相当多的训练周期，才能产生画质较佳的图像。另外，GAN对超参数特别敏感，包括学习率、滤波器尺寸，如果初始值设定得不好，导致生成的数据过差时，判别网络就会将其都判定为伪，到最后生成网络只能一直产生少数类别的数据了。

## 10-8 GAN挑战

纵使GAN应用广泛，但仍然存在以下挑战。

(2）梯度消失（Vanishing Gradient）：当生成的数据过差时，判别网络判定为真的概率接近于 $0$ ，梯度会变得非常 $\imath\rfloor$ ，因此就无法提供良好的梯度来改善生成器，导致生成器梯度消失。发生这种情形时可以通过多使用leaky ReLU activation function、简化判别网络结构或增加训练周期加以改善。

(3）模式崩溃（Mode Collapse）：是指生成器生成的内容过于雷同，缺少变化。如果训练数据的类别不止一种，生成网络则会为了让判别网络辨识的准确率提高，而专注在比较擅长的类别，导致生成的类别缺乏多样性。以制造伪钞来举例，假设钞票分别有 100 $\overline{{\pi}}$ 、500元与1000 $\overline{{\pi}}$ ，若伪钞制造者比较善于制作500 元纸钞，模型可能就会全部都制作500 元的伪钞。

(4）执行训练时间过久：反复实验的时候，如果没有相当的硬件支持，则每次调整参数都要花费大量时间

深度伪造（Deepfake）是目前很成熟的技术，也是一个 $\mathbf{A} \mathbf{I}$ 危害 $\mathcal{A}$ 类社会的明显例子。BuzzFeed.com 在 2018年放上了一段影 $\mathbb{H}$ ，名叫「You Won't Believe What Obama Says In This Video! 影片中奥巴马总统的演说全是伪造的，嘴型和声音都一分逼真，震惊了世 $\mathcal{A}$ ，自此以后，各界疯狂制作各种深度伪造影 $\ H$ ，使得网络上的影片真假难辨，引发了非常严重的假新闻灾难。

深度伪造大部分是在视频中换脸，由于人在说话时头部会自然转动，有各种角度的特写，因此，必须要收集特定人360度的脸部图像，才能让算法成功置换。从网络媒体中收集名人的各种影像是最容易的方式，所以，网络上流传最多的大部分是伪造名人的影 $\Hubuplus$ ，如政治人物、明星等。

深度伪造的技术基础来自于 GAN，类似于前面介绍的
$$
\mathrm{C y c l e G A N}
$$
，架构如图10.55所示，也能结合脸部辨识的功能，在
抓到脸部特征点 ） $\sqrt{\Xi}$ ，就可以进行原始脸部与要置换
(Landmark)
脸部的互换。

## 10-9深度伪造

![](figures/681-4-FIGURE.jpg)

Aayush Bansal等学者在2018年发表「Recycle-GAN: Unsupervised Video Retargeting 31, RecycleGAN 是扩充 CyceGAN的算法，它的损失函数额外加上了时间同步的相关性 (Temporal Coherence），即

类似于时间序列（Time Series t+1时间点的冬像应该是1 至时间点的图像的延续，如图10.56所示。因此，生成网络的损失函数为

图10.55深度伪造的架构示意图

(图片来源：「Understanding the Technology Behind DeepFakes p2j

![](figures/682-4-FIGURE.jpg)

$$
L_{\tau} ( G_{{}_{X}}, G_{{}_{Y}}, P_{{}_{Y}} )=\sum_{t} \left\| \, x_{{}_{t+1}}-G_{{}_{X}} ( P_{{}_{Y}} ( G_{{}_{Y}} ( x_{{}_{1 t}} ) ) ) \, \right\|^{2}
$$

式中： $G_{y} \ \ ( x_{i} )$ 是将x转成y的生成网络

RecyoleGAN算法的演进如图10.57所示

![](figures/683-0-FIGURE.jpg)

图 $1 0. 5 6$ 图像的延续

(图片来源：Recycle-GAN: Unsupervised Video Retargeting
[21]

![](figures/683-3-FIGURE.jpg)

图10.57 RecydleGAN算法的演进

$$
( \overline{{{\S}}} )+\overline{{{\Re}}} ) \equiv\colon B e c y c l e-G A N : U n s u p e r v i s e c \, \rangle
$$
Vlidto Retargeting

(1）Pix2Pi×是成对（Paired data）转换。
(2）CycleGAN 是循环转换，使用成对的网络架构。 (3）RecycleGAN 加上时间同步的相关性 $( P_{x \smallsetminus} \ P_{y} )$ 

(1）Pix2Pix是成对（Paired data）转换。

$$
( 2 ) \ \ \mathrm{C y c l e G A N} \equiv1 \equiv\mathbb{I} \pm\mathbb{I} \pm\mathbb{A} \mathbb{A} \mathbb{A}, \ \ 1 \equiv\mathbb{H}
$$
月成对的网络架构

(3）RecycleGAN加上时间同步的相关性 $( P_{x^{\sim}} \ P_{y} )$ 。

另外，还有Face2Face、嘴型同步技术 $\mathrm{( L i p-s y n c i n g}$ 
technology）等算法，有兴趣的读者可以参阅Jonathan Hui的 Detect Al-generated images & Deeptakes 2l系列文章，里面有大量的图片展示，十分有趣。

Deepfake 的实践可参阅DeepFakes in.5 minutes l文，它介绍如何利用DeepFaceLab 库，在很短的时间内制作出深度伪造
源代码在「DeepFaceLab GitHub
的影片，

TMinitutorial」说明，只要按步骤执行脚本 $( \mathrm{S c r i p t s} )$ 就可以顺利完成影片，不过，它比·GAN需要更强的硬件设备，笔者并未进行测试。

Deepfake 引发了严重的假新闻灾难，许多学者及企业提出反制的方法来辨识真假，简单的像是 $\boldsymbol{D}$ etect Al-generated lmages & Deepfakes (Pat1) a文所述，可以从脸部边缘是否模糊，是否有随机的噪声，脸部是否对称等细节来辨别，当然也有大 $\big< \AA$ 司推出可辨识影片真假的工具，如微软的「Microsoft Video
Authenticator，读者可参阅ITHome 相关的报导”，

不管是Deepiake还是反制的算法，未来发展都值得关注，这起事件也让科学家留意到科学的发展必须兼顾伦理与道德，否则，

## 因此整体的 ReoyoleGAN的损失函数为

$$
\begin{aligned} {} & {{} \operatorname* {m i n}_{G, P} \operatorname* {m a x}_{D} L_{r y} ( G, P, D )=L_{g} ( G_{X}, D_{X} )+\overbrace{L_{g} ( G_{Y}, D_{Y} )+}^{\mathrm{G a N o l l e p e c h r e}}} \\ {} & {{} \lambda_{r x} L_{r} ( G_{X}, G_{Y}, P_{Y} )+\lambda_{r y} L_{r} ( G_{Y}, G_{X}, P_{X} )+\lambda_{r x} L_{r} \underbrace{( P_{X} )+\lambda_{r y} L_{r} ( P_{Y} )}_{\mathrm{f l e c u r e r t i o s s}}} \\ \end{aligned}
$$

tps:/github.coiperov/DeepFacelab），网页附有一个视频好莱坞科幻片的剧情就不再只是幻想，人类有可能走向自我毁灭的道路

自然语言处理（Natural Language Processing, NLP）顾名思 $\mathcal{X}$ ，就是希望计算机能像人类一样，看憧文字或听懂人话，理解语意，并能给予适当的回答。如下图所示，以聊天机器人为例，一个简单的计算机与人类的对话，所涵盖的技术就包括以下几项

(1）当人对计算机说话，计算机会先把这句话转成文字，称之为语音识别（Speech recogniion）或语音转文字（Speech To Text, STT)

(2）接着计算机对文字进行解析，了解意图，称为自然语言理解。

3以文字回复：从语料库或常用问答（FAQ）中找出一段要回复的文字，这部分称为文字生成（Text Generation

® $\operatorname{l o g}$ 声音回复：将回复文字转为语音，称为语音合成 (Speech Synthes $\mathrm{i z e )}$ 或文字转语音（Text To Speech, TTS)

## 第四篇自然语言处理

![](figures/686-6-FIGURE.jpg)

图聊天机器人概念示意图

(3）之后计算机依据对话口答，有以下两种表达方式

整个过程看似容易，实则充满了各种挑战，接下来我们把相关技术仔细演练一遍吧！

自然语言处理的发展非常早，大约从1950年就开始了，当年英国计算机科学家图灵（Alan Mathison Turing）已有先见之明，提出图灵测试（Turing Test），目的是测试计算机能否表现出像人类一样的智慧，时至今 $\mathrm{\Theta}$ ，许多聊天机器人如Siri、小冰等产品的问世，才算得上真正启动了NLP的热潮，即便目前依然无法媲美人类的智慧，但相关技术仍然有许多方面的应用，举例如下。

(10）音乐方面的应用，如曲风分类、自动编曲、声音模仿等等。

## 第11章

## 自然语言处理的介绍

(1）文本分类（Text Glassification
(2)信息检索（Information Retrieval
(3）文字校对（Text Proofing)
(4）自然语言生成（Natural Language Generation） (5）问答系统（Question Answering
(6）机器翻译（Machine Translation
(7）自动摘要（Automatic Summarization)
(8）情绪分析（Sentiment Analysis
(9）语音识别（Speech Recognition）

1）文本分类（Text Classiication O (1)

(2）信息检索（ntormaion Rtrieval) O

$$
( 3 ) \, \rightleftharpoons\pm\pm\pm\mathrm{~ \bar{~} x ~} \pm\mathrm{~ ( T e x t ~ P r o o f i n g ) ~}_{\circ}
$$

(4) 然语言生成（Natural Language Generation O

(5）问答系统（Question Answering) O

(6）机器翻译（Machine Translation O

(7）自动摘要（Automatic Summarization O

8）情绪分析（Sentiment Analysis O

(9）语音识别（Speech Recognion O

 $\mathcal{A}$ 类的语言具高度模糊性，一句话可能有多重的意思或隐喻， 而计算机当前还无法真正理解语言或文字的意 $\grave{\chi}$ ，因此，现阶段的做法与对影像的处理方式类似，先将语音和文字转换成向量，再对向量进行分析或使用深度学习建模，相关研究的进展非常快，这一节我们从最简单的方法开始说起。

词袋（Bag of Words,BOW）是指把一篇文章进行词汇的整理，然后统计每个词汇出现的次数，经白前 $\mathit{\Pi}$ 名的词汇猜测全文大意，如图11.1所示。

(1）分词（Tokenization 将整篇文章中的每个词汇切开， 整理成生字表或字典 $\mathrm{: ~ ( \ r o c a b u l a r y )}$ 。英文较简兽，以空白或句点

## 11-1 词袋与TF-IDF

![](figures/689-4-FIGURE.jpg)

$$
\boxed{\xi} ~ 1 1. 1 ~ ~ ~ \mathrm{i n} \mathrm{f r} \mathrm{f r} ~ \frac{1 8} {\chi}
$$

$$
\left\{\pm x ; \pm x \right| \mathrm{T} :
$$

![](figures/689-7-FIGURE.jpg)

(2）前置处理（Preprocessing）：将词汇作词形还原，转换成小写等。词形还原是动词转为原形，复数转为单数等，避免因为
，词汇统计出现分歧。
词态不同，

(3）去除停用词（Stop Word be 动词、助动词、代名词、介系词、冠词等不具特殊意义的词汇称为停用词，将它们剔除，否则统计结果都是这些词汇出现最多次。

(4）词汇出现次数统计：计算每个词汇在文章出现的次数， 并白高至低排列。

(2）设定停用词：这里直接设定停用词，许多库有整理常用的停用词，如NLTK、spaCy。程序代码如下：

(3）读取文本文件 news.txt，统计词汇出现的次数，资料来自于South Korea's Convenience Store Culturel-文。程序代码如下:

隔开，中文较复杂，须以特殊方式处理

范例1。以BOW实践自动摘要。

请参阅程序：11 01 BOW.ipynb

(1）加载相关库。程序代码如下

1#载人相关库
2 import collections

![](figures/690-9-FIGURE.jpg)

BOW方法十分简单，效果也相当不错，不过它有个缺点，有些词汇不是停用词，也经常出现，但对全文并不重要，如上文的 on $\mathbf{y}$ 、most，对猜测全文大意没有帮助，所以，学者提出改良的算法TF-DF（Term Frequency - Inverse Document Frequency） 它会针对跨文件常出现的词汇给予较低的分数，如on $\mathbf{y}$ 在每一个文件都出现，TF-IDF 对它的评分就相对较低，因此，TF-IDF的公式定 $\grave{\chi}$ 为

1#读取文本文件news.txt，统计字词出现次数
2
3#参数设定
4 maxlen=1000 #生字表最大个数
5
6 #生字表的集合
7 word fregs = collections.Counter()
8 with open('./NLP data/news.txt','r+', encoding='UTF-8') as f: 9 for line in f:
10 #转小写、分司
11 words = line.lower().split(' ')
12 #统计字词出现久数
13 if len(words) > maxlen:
14 maxlen = len(words)
15 for word in words:
16 if not (word in stop words):
17 word freqs[word] t= 1
18
19 print(word freqs.most common(20))

## D执行结果如下

[('stores 15), ('convenience', 14), ('korean', 6), ('these', 6), ('one', 6), ('it's', 6), ('from', 5), ('my', 5), ('you', 5), ('their", 5), ('just', 5), ('has', 5), ('new', 4), ('do', 4), ('also', 4), ('which', 4), ("find', 4), ('would", 4), 'like
4),（'up',4)]

2 名分别为： 前3

$$
\bullet\quad\mathrm{s t o r e s :} \ 1 5 : \mathbb{R}_{\circ}
$$

$$
\bullet\quad\mathrm{c o n v e n i e n c e : ~ 1 4 ~ \rangle~ \mathcal{K} ~_{o} ~}
$$

$$
\bullet\quad\mathrm{k o r e a n} \colon\quad6 \not\nabla_{\circ}
$$

③因比可以猜测这整篇文章应该是在讨论韩国便利商店 (Korea Convenience Store 结果与标题契合。

(1） t (Term Frequency，词频) 考虑词汇出现在跨文件的次数，分母为在所有文件中出现的次数，分子为在目前文件中出现的汉数。有

(2) idif( nverse Document Frequency，逆向档案频率考虑词汇出现的文件数，单一文件出现特定词汇多次，也只视为 $1$ ，分子为总文件数，分母为词汇出现的文件数，加 $1$ 是避免分母为 $\mathbf{0}$ 。有

(3）除了以上的定义，TF-IDF还有一些变形的公式，可参阅维基百科关于 f-idif的说明

除了猜测全文大意外，TF-DF也可以应用到文本分类或问答的配对。

$$
t f \!-\! i d f \!=\! t f \! \times\! i d f
$$

$$
\b\equiv\mp:
$$

$$
t f_{i, j}=\frac{n_{i, j}} {\sum_{k} n_{k, j}}
$$

$$
i d f_{i, j}=\operatorname{l o g} \frac{\mid D \mid} {1+\mid D_{t_{i}} \mid}
$$

范例 $2$ ，以TF-IDF实践问答配对

请参阅程序：11 02 TFIDF.ipynb实践

(1）加载相关库。程序代码如下 (3）将例句转换为词频矩阵，计算各个词汇出现的次数。程序代码如下：

## (2）设定输入数据：最后一句为问题，其他的例句为回答

## 程序代码如下：

![](figures/693-3-FIGURE.jpg)

1#将例句转换为词频矩阵，计算各个字词出现的灭数 2 vectorizer = CountVectorizer()
3 X = vectorizer.fit transform(corpus)
4
5#生字表
6 word = vectorizer.get feature names(
7 print ("Vocabulary :", word)

## 执行结果如下：

Vocabulary and', 'document', 'first', is', one', 'second', 'the', 'third', this']

(4）查看四句话的 BOW。程序代码如下：

 $^1_{2}$ #查看四句话的BOW
print ${\bigl(}^{n} B O W=\backslash n^{n}, \ X. \operatorname{t o a r r a y} ( \, ) {\bigr)}$ 

## 执行结果如下：

\begin{array} {c} {{\mathrm{B O W}=}} \\ {{\left[ \begin{array} {l l} {{\left[ \begin{array} {l l} {{0}} & {{1}} \end{array} \begin{array} {l} {1}} \end{array} \begin{array} \end{array} \begin{array} \ l} {{1}} & {{0} & {0} & {1} \\ {\left[ \begin{array} {l} {{0}} & {{1}} \end{array} \begin{array} \end{array} \begin{array} \begin{array} {l} {{1}} & {{0} & {0} & {{1}} \end{array} \right]}} \\ {{\left[ \begin{array} {l} {{0}} & {{1}} \end{array} \right. 0} & {{1}} \end{array} \right]}} \\ {\left[ \begin{array} {l l} {{1}} & {{0}} & {{0}} & {{0}} \\ {{[ \begin{array} {l} {{1}} & {{0}} & {{0}} \\ {{1}} \end{array} \begin{array} {l} {{1}} \end{array} \begin{array} {l} {{0}} & {{0} & {{1}} \\ {{0}} & {{1}} \end{array} \right]} \right]} \end{array}}} \end{array}

（s）TF-DF转换：将例句转换为TF-IDF。程序代码如下： 执行结果：每一个元素均介于[0， $1 ]$ ，为了显示整齐，取四舍五 $\lambda$ ，实际运算并不需要。执行结果如 $\mathsf{F}$ :

 $( 6 )$ 比较最后一句与其他例句的相似度：以cosine similarity 比较向量的夹角，越接近 $1$ ，表示越相似。程序代码如下

执行结果：第一个例句与最后的问句最相似，结果与文意相符 $\overbrace{\Pi}$ 。结果如下

| TF-IDF= |
| [[0. | 0.438 | 3 0.542 | 0.438 | 30. | 0. | 0.3587 | 0. | 0.4388 |
| [0. | 0.2723 | 0. | 0.2723 | 0. | 0.8532 | 0.2226 C | 0. | 0.2723] |
| 0.5528 3 | 0. | 0. | 0. | 0.5528 | 0. | 0.2885 C | 0.5528 | 0. ] |
| [0. | 0.4388 | 0.542 | 0.4388 3 | 0. | 0. | 0.3587 C | 0. | 0.4388]] |

| # 最后一句与其他句的相似度化较 |
| 2 from sklearn.metrics.pairwise import cosine similarity |
| 3 print (cosine similarity(tfidf[-1], tfidf[:-1], dense e output=False)) |

| (0, | 2) | 0.1034849000930086 |
| (0, | 1) | 0.43830038447620107 |
| (0, | 0) | 1.0 |

传统上，我们会使用NLTK（Natural Language Toolkit）库来进行词汇的前置处理，它具备非常多的功能，并内含超过50个语料库（Corpora）可供测试，只是它不支持中文处理，比较新的 spaCy 库支持多国语系。这里先示范如何运用NLTK做一般词汇的前置处理，之后再介绍可以处理中文数据的库。

 $( 2 )$ 安装 NLTK数据：先执行python，再执行import nlik ntk.download()，出现页面如图11.2所示。它包括库与相关语料库，可下载必要的项目。

白于文件众多，下载时间很久，如需安装至第二台计算机，可直接复制下载目录至其他PC 即可，NLTK加载语料库时，会自动

## 11-2 词汇前置处理

NLTK分为程序和数据两个部分

(1）安装NTK 程：P p nslalnik

![](figures/695-6-FIGURE.jpg)

图11 $. 2$ 安装NLTK数据

检查所有计算机的\ntk dcata目录

范例。使用NLTK进行词汇的前置处理

请参阅程序： 03 词汇前置处理.ipynb
11

(1）加载相关库。程序代码如下

1 $^\sharp$ 都人街会司 $~ 2$ import

(2）输入测试的文章段落程。序代码如下：

1#测试文章段落
2 text="Today is a great day. It is even better than yesterday." + \ "And yesterday was the best day ever."

）将测试的文章段落分割成例句。程序代码如下 (3)

 $^1_{2}$ 上区上家dickekilceo nltk.sent

执行结果：分割成三句。结果如下：

['Today is a great day."
It is even better than yesterday.
And yesterday was the best day ever.']

(4）分词（Tokenize 。程序代码如下：

 $1_{2}$  $\# < \frac{1} {2} \overrightarrow{\frac{1} {2}}$ 
nltk.word tokenize(text)

执行结果如下： D依字根作词形还原（Stemming 速度快，但不一定正确。

2依字典规则做词形还原（Lemmatization 速度慢，但准确率高。

(6）字根词形还原根据一般文法规则，不管字的含义，直接进行字根词形还原，如keeps 删去 $\mathbf{S}$ 、crashing 删去ing都正确但his直接删去 $\mathbf{s}$ 就会发生错误。程序代码如下：

(7）依字典规则的词形还原：查询字典，依单字的不同进行词形还原，如此his、daily均不会改变。程序代码如下：

['Today'
is
a
great
'day
$$
\begin{array} {l} {{\mathrm{I t ~^{\prime} ~},}} \\ {{\mathrm{I s ~^{\prime} ~},}} \\ {{\mathrm{I s ~},}} \\ {{\mathrm{I s ~},}} \\ {{\mathrm{I s ~},}} \\ {{\mathrm{I s ~},}} \\ {{\mathrm{b e t t e ~},}} \\ {{\mathrm{I s t e ~},}} \\ {{\mathrm{I s ~},}} \\ {{\mathrm{I s ~ t e ~},}} \\ {{\mathrm{I s ~},}} \\ {{\mathrm{I s ~},}} \\ {{\mathrm{I s ~},}} \\ {{\mathrm{A n d ~},}} \end{array}
$$
was
'the
'best
'day"
ever
']

(5）词形还原有以下两类方法

![](figures/697-6-FIGURE.jpg)

执行结果：his → hi, daily-》 dail结果如下：

Invalid XML input: no rows found.

(8）分词后剔除停用词：nlik.corpus.stopwords.words
$$
( \mathrm{\^{\prime} e n g l i s h^{\prime}} )
$$
提供常用的停用词，另外标点符号也可以列入停用
词。程序代码如下：

## 执行结果：完全正确。结果如下：

'My system keep crashing his crashed yesterday, ours crash daily'

![](figures/698-3-FIGURE.jpg)

## 执行结果如下：

标点符号:!"#$%& $( 1 )^{*}+,-. / : ; <=>? \textcircled{Q} [ \cdot] \sim\{1 \} \sim$ 
'Today great day It even better yesterday And yesterday best day ever

（9）进行BOW统计。程序代码如下： 执行结果：同样可以抓到文章大意是韩国便利超商。结果如下：

(10）改用正规表达式（Regular Expression 上段程序还是有标点符号未删除，正规表达式可以完全剔除停用词。程序代码如下：

(1）找出相似词（Synonyms) WordNet语料库内含相似词、相反词与简短说明。程序代码如下：

![](figures/699-3-FIGURE.jpg)

$$
\mathrm{T} :
$$

35),('stores', 15), ('convenience', 14), ('one', 8), ('-', 8),（'even 8), ('seoul', 8), ('city', 7),('korea', 6), 'korean', 6), ('cities', 6), ('people',5), summer 4),（'new 4), ('also',4), ('find', 4),（'store 4), ('would', 4), 'like', 4), ('average', 4)]

![](figures/699-6-FIGURE.jpg)

![](figures/699-7-FIGURE.jpg)

(14）找出相反词（Antonyms) 需先调用 emmas()进行词形还原，再调用 antonyms()。程序代码如下：

执行结果：列出前10名，是以例句显 $\overline{{\Pi}}$ ，故许多单字均相同。结果如下：

| nset('love.n.01'), |
| nset('love.n.02'), |
| nset('beloved.n.01'), |
| nset('love.n.04'), |
| nset('love.n.05'), |
| nset('sexual love.n.02'), |
| nset('love.v.01'), |
| nset('love.v.02"), |
| nset('love.v.03') |
| nset('sleep together.v.01') |

12) 显示相似词说明。程序代码如下

 $^1_{2}$ #单字说明
$$
\mathsf{s y n o n y m s} \left[ 0 \right]. \mathsf{d e f i n i t i o n ( A )}
$$

执行结果：列出第一个相似词的单字说明。结果如下：

a strong positive emotion of regard and affection

（13）显示相似词的例句。程序代码如下：

1#单字的例句
2 synonyms[0].examples()

$$
\begin{array} {l l} {1} & {\# \not\equiv\pm\pm\emptyset/ \not\Xi/ \ `} \\ {2} & {\mathrm{s y n o n y m s} \, [ 0 ] \,. \, \mathrm{e x a m p l e s} \, ( 1 )} \\ \end{array}
$$

执行结果：列出第一个相似词的例句。结果如下：

love for his work', 'children need a lot of love']

1#龙出相反词(Antonyms
2 antonyms=[]
3 for syn in nltk.corpus.wordnet.synsets('ugly'):
4 for l in syn.lemmas():
5 if l.antonyms():
6 antonyms.append(1.antonyms()[0].name()） 7 antonyms

（15）分析词性标签（POS Tagging） 依照句子结构，显示每个单字的词性。程序代码如下：

执行结果：ugly → beautiful

1#龙出词性标签(POS Tagging) terrible things $2$ text='I am a human being, capable of doing
3 sentences=nltk.sent tokenize(text)
4 for sent in sentences:
5 print(nltk.pos tag(nltk.word tokenize(sent)))

## 执行结果如下：

[('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), （'human" 'JJ'),('being', 'VBG'),（', ,'), ('capable', ']J'), ('of', 'IN'), ('do ing', 'VBG'),（"terrible', ']J'), ('things 'NNS')]

词性标签列表如下

(1） CC(Coordinating Conjunction）：并列连词。
$$
\mathrm{( 2 ) \hspace{6 p t} C D \hspace{6 p t} ( C a r d i n a l \ n u m u )}
$$
：基数。
(3） DT（Determiner）：量词。
(4）EX（Existential）：存在地，如There。
$$
{\bf( 5 )} ~ \mathrm{F W ~ ( F o r e i g n ~ W o r d )}
$$
：外来语
(6）IN Preposition/Subordinaing Conjunction：介词 (7） JJ Adjective：形容词。
(8） JR Adjecive， Comparative:比较级形容词。 (9） JJS Adjiective，Superlative：最高级形容词。 (10)LS（List Marker） 1：列表标记

$$
( 1 ) \textit{C C} \mathrm{( C o o r d i n a t i n g ) C o n j u n c t i o n}
$$
）：并列连词。

$$
\mathrm{( 2 ) ~ \ C D ~ \left( C a r d i n a l ~ D i g i t \right) ~ : ~ \frac{\pm~} {2} ~}
$$
故

(3）DT（Determiner）：量词。

(4）EX（Existential）：存在地，如There

$$
\mathrm{( 5 ) ~ F W ~ ( F o r e i g n ~ W o r d ) ~ : ~ \partial l : \sharp~ i \frac{\hbar} {2} ~,}
$$

$$
\mathrm{( 6 ) ~ \amalg P r e p o s i t i o n / S u b o r d i n a t i n g ~ C o t}
$$
iunction：介词。

$$
( 7 ) \mathrm{~ J J ~ A d j e c t i v e :}
$$
形容词

8) ，Comparative：比较级形容词 JJR Adjective,

(9) JJS Adjective，Superlative:最高级形容词

$$
( 1 0 ) ~ ~ \mathrm{L S} ~ ~ ( \mathrm{L i s t ~ M a r k e r} ) ~ ~ 1 : ~ \sharp\mathrm{J \pm~ k \hbar~ i \Xi_{\circ} ~}
$$

$$
\mathrm{( 1 6 ) \; \mathrm{~ P D T ~ \ ( P r e d e t e r m i n )}} \, \mathrm{l o t ~ o f_{o}}
$$
er 放在量词的前面，如both、a

(19) PRP$ Possessive Pronoun：所有格代名词，如my his、hers.

（1I）MD （Moda）：情态动词。

(12)NNNoun, Singular：名词单数

$$
( 1 3 ) \ \mathrm{N N S \ N o u n \ P l u r a l : \ \ Z i \overline{{v}}} ] \not\equiv\mathbb{F}_{\circ}
$$

(14)NNPF Singular：专有名词单数。

$$
( 1 5 ) \ \mathrm{N N P S ~ P r o p e r ~ N o u n}, \ \ \mathrm{P}
$$
lural：专有名词复数

$$
( 1 7 ) \, \mathrm{~ P O S ~} \, ( \mathrm{P o s s e s s i v e ~ E n d i n g} ) \, : \, \mathrm{~ \mathbb{P} P S ~}
$$
新有格,parent's.

(18） PRP （Personal Pronoun）：代名词，如 $\grave{\mathrm{I}}$ he、she.

(20）RB Adverb：副词，如very、silently.
(21）RBR Adverb,Comparative:比较级副词，如etter。 (22） RBS Adverb，Superlative: 最高级副词，如best (23）RParice:助词，如give up。
(24）TOto：例如go『to』 the store.
(25）UHInterjection：感叹词，errrm.
(26） VB Verb，Base Form：动词，如take

(20）RB Adverb：副词，如veryssilently。

(21）RBR Adverb,， Comparative：比较级副词，知eter.

(22)RBS Adverb,Superlative 最高级副词，如best.

(23）RP Partiole：助词，如give up

(24)TOto:例如go 『io thestore.

(25) UH lnterjection：感叹词，errrrm.

$$
( 2 6 ) \ \ \mathrm{V B} \ \mathrm{V e r b}
$$
，Base Form：动词，如take

$$
\begin{array} {r r} {( 2 8 ) \ \ \mathrm{V B G} \ \mathrm{V e r b}, \ \ \mathrm{G}} \\ {\ne0 \ \mathrm{t a k i n g}_{\circ}} \\ \end{array}
$$
ierund/Present Participle：动词进行式，

$$
( 3 0 ) ~ \mathrm{V B P ~ V e r b},
$$
Sing Present non-3d：动词现在式单
数，如take

$$
( 3 1 ) ~ \mathrm{V B Z ~ V e r b},
$$
3rd person sing. present：动词现在式复
数，如takes

(34） WP$ possessive wh-pronoun：疑问代名词所有格，如 whose

(27） VBD Verb， Past Tense：动词过去式，如took

(29） VBN Verb，Past Participle：动词过去分词，如 taken.

(32)WDT wh-determiner：疑问代名词，如whioh

$$
( 3 3 ) \ \ \mathrm{W P ~ w h-p r o n o u n ~ w h o}, \ \ \mathrm{w i t h}
$$
hat：疑问代名词

(35) WRB wh-abverb：疑问副词，如where、when.

spaCy库提供更强大的分析功能，但由于内容涉及词向量 (Word2Vec) 所以我们留待后续章节再讨论。

BOW 和TF-IDF都只着重于词汇出现在文件中的次数，未考虑语言、文字有上下文的关联，比如，「这间房屋有四扇？」，从上文大概可以推测出最后一个词汇是「窗户」，又如，我说喜欢吃辣，那我会点「麻婆豆腐」还是「家常豆腐」呢？相信听到「吃辣」，应该都会猜是「麻婆豆腐」。另一方面，一个语系的单字数有限，中文大概就几万个字，我们是否也可以比照影像辨 $\grave{\imath} \P$ ，对所有的单字建构预先训练的模型呢？之后是否就可以实现转换学习 (Transter Learning）呢？

针对上下文的关联，Google 研发团队 Tomas Mikolov等 $\mathcal{A}$ 于 2013年提出词向量（Word2Vec），他们搜集了1000亿个字
(Word）加以训练，将每个单字改以上下文表达，然后转换为向量，而这就是词嵌入（Word Embedding）的概念，与TF-IDF输出是稀疏向量不同，词嵌入的输出是一个稠密的样本空间。

(1）连续CBOW（Continuous Bag-of-Words) 以单字的上下文预测单字。

$$
\mathrm{~ F \times~}_{\circ} ( 2 ) \mathrm{~ C o n t i n u o u s ~ S k i p-g r i g m}
$$
am Model：刚好相反，以单字预测上

## 11-3 词向量

词向量有以下两种做法，如图11.3所示揭开CBOW算法来看，它就是一个深度学习模型，如图11.4 所示

以单字的上下文为输 $\lambda$ ，以预测的单字为目标，如同下面 $2-$ gran的模型，例句为 $\Gamma$ Hey, this is sample corpus using only one context word.」使月One-hot encoding，输出表格如图11.5 所示。

![](figures/705-2-FIGURE.jpg)

图 $1 1. 3$ CBOW 与 Continuous Skip-gram Model

(图片来源：Exyploiing Similaries among Languages fon Machine Transtationl3

![](figures/705-5-FIGURE.jpg)

11.4 CBOW的网络结构冬

(图片来源：An Intuitive Understanding of Woro Embedcings: From Count Vectos to Word2Vec'l) 2-gram是每次取两个单字，然后窗口滑动一个单字，输出如图 11.6 所示。

接着以第一个单字 One-hot encoding为输 $\lambda$ ，第二个单字为预测目标，最后模型预测的是各单字的概率。这是一个简略的说明，当然，实际的模型不会这么简单，还会额外考虑以下状况

$$
( 1 ) \pi\unlhd\exists\pm\pm\pm\cdots
$$
个单字，会将上下文各 $\boldsymbol{n}$ 个单字都纳 $\lambda$ 考

(2）如此做法，输出是1000亿个单字的概率，模型应该无法承担如此多的类别，因此改用所谓的负样本抽样（Negaive Sub-samping），只推论输出输入是否为上下文，例如 orange
juice， $\mathcal{M} P$ (juiceorange）改为预测 $\boldsymbol{P}$  $( 1 )$ <orange, juice>），即从多类别 $( 1 0 0 0 \langle\mathbb Z \uparrow)$ 模型转换成二分类（真/假）模型。

| 1Rnt | Output thic  | natannint1 | Mo | Thie | is 0 | GAmnIA | corpus 0  | using 0 | Anlw |  | context n  | word n | APE Hov | 1 | n | SS n  | S 0 | UE 0 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| thic | M ihov | 一 natannint9 | 一 n | “ 1 | 5 0 | n | “ n | 5 0 |  n | 二 0 | 5 0 |  n |
| U ic | thic |  P-Datsnmint2 | 5 0 | 一 n | 5 1 |  n |  -0 | 5 0 | 5 n | ← n | 0 | 5 n |
| 5 ic | cAmnio | CCCPL natannintA | 5 n | 5 n | 一 1 | 5 n | 5 n | 5 n | 5 n | ← n | n | 5 n |
| 5 camno | SCPC ie  | CCCoL natannintR | ， n | ← n | + n | ， 1 | ， n | n | 4 n | ← n | n | ， n |
| SCC camnlo | O CArUIG | CCCPL natsnaintR |  n | 4 n | 5 n | 一 1 | ， n | n |  n | 5 n | n | ， n |
| SIPC | SIPCS camnlo | CCCPOUL natannint7 |  n | n | 5 n | 一 n | ， 1 | n |  n | ← n | n | ， n |
| corpus | using | Datapoint 8 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 |
| using | corpus | Datapoint 9 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 |
| using | only | Datapoint 10 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 |
| only | using | Datapoint 11 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 |
| only | one | Datapoint 12 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 |
| one | only | Datapoint 13 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 |
| one | context | Datapoint 14 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 |
| context | one | Datapoint 15 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |
| context | word | Datapoint 16 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 |
| word | context | Datapoint 17 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 |

11.5 2-gram 5 One-hot encoding 冬

![](figures/706-6-FIGURE.jpg)

11.6 滑动窗二冬 2-gram

(1）简单，而且比传统确定性模型（Deterministic methods 的效能更佳。

(1）例如， $\mathrm{A p p l e}$ 可能是指水果，但也可能是在指 $\big< \lambda$ 司名称遇到这样的情 $\gg\Pi$ ，CBOW 会取平均值，导致失准，故CBOW无法处理一字多 $\mathcal{X}$ .

(2）因为 CBOW输出高达1000亿个单字概率，所以优化求解的收敛十分困难。

因此，后续发展出 $\overline{{\j}}$ Skip-gram模型，颠倒输出与输 $\lambda$ ，改白单字预测上下文，我们再以同样的句子来举例，如图11.7所示。

(1）一个单字可以预测多个上下文，解决 $\overline{{\jmath}}$ 一字多义的问题。

CBOW的优点如下。

(2）对比相关矩阵，CBOW的内存消耗节省了很多

 $\mathrm{C B O W}$ 的缺点如下。

| Input | Output(Context1) | Output(Context2) |
| --- | --- | --- |
| Hey | this | <padding> |
| this | Hey | is |
| is | this | sample |
| sample | is | corpus |
| corpus | sample | corpus |
| using | corpus | only |
| only | using | one |
| one | only | context |
| context | one | word |
| word | context | <padding> |

11.7 Skip-gram 模型的输出与输入冬

$$
\mathrm{S k i p-g r a m} \left\vert\left\vert\right\vert\right\vert\hbar\vert\equiv\vert\mathrm{I I T}_{\circ}
$$

(2）结合负样本抽样技术，效果比其他模型佳。负样本可以是任意单字的排列组合，如果要把所有负样本放入训练数据中，数量可能过于庞大，而且会产生不平衡数据（Imbalanced Data） 因此采用负样本拍样的方法。因我们不实践 Skip-gram 模型训练， 细节就暂且不介绍，有兴趣的读者可参阅NLP 102: Negative
$$
\mathit{S a m p l i n g \, a n d \, G l o V e^{[ 5 ]}}_{\circ}
$$

我们可以利用预先训练的模型来实验 $\grave{z}-\mathrm{T}$ ，Gensim 和 spacy 库均提供 Word2Vec模型

使用前须先安装Gesim p: p install gensim.

范例1. 进行相似性比较运用Gensim

设计流程如图11.8所示

![](figures/708-5-FIGURE.jpg)

图11 $8$ 设计流程

请参阅程序：11 ，相似性比较.ipynb。

1）加载相关库。程序代码如下 ## (2）测试的文章段落如下。程序代码如下

| 1 | #语料库 |
| --- | --- |
| 2 | documents=[ |
| 3 | "Human machine interface for lab abc computer applications" |
| 4 | "A survey of user opinion of computer system response time" |
| 5 | "The EPS user interface management tsystem" |
| 6 | "System and human system engineering testing of EPS", |
| 7 |  |
| 8 | "The generation of random binary unordered trees", |
| 9 | "The intersection graph of paths  |
| 10 | "Graph minors IV Widths of trees and well quasi ordering", |
| 11 | "Graph minors A survey", |
| 12 | 1 |

## 3）分词、前置处理。程序代码如下：

1#在意设定一些停用词
2 stoplist = set('for a of the and to in'.split())
3
4#分词，转小写
5 texts = [
6 [word for word in document.lower().split() if word not in stoplist] 7 for document in documents
8 ]
9 texts

## 执行结果如下：

[['human', machine', 'interface', 'lab', 'abc', computer', 'applications']. ['survey', 'user', 'opinion', 'computer' system', 'response', 'time'],
['eps', user 'interface management system'],
[' system','human" system' 'engineering', 'testing', "eps']
['relation', user 'perceived', 'response', 'time', 'error', 'measurement'], ['generation', 'random', "binary unordered', 'trees'],
['intersection', 'graph', 'paths', 'trees'],
['graph', 'minors' 'widths 'trees', 'well', 'quasi', 'ordering'],
['graph', 'minors', 'survey']]

## (4）单字出现的次数统计。程序代码如下

 $1$ #单字出现灭数统计
frequency = defaultdict(int) $3$ for text in texts:
4 for token in text:
 $5$ frequency[token] += 1 $6$ frequency

执行结果：执行结果如下显示每个单字出现的次数。 (5）删除只出现 $\longrightarrow$ 次的单字：仅专注在较常出现的关键词程序代码如下：

## (5）删除只出现一次的单字：仅专注在较常出现的关键词。

| 5-53 {'human':2, 'machine': 1, 'interface':2, 'lab':1, 'abc':1, 'computer':2, 'applications': 1, 'survey':2, 'user':3, 'opinion': 1, 'system': 4, 'response': 2, 'time':2, 'eps':2, 'management': 1, 'engineering': 1, 'testing': 1, 'relation': 1, 'perceived': 1, 'error': 1, 'measurement': 1, 'generation':1, |

| 1 | #力删除只出现一火的单字 |
| 2 | texts= |
| 3 | [token for token in text if frequency[token]> 1] |
| 4 | L for text in texts  |
| 5 |  |
| 6 | texts |

执行结果：每句筛选的结果如下

[['human', 'interface', 'computer'],
['survey' user', "computer', system', 'response', 'time'], ['eps', 'user', 'interface', 'system'],
['system' 'human' 'system' 'eps'],
['user', 'response', 'time'],
['trees'],
['graph', 'trees']
['graph', 'minors', 'trees'],
['graph', 'minors', 'survey']]

## (6）转为BOW。程序代码如下：

1#转为字典
2 dictionary = corpora.Dictionary(texts)
3
4#转为BOw
5 corpus = [dictionary.doc2bow(text) for text in texts] 6 corpus

## 执行结果如下：

(7）建立LS（Latent semantic indexing）模型：可指定议题的个数，每一项议题皆白所有单字加权组合而成。程序代码如下

执行结果：将例句带入到两项议题公式中，计算LSl值，结果比较接近第一项议题。结果如下

$$
\begin{matrix} ( 0 ) \lfloor\lfloor\lfloor\hat{x} \rfloor\rfloor\rfloor=\frac{\pm} {\pm} \pm\frac{\pi} {\pm} \left\{\begin{matrix} \\ \pm\end{matrix} \right. \\ \exists=\pm\frac{\pi} {\pm} \left\{\begin{matrix} \\ \end{matrix} \right. \\ \exists=\frac{\pi} {\pm} \left\{\begin{matrix} \end{matrix} \end{matrix} \right. \\ \exists=\frac{\pi} {\pm} \left\{\begin{matrix} \end{matrix} \end{matrix} \right.
$$
落内每一个句子的相似概率。程序代

![](figures/711-3-FIGURE.jpg)

![](figures/711-4-FIGURE.jpg)

## 执行结果：两项议题的公式如下。

![](figures/711-6-FIGURE.jpg)

## (8）测试LS|模型。程序代码如下

1#例句
2 doc = "Human computer interaction
3
4#测试LSI（Latent semantic indexing）模型
5 vec bow = dictionary.doc2bow(doc.lower().split()) 6 vec lsi = lsi[vec bow]
7 print(vec lsi)

$$
[ \, ( \emptyset, \, \, \, \emptyset. 4 6 1 8 2 1 0 0 4 5 3 2 7 1 5 7 \, ) \,, \, \, \, ( 1 \,, \, \, \,-\emptyset. 0 7 0 0 2 7 6 6 5 2 7 9 0 0 0 6 7 ) \, ]
$$

执行结果：将例句带入到两项议题 $\angle\searrow$ 式中，计算LSl值。结果如 $\mathrm{T}$ :

执行结果：前两句概率最大，依语意判断结果正确无误。结果如下：

Gensim 不仅提供 Word2Vec预先训练的模型，也支持自定义数据训练的功能，预先训练的模型可提供一般内容的推论，但如果内容是属于特殊领域，则自行训练模型会比较恰当，Gensim
Worcd2Vec的用法请参考「Gensim 官网 Word2Vec 说明文件」 .(0, 0.998093), (1, 0.93748635), (2, 0.9984453), (3, 0.98658866), (4, 0.90755945), (5, -0.12416792), (6, -0.1063926),(7, -0.09 879464),(8,0.05004177)]

## 10）按照概率进行降序排序。程序代码如下

1#依相似概率隆序排序
2 sims = sorted(enumerate(sims), key=lambda item: -item[1]) 3 for doc position, doc score in sims:
4 print(doc_score, documents[doc position])

| 0. | 453 B The EPS user interface management system |
| 0. | 93 Human machine interface for lab abc computer applications |
| 0. | 8866 System and human system engineering g testing of EPS |
| 0. | 3635 5Asurvey of user opinion of computer system response time |
| 0. | 5945 Relation of user perceived response etime to error measurement |
| 0. | 4177 Graph minors A survey |
| -0 | 79464 4Graph minors IVl Widths of trees and well quasi ordering |
| -0 | 8926 The intersection graph of paths in trees |
| -0 | 16792 The generation of random binary unordered trees |

范例 $2$ .运用 Gensim 进行 Word2Vec训练与测试

个执行结果：传回两个值 $( 0, 6 )$ 包括所有执行周期的有效字数与总处理字数，其中前者为内部处理的逻辑，后者数字为6-3 个单字×2个执行周期。

请参阅程序：1
$$
\mathrm{1. 0 5 \_g e n s i m \_W o r d 2 V e c. i p y n b.}
$$

(1）加载相关库。程序代码如下

(2）以Gensim进行简单测试：把Gensim 内建的语料库 common texts作为训练数据，并且对「hello」world」
「michae」三个单字进行训练，产生词向量。程序代码如下：

![](figures/713-4-FIGURE.jpg)

②train)的参数有很多，可参阅上面所提的「Gensim 官网 Word2Vec 说明文件」 这里仅摘录此范例所用到的参数。

sentences：训练数据。
size：产生的词向量大小。
window：考虑上下文各自的长度。
min count：单字至少出现的次数。
workers：线程的个数。

$$
\bullet\mathrm{s e n t e n c e s : ~ i l l e \pm~ \pm~ \pm~ x \pm~ k l e}
$$

size：产生的词向量大小。

window：考虑上下文各自的长度。

min count：单字至少出现的次数

workers：线程的个数

(4）实例测试：载入OpinRank语料库，文章内容是关于车辆与旅馆的评论。程序代码如下：

(5）读取OpinRank 语料库，并进行前置处理，如分词。程序代码如下：

## （3）另一个例子。程序代码如下：

1 sentences = [["cat", "say", "meow"], ["dog", "say", "woof"]]
3 model simple = gensim.models.Word2Vec(min count=1)
4 model simple.build vocab(sentences）#建立生字表(vocabulary）
5 model simple.train(sentences, total examples=model simple.corpus count
, epochs=model_simple.epochs)

执行结果：传回 $( 1, 3 0 )$ ，其中30-6个单字 $\times5$ 个执行周期

![](figures/714-4-FIGURE.jpg)

## 执行结果如下：

b"Oct 12 2009 \tNice trendy hotel location not too bad.\tI stayed in this hotel for one night. As this is a fairly new place so me of the taxi drivers did not know where it was and/or did not want to drive there. Once I have eventually arrived at the hote l, I was very pleasantly surprised with the decor of the lobby/ground floor area. It was very stylish and modern. I found the r eception's staff geeting me with "Aloha' a bit out of place, but I guess they are briefed to say that to keep up the coroporate image.As I have a Starwood Preferred Guest member, I was given a small gift upon-check in. It was only a couple of fridge magne ts in a gift box, but nevertheless a nice gesture.My room was nice and roomy, there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by 'bliss'.The location is not great. It is at the last met ro stop and you then need to take a taxi, but if you are not planning on going to see the historic sites in Beijing, then you w ill be ok.I chose to have some breakfast in the hotel, which was really tasty and there was a good selection of dishes. There a re a couple of computers to use in the communal area, as well as a pool table. There is also a small swimming pool and a gym ar ea.I would definitely stay in this hotel again, but only if I did not plan to travel to central Beijing, as it can take a long time. The location is ok if you plan to do a lot of shopping, as there is a big shopping centre just few minutes away from the hotel and there are plenty of eating options around, including restaurants that serve a dog meat!\t\r\n"

| 1 9 | #读取OpinRank 语料库，并作前置处理 dafnasd innut/innut f110.  |
| 3 | 一 with gzip.open n (input file, 'rb') as f: |
| 4 | for i, line in enumerate (f): |
| 5 | #前置处理 |
| 6 | yield gensim.utils.simple preprocess(line) |
| 7 |  |
| 8 | #载人 OpinRank 语料库，分司 |
| 9 | documents = list(read input(data file)) |
| 10 | documents |

执行结果：结果如下，为一个List

执行结果：
$$
( 3 0 3, 4 8 4, 2 2 6, 4 1 5, 1 9 3, 5 8 0 )
$$
，处理了数亿个单
字

[['oct'
'nice
'trendy'
'hotel'
'location
$$
\begin{array} {r l} {{}} & {{\mathrm{n o t ~^{\prime} ~},}} \\ {{}} & {{\mathrm{r o o t ~^{\prime} ~},}} \\ {{}} & {{\mathrm{r o d ~},}} \\ {{}} & {{\mathrm{r o d ~},}} \\ {{}} & {{\mathrm{r o t a y e d ~^{\prime} ~},}} \\ {{}} & {{\mathrm{r o t a t ~},}} \\ {{}} & {{\mathrm{r o t a ~},}} \\ {{}} & {{\mathrm{r o t a ~},}} \\ {{}} & {{\mathrm{r o t a ~^{\prime} ~},}} \\ {{}} & {{\mathrm{r o t a ~^{\prime} ~},}} \\ {{}} & {{\mathrm{r o r ~^{\prime} ~},}} \end{array}
$$
'night
as
'this'

(6）Word2Vec模型训练：大约需10分钟。程序代码如下：

1 # Word2Vec模型训练，约10分钟
2 model = gensim.models.Word2Vec(documents, size=150, window=10, 3 min count=2, workers=10)
4 model.train(documents,total examples=len(documents),epochs=10)

接下来我们进行各种测试。

(7）测试「dirty」的相似词。程序代码如下：

1#测试·航脏相以词
2 $w {\bf1} ~=~^{n} d i r t y^{n}$ 
3 model.wv.most similar(positive=w1) # positive：相似词

执行结果：显示10个最相似的单字。结果如下：
品

[('filthy', 0.8602699041366577),
('stained', 0.7798251509666443)，
('dusty'， $\diamond$ .7683317065238953)，
('unclean', 0.7638086676597595),
('grubby', 0.757234513759613)，
('smelly', 0.7431163787841797),
('dingy', $\diamond$ .7304496169090271)，
('disgusting', 0.7111263275146484), ('soiled', 0.7099645733833313)，
('mouldy', 0.706375241279602)]

(8）测试「rance」的相似词：toon可指定列出前 $\boldsymbol{n}$ 名。程序代码如下：

执行结果：显示 $6$ 个最相似的单字。结果如下：
显示个

[('germany 0.6627413034439087), canada'， $\big tharpoondown$ .6545147895812988), spain', $\diamond$ .644172728061676), england', $\diamond$ .6122641563415527) $\epsilon$ 'mexico 0.6106705665588379)， $\epsilon$ 'rome', $\big$ .6044377684593201)]

(9）同时测试多个词汇：「床、床单、枕头」的相似词与 「长椅」的相反词。程序代码如下

1#测试·床、床单、枕兴相似词及“长椅’相友词
2 w1= ["bed",'sheet','pillow']
3 ${\sf w 2} \ =\ [$ 'couch']
4 model.wv.most similar (positive=wl, negative=w2, topn=10) # negative：相反词

## 执行结果： 显示10个最适合的单字。结果如下：

[('duvet', 0.7157680988311768),
('blanket', $\diamond$ .7036269903182983)， ('mattress', $\diamond$ .7003698348999023), ('quilt', 0.7003640532493591)，
('matress', $\diamond$ .6967926621437073)， ('pillowcase', $\omicron$ .665346086025238), ('sheets', 0.6376352310180664)， ('pillows', $\diamond$ .6317484378814697), ('comforter', $\varrho$ .6119856834411621), ('foam', $\diamond$ .6095048785209656)]

10）比较两个词汇的相似概率。程序代码如下

1#比较两词相似概率
2 model.wv.similarity(w1="dirty",w2="smelly")

执行结果：相似概率为0.7431163

11）挑选出较不相似的词汇。程序代码如下：

(12）接着测试加载预先训练模型，有以下两种方式：程序直接下载或者手动下载后再读取文件。

②手动下载后加载，预先训练模型的下载网址为
https:/drive.google.com/fle/d/0B7XkCwpl5KDYNNUTTISS21pQm M/edit。程序代码如下：

执行结果：france.

D程序直接下载。程序代码如下：

1#下载预先训练的模型
 $~ 2$ import gensim.downloader as api
3 wv = api.load('word2vec-google-news-300

1#载人本机的预先训练模型
2 from gensim.models import KeyedVectors
3
4 #每个词向量有300个元素
5 model = KeyedVectors.load word2vec format(
6 ./Word2Vec/GoogleNews-vectors-negative300.bin', binary=True)

接下来我们进行各种测试。

（13）取得dog的词向量。程序代码如下：

 $1$ #取得dog的词向量(300个元素 $2$ model['dog']

执行结果：共有300个元素。结果如下： ## (14）测试「woman, king」的相似词和「man的相反词。 程序代码如下：

执行结果：这就是有名的kig- man - woman - queen。结果如下：

执行结果：概率为0.76640123 Fwoman」和「man」是相似的。

| array([5.12695312e-02， | -2.23388672e-02,- | -1.72851562e-01， | 2 1.61132812e-01, |
| -8.447265622-02, | 5.73730469e-02， | 5.85937500e-02， | -8.25195312e-02, |
| -1.53808594e-02, | -6.34765625e-02, | 1.79687500e-01， | -4.23828125e-01, |
| -2.25830078e-02, | -1.66015625e-01,- | -2.51464844e-02， | 1.07421875e-01 |
| -1.99218750e-01， | 1.59179688e-01，- | -1.87500000e-01， | -1.20117188e-01. |
| 1.55273438e-01， | -9.91210938e-02， | 2 1.42578125e-01， | -1.64062500e-01. |
| -8.93554688e-02, | 2.00195312e-01,- | -1.49414062e-01， | 3.20312500e-01. |
| 3.28125000e-01， | 2.44140625e-02,- | -9.71679688e-02， | -8.20312500e-02 |
| -3.63769531e-02, | -8.59375000e-02,- | -9.86328125e-02, | 7.78198242e-03 |
| -1.34277344e-02, | 5.27343750e-02, | 1.48437500e-01, | 3.33984375e-01 |

## 14）测试「woman, king」的相似词和「man」的相反词

1#测试"woman，king’相以词及 $c_{m a n},$ 相反词
2 model.most similar(positive=['woman king'], negative=['man'])

[('queen', 0.7118192911148071)，
（'monarch’, $\diamond$ .6189674139022827)，
('princess', $\diamond$ .5902431011199951)，
('crown prince', $\diamond$ .5499460697174072)， ('prince', 0.5377321243286133),
('kings', $\diamond$ .5236844420433044),
('Queen Consort', 0.5235945582389832)， ('queens', $\diamond$ .518113374710083)，
('sultan', $\big$ .5098593235015869)，
('monarchy', 0.5087411999702454)]

## 15）挑选出较不相似的词汇。程序代码如下：

1#选出较不相似的字司
2 model.doesnt match("breakfast cereal dinner lunch".split())

执行结果：cereal（麦片）与三餐较不相似

16）比较两词相似概率。程序代码如下

 $\perp_{2}$ 南.生我区艺河有名代餐管业司 man') model.similarity('

日上面测试可以知道，对于一般的文字判断，使用预先训练的模型都相当准确，但是，如果要判断特殊领域的相关内容，效果可能就会打折。举例来说，Kaggle 上有一个很有趣的数据集 $\Gamma$ 辛普生对话」（Dialogue Lines of The Simpsons），是有关辛普生家庭的卡通剧情问答，像是询问剧中人物Bart 与 Neson的相似度，结果 $\P$ 有 $0. 5$ ，因为在卡通里面他们虽然是朋友，但不是很亲近，假如使用预先训练的模型，来推论问题的话，答案应该就不会如此精确。除此之外，还有很多其他的例子，读者有时间不妨测试看看范例程序「Gensim Word2Vec Tutorial」（详见
htos $/ \! / \mathsf{W N W}$ .kaggle.com/pierremegret/gensim-word2vec-
tutorial)

之前我们都是比较单字的相似度，然而更常见的需求是对语句 (Sentence）的比对，如常见问答集（F $\mathrm{A Q} )$ 或是对话机器 $\mathcal{A}$ ，系统会先比对问题的相似度，再将答案回复给使用者，Gensim 支持 Doc2Vec算法，可进行语句相似度比较。

(1）笔者从Starbucks 官网抓 $7-$ 段FAQ的标题当作测试语料库。程序代码如下：

另外TensorFlow官网也提供了一个范例，能够直接使用 TensorFlow 实现. Skipgram 模型，网址为
https://www.tensorlow.org/tutorials/text/word2vec.

![](figures/720-0-FIGURE.jpg)

## (2）训练Doc2Vec模型。程序代码如下

1#分词函数
2 def tokenize(text, stopwords, max len = MAX WORDS A LINE):
3 return [token for token in gensim.utils.simple preprocess(text
4 , max len=max len) if token not in stopwords] 5
6#分司
7 document tokens=[] # 整理后的字词
8 for line in corpus:
9 document tokens.append(tokenize(line, stopword list))
10
11 #设定为Gensim 标签文件格式
12 tagged corpus = [TaggedDocument(doc, [i]) for i, doc in
13 enumerate(document tokens)]
14
15#训练 Doc2Vec模型
16 model d2v = Doc2Vec(tagged corpus, vector size=MAX WORDS A LINE, epochs=200) 17 model d2v.train(tagged corpus, total examples=model d2v.corpus count,
18 epochs=model d2v.epochs)

## (3）比较语句的相似度。程序代码如下

1#测试
2 questions = []
3 for i in range(len(document tokens)):
4 questions.append(model d2v.infer vector(document tokens[i])) 5 questions = np.array(questions)
6#print(questions. shape)
7
8#测试语句
9 # text = "find allergen information"
10 text = "mobile pay"
11 filtered tokens = tokenize(text, stopword list)
12 # print(filtered tokens)
13
14#比较语句相似度
15 similarity = cosine similarity(model d2v.infer vector(
16 filtered tokens).reshape $( 1, \ -1 )$ , questions, dense output=False) 17
18#选出前10名
19 top n = np.argsort(np.array(similarity[0]))[::-1][:10]
20 print(f'前10名 index:{top.n}\n')
21 for i in topin:
22 print(round(similarity[0][i], 4), corpus[i].rstrip("\n'))

执行结泉：以「mobile pay (手机支付）寻找前 $1 0$ 名相似的语句，结果达到了预期效果。读者可再试试其他语句，笔者测试其他的结果并不理想，后面改用BERT模型时，准确率会提升许多。

另外，TensorFlow也提供 $7-$ 个词嵌入的可视化工具
Embedding Projector thtts:/projector.tensorflow.org/），支持 3D 的向量空间，如图11.9所示。读者可以按以下步骤操作。

(2）选择其中一个候选字，接着系统会显示相似字，且利用各种算法（PCA、T-SNE、UMAP）来降维，以3D接口显示单字间的距离。

(3）选择「Isolate 101 points 只显示距离最近的101个单字

(4）也可以修改词嵌入的模型：Word2Vec All、Word'2Vec 10K、GNMT（全球语言神经机器翻译）等

(1）在右方的搜寻字段输入单字后，系统就会显示候选字

![](figures/721-6-FIGURE.jpg)

图11.9 TensorFlow Embedig Prgjcor可视化工具

Glo $\mathsf{V e}$ （Global Vectors）是由斯坦福大学Jefirey Pennington 等学者于 2014所提出的另一套词嵌入模型，与Word2Vec齐名， 他们认为Word2Vec 并未考虑全局的概率分布，只以移动窗内的词汇为样本，没有掌握全文的信息，因 $\rfloor\b L$ ，他们提出了词汇共现矩阵（Word-Word Coocurrence Matrix），考虑词汇同时出现的概率，解决Word2Vec $\P$ 看局部的缺陷以及BOW稀疏向量空间的问题，详细内容可参阅GloVe: Global Vectors for Word
$$
R e p r e s e n t a t i o n^{[ 7 ]} {}_{\circ}
$$

(htps:/lo.staniord.eduvdatawordvecsglove.42B.300d.zip) 430亿词汇 300维向量，占1.75 GB的文件空间。

{htps:/nlo.staniord.eduvdatawordvecsglove.840B.300d.zip) 8400亿词汇 300维向量，占2.03 GB的文件空间。

(https:/nlpostaniord.edu/datawordvecs/glove.6B.zip) 60 亿词汇，300维向量，占822MB的文件空间

(hts:/lostanord.ecdudatawordvecsoglove.twitter.27B.zip) 270 亿词汇 200维向量，占1.42 GB的文件空间

## 11-4 GloVe模型

Glove 有以下 $4$ 个预先训练好的模型。

$$
( 1 ) \mathrm{\ g l o v e.} 4 2 \mathrm{B. 3 0 0 d. z i p}
$$

$$
( 2 ) \mathrm{\ g l o v e. 8 4 0 B. 3 0 0 d. z i p}
$$

$$
( 3 ) ~ \mathrm{g l o v e. 6 B. 3 0 0 d. z i p}
$$

$$
( 4 ) ~ \mathrm{g l o v e. t w i t t e r. 2 7 B. z i p}
$$

GloVe词向量模型文件的格式十分简单，每行是一个单字，每个字段以空格隔开，第一列为单字，第二列以后为该单字的词向 $\frac{\mathtt{g}} {\mathtt{g}}$ 。所以，通常把模型文件读入后，转为字典的数据类型，以利于查询。

(2）取得GloVe的词向量：任选一个单字 $\mathrm{( l o v e )}$ 测试，取得Glove 的词向量。程序代码如下：

范例.GloVe测试

请参阅程序：11 06 GloVe.jpynb

(1）载入GloVe词向量档glove.6.30od.xt。程序代码如下：

| 1 | #载人相库 |
| 2 | import numpy as np |
| 3 |  |
| 4 | #载人GLoVve词向量档glove.6B.300d.txt |
| 5 | embeddings dict= {} |
| 6 | with open("./glove/glove.6B.300d.txt", 'r', encoding="utf-8") as f: |
| 7 | for line in f: |
| 8 | values = line.split( |
| 9 | word = values[o] |
| 10 | vector = np.asarray(values[1:], "float32") |
| 11 | embeddings dict[word] = vector |

 $^1_{2}$ #随意测试一个单字 $( l o v e )$ ，取得GLoVe 的词向量 embeddings dict['love']

部分执行结果如下：

(3）指定以欧几里得（Euclidean）距离计算相似性：找出最相似的10 个单字。

cqeen','monarch orine kindom, .rein, i ", broher 'crown'

(4）任选100个单字，并以散点图观察单字的相似度。程序代码如下：

| array([-4.5205e-01， | -3.3122e-01， | -6.3607e-02, | 2.8325e-02， | -2.1372e-01, |
| 1.6839e-01， | -1.7186e-02， | 4.7309e-02， | -5.2355e-02， | -9.8706e-01, |
| 5.3762e-01， | -2.6893e-01， | -5.4294e-01, | 7.2487e-02, | 6.6193e-02, |
| -2.1814e-01， | -1.2113e-01， | -2.8832e-01, | 4.8161e-01, | 6.9185e-01, |
| -2.0022e-01, | 1.0082e+00， ， | -1.1865e-01, | 5.8710e-01, | 1.8482e-01, |
| 4.5799e-02， | -1.7836e-02, | -3.3952e-01, | 2.9314e-01, | -1.9951e-01, |
| -1.8930e-01, | 4.3267e-01， ， | -6.3181e-01， | -2.9510e-01, ， | -1.0547e+00, |
| 1.8231e-01， | -4.5040e-01, ， | -2.7800e-01， | -1.4021e-01, | 3.6785e-02, |
| 2.6487e-01， | -6.6712e-01， | -1.5204e-01， | -3.5001e-01, | 4.0864e-01, |
| -7.3615e-02, | 6.7630e-01, | 1.8274e-01， | -4.1660e-02, | 1.5014e-02, |
| 2.5216e-01， | -1.0109e-01, | 3.1915e-02， | -1.1298e-01， | -4.0147e-01, |
| 1.7274e-01, | 1.8497e-03, | 2.4456e-01, | 6.8777e-01, | -2.7019e-01, |
| 8.0728e-01， | -5.8296e-02, | 4.0550e-01, | 3.9893e-01， | -9.1688e-02, |
| -5.2080e-01， | 2.4570e-01, | 6.3001e-02, | 2.1421e-01, | 3.3197e-01, |
| -3.4299e-01， | -4.8735e-01, | 2.2264e-02， | 2.7862e-01， | 2.3881e-01, |

![](figures/725-4-FIGURE.jpg)

## 执行结果：大部分与「king」的意义相似。结果如下：

执行结果：如图11.10所示，每次的执行结果均不相同，可以看到相似词都集中在局部区域。

![](figures/726-1-FIGURE.jpg)

![](figures/726-2-FIGURE.jpg)

图11.10以散点图观察单字相似度

前面介绍的都是英文语料，中文是否也可以比照处理呢？答案是肯定的，NLP 所有做法都考虑了非英语系的支持。Jieba库提供中文分词的功能，而spaCy 库则有支持中文语料的模型，现在我们就来介绍这两个库的用法。

(2）默认为简体语词字典，如需使用繁体中文，则应自
httos://github.com/APCLab/jieba-tw/tree/maste $\mathbf{r} / \mathbf{j i}$ eba下载繁体字典，可覆盖安装的文件，也可以于程序中使用set dictionary()设定繁体字典，下面的范例使用后者。

## 11-5 中文处理

Jieba 的主要功能如下。

$$
( 1 ) \nmid\ j \overline{{\mathrm{h}}} \exists\hspace{0. 3 c m} \mathrm{\partial_{o}}
$$

(2）关键词提取（Keyod Exracion O

 $( 3 )$ 词性标注 $\mathrm{( P O S )}$ 。

Jieba 安装指令如下

$$
( 1 ) \mathrm{\ p i p \ i n s t a l l \ j i e b a}
$$

范例，以Jieba库进行中文分词

请参阅程序： ${\bf1 1} \bsmile0 7$ 1 NLP.ipynb.
中文

(1）简体字分词：包含以下三种模式

 $( 2 )$ 繁体字分词：先调用 set dicionar $\mathbf{y} ( )$ ，设定繁体字典 dict.x。程序代码如 $\mathrm{T}$ :

D全模式（Ful Mode) 显示所有可能的词组。

②精确模式: 显示最有可能的词组，此为默认模式

3搜索引擎模式：使用隐马尔夫链（HMM）模型

$$
\star\equiv\lfloor\pm\lfloor\exists\pm\rfloor\rceil\rceil\Gamma:
$$

1#测试语句来启新闻 $h t t p ; / / f i n a n c e. p e o p l e. c o m. c n / n 1 / 2 0 2 1 / 0 9 0 2 / c 1 0 0 4-3 2 2$ 15242.htmL 2#载人相关库
3 import numpy as np
4 import jieba
5
6#分词
7 text ="增加用户数量和使用黏性，提升平台活跃度，提高平台在广告谈判中的议价能力"
8 $\# \ c u t \lunderline{{a}} U L=T r u e$ ：全模式
9 seg list = jieba.cut(text, cut all=True)
10 print("全模式："+"/".join(seg list)）
11
12 # cut all=False:精确模式
13 seg list = jieba.cut(text, cut all=False)
14 print("精确模式：”+"/".join(seg list))
15
16 # cut for search：搜索引引擎模式
17 seg list = jieba.cut for search(text)
18

## 执行结果如下：

全模式:增加/用/户/数量/和/使用/黏性/，/提升/平台/活/跃/度/，/提高/高平/平台/在/广/告/谈/判中/的/议/价/能力精确模式:增加/用户/数量/和/使用/黏性/，/提升/平台/活跃度/，/提高/平台/在/广告/谈判/中/的/议价/能力
搜索引擎模式：增加，用户，数量，和，使用，黏性，，，提升，平台，活跃度，，，提高，平台，在，广告，谈判，中，的，议价，能力

![](figures/728-8-FIGURE.jpg)

(4）加词：假如词汇不在默认的字典中，可使用 add word) 将词汇加入字典中，各行各业的专用术语都可以利用此方式加 $\lambda$ , 不 $\grave{\varPsi}$ 直接修改 dict.txt。

## 执行结果如下：

全模式：新竹/的/交通/交通大/大學/在/新竹/的/大學/大學路/學路/路上精确模式:新竹/的/交通/大學/在/新竹/的/大學路/上
搜索引擎模式：新竹，的，交通，大學，在，新竹，的，大學，學路，大學路，上

(3）分词后， 显示词汇的位置。程序代码如下：

1 text = "新竹的交通大学在新竹的大学路上"
2 result = jieba.tokenize(text)
3 print("单字\t开始位置\t结束位置"）
4 for tk in result:
5
$$
\mathrm{p r i n t} \big( f^{n} \{\mathrm{t} \mathrm{k} [ 0 ] \} \setminus\mathrm{t} \mathrm{t} \mathrm{t} \mathrm{k} [ 1 ] :-2 d \} \setminus\mathrm{t} \mathrm{t} \mathrm{k} [ 2 ] :-2 d \mathrm{t}^{n} \big)
$$

## 执行结果如下：

| 单字 | 开始位 | 位置 |
| --- | --- | --- |
| 新竹 | 0 |  |
| 的 | 2 |  |
| 交通 | 3 |  |
| F 大学 | 5 |  |
| 在 | 7 |  |
| 立心新竹 | 8 |  |
| 的 | 10 |  |
| 大学路 | 11 |  |
| 上 | 14 |  |

![](figures/729-7-FIGURE.jpg)

执行结果：原本的「三天三夜」分为两个词天三」和 「夜」 加词后，分词就正确了

(5）关键词提取：调月extract tags()函数，提取关键词，参可指定显示的批数。测试语句来自经济口报的新闻。程序数topk
代码如下：

执行结果：新闻标题为「互联网平台切莫忽视用户导向」，提取的关键词还算不错。

(6）设定停用词改进：调用stop words （file name）函数。 程序代码如下：

执行结果：设定停用词为「这本来、知用户、过分读」，提取的关键词调整如下：

![](figures/730-4-FIGURE.jpg)

[*平台'，「用户'，'互联网 '算法'，'推送'，"信息'，'推荐' 这本来"，'知用户'，'过分读"1

![](figures/730-6-FIGURE.jpg)

1·平台'，'用户'，'互联网*，'算法'，'推送'，"信息'，"推荐'，1户数据'，*这无疑'，“工具人·1
K

词性代码表可参阅「汇整中文与英文的词性标注代号」一 $\grave{\chi}$ ，内文有完整的说明与范例。

(7）取得词性标注：调用posseg.cut函数，可使用 POSTokenizer自定义分词器。程序代码如下：

![](figures/731-2-FIGURE.jpg)

## 执行结果如下：

![](figures/731-4-FIGURE.jpg)

spaCy 库支持超过64种语言，不只有Wod2Vec 词向量模型， 也支持BERT预先训练的模型，主要的功能见表11.1。

(1）可利用spaGy网页（ttps://spacy.iousage）的选单产生安装指令，产生的指令如下

## 11-6 spaCy库

表11.1 spaCy库主要功能

| 项次 | 功能 | 说明 |
| --- | --- | --- |
| 1 | 分词 | 词汇切割 |
| 2 | 词性标签 | 分析语句中每个单字的词性 |
| 3 | 文法解析(Dependency Parsing) | 依文法解析单字的相依性 |
| 4 | 词性还原（Lemmatization | 还原成词汇的原型 |
| 5 | 语句切割（Sentence Boundary Detection) | 将文章段落切割成多个语句 |
| 6 | 命名实体识别(Named Entity Recognition) | 识别语句中的命名实体，如人名、地点、 机构名称等  |
| 7 | 实体链接(Entity Linking) | 根据知识图谱链接实体 |
| 8 | 相似性比较（Similarity | 单字或语句的相似性比较 |
| 9 | 文字分类 | 对文章或语句进行分类 |
| 10 | 语意标注(Rule-based Matching) | 类似于Regular expression，依据语意找出词汇的顺序  |
| 11 | 模型训练(Training) |  |
| 12 | 模型存盘（Serialization |  |

$$
\mathbb{T} \mathrm{p i p ~ i n s t a l l ~ s p a c y_{o}}
$$

②支持 GPU，须配合 CUDA版本： pi inatall -U spacy[cudalll]。

Gcudall: 为 udavr.1版

Spkuseg：支援多领域分词，可参闭「pkuseg GitHub 1 依照文件说明，pkuseg的各项效能（Precision、Recall、F1）比 ieba 来得好。

spaCy相关功能的展示，可参考 spaly官网 「spaCy 101: Everything $\mathbf{y}$ ouneed to know1的说明，以下就依照该文测试相关的功能。

(2）下载词向量模型，spaCy称为 peine，指令如下。

D英文： python -m spacy download en core web sm

②中文： -m spacy download zh core web sm,
python

3其他语系亦可参考『spaCy Quickstart网页 (htips:/spacy.io/usage/models)

（3）词向量模型分成大型（g 中型（md) 小型 (sm

(4）中文分词有三个选项，可在组态档（oonfg.cfg）选择

Dchar：默认选项。

@jieba：使用Jieba库分词

范例.spaCy 相关功能测试

请参阅程序：11 08 test.ioynb
spaCy_

(1）加载相关库。程序代码如下 htps:/github.com/explosion/spaCy/blob/masterispacy/glossary,py

ht'ps://githu.con
$$
\mathrm{h / e x p l o s i o n / s p a C y / b l o b / m a s t e r / s p}
$$
acyglossany,py

(2）加载 $\imath\rfloor\omicron$ 型词向量模型。程序代码如下

1#载人词向量模型
2 nlp = spacy.load("en core_web sm")

$$
( 3 ) \dashgenfrac{i n d} {i n} \mathbb{J} \not\equiv\mathbb{B} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \in\mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \mathbb{J} \in\mathbb{J} \mathbf{J} \mathbf{J} \mathbf{J} \mathbf} \mathbf{J \times} \mathbf{J \mathbf} \mathbf\mathbf{J.} \mathbf\mathbf\mathbf\mathbf3}
$$
STaging O

Droken的属
$$
\# \overline{{\mathrm{~ p J ~}}} \oplus\mathrm{~ i j ~ h t t p s : / / s p a c y. i o l a p i / t o k e n_{o} ~}
$$

②词性标签表则请参考

## 程序代码如下：

## 执行结果如下：

$$
\begin{array} {l} {{\mathrm{A p p l}} \in\mathrm{P R o p N} \ \ \mathrm{n s u b i j}} \\ {\hline{\mathrm{1 s p l}} \mathrm{A U X} \ \ \mathrm{a u x}} \\ {\mathrm{1 s o k l} \ \mathrm{A D P} \ \ \mathrm{V E R B} \ \ \mathrm{R O T}} \\ {\mathrm{a t} \ \mathrm{A D P} \ \ \mathrm{V E E P}} \\ {\mathrm{b u y i n g} \ \ \mathrm{V E R B} \ \ \ \mathrm{p c o m p}} \\ {\mathrm{U, K} \ \ \ \mathrm{P R O P N} \ \ \mathrm{p c o m p}} \\ {\mathrm{S t a r} \ \mathrm{A D P} \ \ \mathrm{N O U N} \ \ \mathrm{a d V C I}} \\ {\mathrm{F o r} \ \ \mathrm{A D P} \ \ \mathrm{P C E P}} \\ {\ \mathrm{S Y M} \ \mathrm{c o m p c u p d}} \\ {\mathrm{I} \ \mathrm{N M} \ \mathrm{C o m p c u p d}} \end{array}
$$
billion NUM pobj

## (4）取得词性标签详细信息。程序代码如下

1#取得洋细的词性标签(POS Tagging）
2 for token in doc:
3 print(token.text, token.lemma , token.pos_, token.tag_, token.dep. 4 token.shape_, token.is alpha, token.is stop)

(5）以displaCy Visualizer 显示语意分析图，displacy.serve 的参数请参阅「displaCy visualizer 的说明文件」
$$
\mathrm{( h t t p s : / s p a c y. i o / a p i / t o p-l e v e l )}
$$
#dlisplacy) 程序代码如下：

执行结果：可使用网页浏览http://127.0.0.1:5000。如图11.11 所示，箭头表示依存关系，如looking 的主词是 $\mathrm{A p p l e}$ ,buying的受词是UK。

(6）以 displaCy visualizer标示命名实体（Named Entity) 程序代码如下：

执行结果如下：

![](figures/735-4-FIGURE.jpg)

$$
\mathrm{( h t t p s. / / s p a c y. i o / a p i / t o p-l e v e l \# d i s p l a c y )}
$$
。程序代码如下

![](figures/735-6-FIGURE.jpg)

![](figures/735-7-FIGURE.jpg)

11.11 显示语音分析图图

执行结果：大学被切割成两个词，结果不太正确，建议实际执行时可以先用简体分词后，再转回繁体。结果如下：

![](figures/736-1-FIGURE.jpg)

执行结果：可使用网页浏览htto $\mathrm{:} / {\vphantom{1} 1 2 7. 0. 0. 1}$ ：500。结果如下:

$$
\mathrm{T} :
$$

![](figures/736-4-FIGURE.jpg)

(7）繁体中文分词。程序代码如下：

![](figures/736-6-FIGURE.jpg)

$$
\left[ \begin{matrix} \frac{\buildrel\angle} {\angle} \equiv\mathrm{N O U N ~ n s u b j} \\ \frac{\angle} {\angle} A D V \mathrm{a d v m o d} \\ \frac{\ll} {\angle} A D V \mathrm{d e p} \\ \frac{\sideset{} {^<}} {\nimplies} A D P \mathrm{c a s e} \\ \frac{\ncong} {\hline} \\ \frac{\ncong} {\hline} \\ \frac{\sideset{} {} {^\angle}} {\not=} \\ \frac{\sideset{} {} {^\angle}} {\not=} \\ \end{matrix} \right] ] \{\begin{matrix} {\frac{\angle} {\angle}} \\ \end{matrix}
$$

(s）简体中文分词。程序代码如下

1 #简体中文分词
2 import spacy
3
4 nlp = spacy.load("zh core web sm")
5 doc = nlp("清华大学位于北京")
6 for token in doc:
 $7$ print(token.text, token.pos_, token.dep-)

## 执行结果如下： 执行结果：如图11.12所示，可使用网页比hp:/127.0.0.1: 5000。

执行结果：结果如下，afskisd不在字典中。注意：必须使用中型以上的模型，小型会出现错误。

 $( 9 )$ 显示中文语意分析图。程序代码如下：

![](figures/737-3-FIGURE.jpg)

![](figures/737-4-FIGURE.jpg)

图11.12中文语意分析图

(10）分词，并判断是否在字典中（Out of Vocabulary, OOV）。程序代码如下：

![](figures/737-7-FIGURE.jpg)

## (11）相似度比较。程序代码如下

1#相似度比较
2 nlp = spacy.load("en core web md")
3
4 #测试两语句
5 doc1 = nlp("I like salty fries and hamburgers.")
6 doc2 = nlp("Fast food tastes very good.")
7
8#两语句的相似度比较
9 print(doc1, "<->", doc2, docl.similarity(doc2))
10
11 #关键字的相似度比较
12 french fries = doc1[2:4]
13 burgers = doc1[5]
14 print(french fries, ${}^{1} <-\rangle^{1 1}$ , burgers, french fries.similarity(burgers))

## 执行结果如下： 上一章我们认识了自然语言处理的前置处理和词向量应用，本章我们接着探讨自然语言处理相关的深度学习算法。

自然语言的推断（Inference）不仅需要考虑文本上下文的关联，还要考虑人类特殊的能力一一记忆力。例如，我们从小就学习历史，讲到治 $\mathrm{z k}$ ，第一个可能想到治水的大禹，这就是记忆力的影响，就算时间再久远，都会印在脑中。因此，NLP相关的深度学习算法要能够提升预测准确率，模型就必须额外添加上下文关联与记忆力的功能。

我们会依照循环神经网络发展的轨迹依序研究，从简单的 RNN、LSTM、注意力机制（Attention) 到Transformer等算法，包括目前最实用的BERT模型。

## 第12章

## 自然语言处理的算法

一般神经网络以回归为基础，以特征 $( X )$ 预测目标（Y），但 NLP的特征并不互相独 $\overrightarrow{\Psi}$ ，它们有上下文的关联，因此，循环神经网络（Recurrent Neura Network, RNN）就像自回归（Auto-
regression）模型一样，会考虑同一层前面的神经元影响。可以用数学式表示两者的差异。

可以看到时间点t的h会受到前一时间点的 $h_{t-1}$ 影响，如图12.2所示。

## 12-1 循环神经网络

(1）回归： $Y_{=} W X_{+} B_{\circ}$ 示意图12.1所示

![](figures/740-4-FIGURE.jpg)

图12.1、回归的示意图

$$
( 2 ) \mathrm{\ R N N}
$$

$$
h_{t}=W^{\star} \, h_{t 1}+U^{\star} \, x_{t}+b
$$

$$
y=V^{\star} ~ h_{t}
$$

式中：W、人、V郜是权重、1为隐藏层的输出由于每一个时间点的模型都类似，因此又可以简化为图 12.3所示的循环网络，这不仅有助于理解，在开发时也可以简化为递归结构。

归纳上述说明：一般神经网络假设同一层的神经元是互相独立的；而RNN则将同一层的前一个神经元也视为输 $\lambda$ .

深度学习框架如TensorFlow、PyTorch 均直接文持RNN神经层，下面我们就以TensorFlow/Keras 实践一模型，看看效果如何。

![](figures/741-3-FIGURE.jpg)

1 $1 2. 2$ RNN的示意图冬

![](figures/741-5-FIGURE.jpg)

 $1 2. 3$ RNN循环图

范例.简单的RNN测试

请参阅程序：12- 01 RNN test.ipynb

1）加载相关库。程序代码如下

(2）嵌入层测试：模型只含嵌 $\lambda\Xi$ （Embedding Layer）。注意：RNN系列的神经网络模型的第 $- \sqrt{\Xi} \imath\check{U}$ 须为嵌入层，它会将输入转为稠密的向量空间（Dense Vector 嵌 $\lambda$ 层的参数如下。

mask zero：作长度不足时是否补 $0$ ，使用时须有许多配合措施，请参阅 Keras中文官网
$$
\mathrm{( h t t p s : / k e r a s. i o / z h / l a y e r s / e r r )}
$$
beddings

input dim：字典的尺寸

output dirm：输出的向量尺寸。

input ength：每笔输入语句的词汇长度，如果后面接 Flatten 和 Dense $\P$ ，则此参数是必填的。

(tps:/kera
$$
\mathrm{s. i o / z h / l a y e r s / e m b e d d i n g s} / ) \ \ .
$$
D

$$
\pi\equiv\Gamma\pm\Gamma\pm\pm\pm\Gamma\Gamma:
$$

![](figures/742-7-FIGURE.jpg)

执行结果：嵌入层（Embedding Layer）输入尺寸
input dim）为1000，代表字典的尺寸，嵌入层输入必须为二维，包含批数（32) 语句字数（10) 输出会加上一维，向量空间尺寸设定为64,故输出维度大小为（32 $1 0, \, 6 4 )$ 结果如下：

(3）使用真实的数据转换，观察嵌 $\lambda$ 层转换的结果。程序代码如下：

D嵌入层输入必须为固定尺寸，否则无法作向量运算，故长度不足时，则后面需补0。

②执行结果：（输出批数、语句字数、 (10,
单字向量维度)
4, 64)

![](figures/743-4-FIGURE.jpg)

![](figures/743-5-FIGURE.jpg)

(4）观察One-Hot Encoding 转换结果与补 $0$ 后的输入维度。 程序代码如下：

执行结果：[34,33为 One-Hot Encoding的两个单字编码，补0
 $( 1 0, 4 )$ 
后的输入维度为

![](figures/744-2-FIGURE.jpg)

(5）模型接上完全连接层，进行分类预测。程序代码如下

![](figures/744-4-FIGURE.jpg)

执行结果：准确率为80%，效果尚可。结果如下

(7）加上RNN神经层：simple rnn()也称为简单（Vanila) RNN，第一个参数为输出的神经元个数，另一个参数unrol-True 时，会使网络以图12.2所示的架构求解，好处是速度可加快，缺点是需要占用较大的内存；反之，则以递归的架构执行，如图12.3所 $\overline{{\Pi}}$ 。程序代码如下：

| Model: "sequential 2" |  |  |
| IAIAnI+InG | AtnutChana | DanmH |
| Layer (type) embedding 2（Embedding) | Output Shape (None,4,8) | Param # 400  |
| flatten (Flatten) | (None,32) | 0 |
| dense(Dense) | (None,1) | 33 |
| Total params: 433 Trainable params: 433 Non-trainable params: 0 |  |  |
| None |
| Accuracy: 80.000001 |

）测试数据预测。程序代码如下： (6)

1 model.predict(padded docs)

执行结果：概率均在50%上下，答案并不肯定。结果如下

![](figures/745-4-FIGURE.jpg)

执行结果：正面预测概率接近于 $1$ ，负面预测概率接近于0。 结果如下：

![](figures/746-1-FIGURE.jpg)

## 执行结果：准确率为100%。结果如下：

| Model: "sequential" |
| Layer (type) | Output Shape | Param # |
| embedding 9 (Embedding) | (None,4,8) | 400 |
| simple rnn 1 (SimpleRNN) | (None,128) | 17536 |
| dense3(Dense) | (None，1) | 129 |
| Non-trainable params: o |
| None |
| Accuracy:100.000000 |

(s）观察概率预测。程序代码如下

$$
\fbox{1 \, \mathrm{~ m o d e l ~. p r e d i c t \, ( p a d d e d_{-} d o c s \, )}}
$$

 $( 9 )$ 改用词向量（Word2Vec） 读取 GloVe 300维的词向量，产生字典数据型变量，方便搜寻。程序代码如下

执行结果：正面预测概率接近于 $1$ ，负面预测概率接近于0。 结果如 $\mathrm{T}$ :

| array([[9.99999642e-011, |
| 19.99 | 9881e-0113 |
| 19.97 | 8693e-0111 |
| 19.97 | 9895e-0111 |
| 19.99 | 4608e-011, |
| 12.61 | 8721e-0311 |
| [1.31 | 3715e-0513 |
| 1.73 | 2054e-0311 |
| 17.56 | 3222e-06, |
| 8.76 | 5386e-0611,dtype=float32) |

| 1 | # load the whole embedding yinto memory |
| 2 | embeddings index = dict() |
| 3 | f = open('./GloVe/glove.6B.300d.txt', encoding='utf8') |
| 4 | for line in f: |
| 5 | values = line.split() |
| 6 | word = valuesro] |
| 7 | coefs = np.array(values 1:], dtype='float32'` |
| 8 | embeddings index[word] l=coefs  |
| 9 | f.close() |

## （10）分词、转为序列整数并补0。程序代码如下

| 1 | #分司 |
| 2 | from tensorflow.keras.preprocessing.text import Tokenizer |
| 3 | t=Tokenizer() |
| 4 | t.fit on texts(docs) |
| 5 |  |
| 6 | vocab size= len(t.word index) + 1 |
| 7 |  |
| 8 | #转为序列整数 |
| 9 | encoded docs = t.texts to sequences(docs) |
| 10 |
| 11 | #补园 |
| 12 | padded docs = pad sequences(encoded docs, maxlen=maxlen, padding='post' |
| 13 | padded docs |

执行结果：正面预测概率接近于1，负百预测概率接近于0

| array([[6 | 5， 2 | ， 0 | 0], |
| [3 | ， 1 | , 0 | 0], |
| [7 | ， 4 | , 0 | 0], |
| [8 | 8， 1 | , 0 | 0], |
| [9 | ， ( | , 0 | 0], |
| [10 | , ( | , 0 | 0], |
| [5 | , 4 | , 0 | 0], |
| [11 | , 3 | , 0 | 0], |
| [5 | 5， 1 | , 0 | 0], |
| [12 | 2, 13 | , 2 | 1 14]]) |

(12）Embedding 层设为不需训练
$$
\mathrm{( t r a i n a b l e=F a l s e )}
$$
直接
使用词向量作为权重。程序代码如 $\mathrm{F}$ :

## (11）转换为 GloVe300维的词向量。程序代码如下：

| 1 | #转换为GLove300维的词向量 |
| 2 | #初始化输出 |
| 3 | embedding matrix = np.zeros((vocab size, 300)) |
| 4 |  |
| 5 | #读取词向量值 |
| 6 | for word, i in t.word index.items(): |
| 7 | embedding vector = embeddings index.get(word) |
| 8 | 兖衣仁一一 if embedding vector is not None:  |
|  — 9 embedding matrix|il= embedding vector |
| 10 |  |
| 11 | #仔取一组观察 |
| 12 | embedding matrix[2] |

## 执行结果：整数转为词向量。结果如下：

| array([ | 0.19205999 | 0.16459 | 0.060122 -0.11763 5 0.093991 5 -0.099847 1 0.42120999 -0.46766999 -0.22652 0.1358 上 | 0.17696001 | -0.27405 |  |
|  | 0.079646 | -0.25292999 | 0.17614 | -1.97870004 |  |
|  | 0.10707 | -0.028088 | 0.4813 | -0.037581 |  |
|  | 0.0059231 | -0.11118 | -0.22189 | 0.0062044 | 2 |
|  | 0.17721 | 0.25786 | -0.13085 | -0.32839 |  |
|  | 0.39208999 | -0.050214 | -0.063107 | -0.0023065 |  |
|  | 0.21005 | 0.26982 | -0.42958999 | -0.89682001 |  |
|  | 0.21932 | -0.0020377 | -0.12661999 | -0.058927 | 0 |
|  | 0.0049502 | -0.28457999 | -0.29530999 | -0.29295999 | -0.24212 |  |
|  | 0.091915 | 0.01977 | 0.14503001 | 0.2649599 | 0.10817 |  |
|  | 0.029115 | 0.075254 | 0.16463999 | 0.12097 | -0.37494001 |  |
|  | 0.52671999 | 0.094318 | -0.054813 | -0.021008 | 0.081353 |  |
|  | 0.18735 | -0.14458001 | -0.031203 I | 0.31753999 | 0.027703 |  |
|  | -0.28657001 | 0.34630999 | -0.27772 | 0.18669 | -0.11684 | ) |
|  | 0.21551999 | -0.21927001 | 0.19778 | 0.68763 | -0.076211 | 0 |
|  | -0.06296 | 0.13236 | 0.55324 | 0.15331 | -0.17332999 |  |
|  | -0.35551 | 0.16426 | 0.34196001 | -0.13568 | 0.071228 |  |
|  | 0.49147001 | -0.45590001 | 0.28874999 | -0.14091 | -0.025825 |  |
|  | -0.55035001 | 0.4946 | -0.2378 | -0.10571 | 0.06842 |  |

执行结果：正面预测概率接近于 $1$ ，负面预测概率接近于0 答案肯定正确。

![](figures/749-1-FIGURE.jpg)

## 执行结果：准确率为100%。结果如下：

| Model: "sequential 14" |  |  |
| Layer (type) | Output Shape | ram # |
| embedding 13 (Embedding) | (None, 4, 300) | 00 |
| simple rnn 5 (SimpleRNN) | (None,128) | 912 |
| denso7(Dense） | (Nono.1） | 9 |
| 一 |
| Total params: 59,541 |
| Trainable params: 55,041 |
| Non-trainable params: 4,500 |

## 13）观察预测结果。程序代码如下

1 list(model.predict classes(padded docs).reshape(-1))

执行结果：完全正确。结果如下：

14）观察概率预测。程序代码如下

$$
\fbox{1 \mod_{\mathsf{L}} \mathsf{I. p r e d i c t \, ( p a d d e d \_\, d o c s )}}
$$

| -0113 |
| -01]. |
| -013 |
| -011. |
| -0113 |
| -04]. |
| -04], |
| -04], |
| -04], |
| -04]1, dtype=float32) |

简单 RNN只考虑上文（ $\mathrm{L-}$ 个神经元如果要同时考虑下文，可以直接将simple rnn 包在 Bidirectional()函数内即可。

简单RNN有一个重大的瑕疵，它与CNN一样是简化模型，均假设权值共享（Shared Weights 因为

在优化求解时，进行反向传导（Backpropagation） 偏微分后有以下情况。

(1）若W<1，则越前面的神经层"会越来越 $\imath\rfloor$ ，导致影响力越 $\imath\rfloor$ ，这种现象称为梯度消失。

(2）反之，若WA1，则越前面的神经层W′会越来越大，引起梯度爆炸，优化求解无法收敛

梯度消失导致考虑的上文长度有限，因此，Hochreiter和 Schmidhuber于1997年提出长短期记忆网络（Long Short Term Memory Network, LSTM）算法，额外维护一条记忆网络，图12.4 所示为比较RNN 与LSTM的差别

## 12-2 长短期记忆网络

$$
\begin{array} {l} {{h_{i}=\displaystyle W^{*} ~ h_{i \cdot1}+U^{*} ~ x_{i}+\mathbf{b}}} \\ {{\}} \\ {{h_{k 1}=\boldsymbol{W}^{*} ~ h_{k 2}+U^{*} ~ x_{k 1}+\mathbf{b}}} \\ {{\Rightarrow\boldsymbol{h}_{i}=\boldsymbol{W}^{*} ~ ~ ( \boldsymbol{W}^{*} ~ h_{k 2}+\boldsymbol{U}^{*} ~ x_{k 1}+\mathbf{b} ) ~+\boldsymbol{U}^{*} ~ x_{i}+\mathbf{b}}} \\ {{\Rightarrow\boldsymbol{h}_{i}=\begin{array} {l} {{( \boldsymbol{W}^{2} ~^{*} ~ h_{k 2}+\boldsymbol{W}^{*} ~ U^{*} ~ x_{k 1}+\boldsymbol{W}^{*} ~ b ) ~+\boldsymbol{U}^{*} ~ x_{i}+\mathbf{b}}} \\ \end{array}}} \\ \end{array}
$$

$$
h_{t}=W^{\star} \, h_{t 1}+U^{\star} \, x_{t}+b
$$

我们将图12.4的LSTM进行拆解，就可以了解LSTM的运算机制。

(2）LSTM多了四个阀 $( \mathrm{G a t e} )$ 用来维护记忆网络与预测网络：即图12.4中的 (原图为粉红色标志)

(3）遗忘阀（Forget Gate）：决定之前记忆是否删除，为 sigmoid 神经层，输出为0时，乘以原记 $\mathrm{Z}$ ，表示删除，反之则为保留记化 $\mathrm{Z}$ ，如图12.6所示。

![](figures/752-3-FIGURE.jpg)

12.4 RNN 与LSTM内部结构的比较冬

$$
\mathrm{( a ) ~ \ R N N ; ~ \Gamma( b ) ~ \ L S T M}
$$

(图片来源：U
$$
\mathit{n d e r s t a n d i n g \, L S T M \, N e t w o r k s}^{\, [ 1 ] \,} )
$$

(1）额外维护一条记忆线（Cel State ，如图12.5月所示

![](figures/752-8-FIGURE.jpg)

图12.5、维护一条记忆线

(4）输入阀（Input Gate）：输 $\lambda$ 含目前的特征 $( \boldsymbol{x}_{t} )$ 加t-1时间点的隐藏层 $( h_{t 1} )$ ，透过 $\sigma$ (sigmoid) 得到输出 $( i_{t} )$ ，
而记 $\ref{i f f}$  $( C_{t} )$ 使用tanh activation tunction，其值介于 $(-1, 1 )$ ，如图 12 $7$ 所示

(5）更新阀（Update Gate 更新记忆 $( C_{t} )$ 为之前的记忆加上目前增加的信息，如图12.8所示。

![](figures/753-2-FIGURE.jpg)

图12.6遗忘阀

$$
f_{t} \!=\! \sigma\left( W_{f} [ h_{t} \!-\! 1, \! x_{t} ] \!+\! b_{f} \right)
$$

![](figures/753-5-FIGURE.jpg)

图12.7输 $\lambda$ 阀

$$
\begin{array} {l} {i_{t}=\sigma( W_{i} \cdot[ h_{t-1}, x_{t} ]+b_{i} )} \\ {\tilde{C}_{t}=\operatorname{t a n h} ( W_{C} \cdot[ h_{t-1}, x_{t} ]+b_{C} )} \\ \end{array}
$$

(6）输出阀（Qutput Gate 输出包括目前的正常输出，乘以更新的记化 $\mathrm{Z}$ ，如图12.9所示。

依照前面的拆解，读者大概就能知道LSTM 是如何保存记忆及使用记 $\mathrm{Z} \zeta$ ，网络上也有人直接用NumPy开发LSTM，不过，既然TensorFlow/Keras 已经直接定义LSTM神经层了，于是我们可直接拿来使用。

范例1.以LSTM实践情绪分析（Sentiment $\mathbf{A n a l y s i s} )$ 情绪分析是预测一段评论为正面或负面的情绪。

![](figures/754-3-FIGURE.jpg)

图12.8、更新阀

$$
C_{t}=f_{t}^{\ *} C_{t-1}+i_{t}^{\ *} \tilde{C}_{t}
$$

![](figures/754-6-FIGURE.jpg)

图12.9输出阀

$$
\begin{array} {l} {o_{t}=\sigma( W_{o} [ h_{t-1}, x_{t} ]+b_{o} )} \\ {h_{t}=o_{t}^{\ *} \operatorname{t a n h} ( C_{t} )} \\ \end{array}
$$

数据集：影评数据集（IMDB movie review）。注意：
TensorFlow 已将数据转为索引值，而非文字，如果要取得文字数据集，可自
htps://aistanford.edu/-amaas/data/sentiment/acllmdb vl.tar.gz 下载。

请参阅程序
$$
: ~ 1 2 \_0 2 \_{\bf L S T M}_{-} {\bf I M D B. i p y n b_{o}}
$$

12.10所示设计流程如图

![](figures/755-3-FIGURE.jpg)

图12.10设计流程

1）加载相关库。程序代码如下：

1#载人相库
2 import tensorflow as tf
3 from tensorflow.keras.datasets import imdb
4 from tensorflow.keras.layers import Embedding, Dense, LSTM
5 from tensorflow.keras.losses import BinaryCrossentropy
6 from tensorflow.keras.models import Sequential
7 from tensorflow.keras.optimizers import Adam
8 from tensorflow.keras.preprocessing.sequence import pad sequences

(2）参数设定。程序代码如下 (3）建立模型：使用Embedding+ LSTM+ Dense 神经层。程序代码如下：

(5）虽然，模型准确率很高，但我们还是希望能以自定义的数据的模型测试 $- \mathrm{T}$ 。由于IMDB的影评内容都非常长，假使以短句测试，如「l like the movie」或「lhate the movie 并不能得

![](figures/756-2-FIGURE.jpg)

![](figures/756-3-FIGURE.jpg)

## (4）训练模型与评估。程序代码如下

![](figures/756-5-FIGURE.jpg)

执行结果：训练执行 $5$ 周期，损失为0.3370，准确率为 86.63

到正确的预测值，因此，我们使用测试数据的前两句来测试。首先我们要取得词汇与索引的对照表字典。程序代码如下

(6）反转字典，变成索引与词汇的对照表：要将数据集的数据还原成文字。程序代码如 $\mathrm{F}$ :

(7）还原测试数据前两批为文字：由于数据有许多0，而空白的索引值也是0，因此为避免以空白切割词汇时，将空白均删除， 故以 $\Gamma,$ 」隔开单字。程序代码如 $\mathrm{F}$ :

![](figures/757-3-FIGURE.jpg)

![](figures/757-4-FIGURE.jpg)

| #必原测试数据前两批为文字 |
| text= [] |
| 3 for i, line in enumerate(padded inputs test[:2l): |
| text.append(y |
| 6 for j, word in enumerate(line): |
| 5 if word!=0: |
| 7 text[il += imdb dict reversed[word]+', |
| 3 else: |
| 0 text[iT += |
| 0 text |

## 执行结果如下：

the,wonder,own,as,by,is,sequence,i,i,and,an d to,of ,hollywood,br,of,down,and,getting,boring,of,ever,it,sadly,sadly,sadly,i,i,was,then,does,don't,close,and,after,one,carry, as,by,are,be,and, all,family ,turn,in,does,as,three,part,in,another,some,to,be,probably ,with,world,and,her,an,have,and,beginning, own,as,is,sequence,",

## (8）预测。程序代码如下

1#长度不足时补 $o$ 
2 padded inputs = pad sequences(X index, maxlen=max sequence length,
3 padding-pad type, truncating-trunc type, value = 0.0) 4
5 #预测
6 model.predict classes(padded inputs)

数据集：文字型的影评数据集（IMDB movie review 可自 htts://aistanord.edu/-amaas/data/seniment/aclmdb vl.tar.gz 下载。

(1）以文字数据集作为测试数据：以TensorFlow Dataset数据类型处理。

其他程序与12 02 LSTM IMDB.ipynb大同小异，此处不再详细介绍，读者可直接测试程序

（9）以原数据预测：确认答案相同。程序代码如下：

## 范例2。以文字数据集，实践LSTM情绪分析。

请参阅程序：12 03 LSTM IMDB Text.ipynb

与范例1不同的要点如下。

(2）也以卷积模型 $( \mathrm{C o n v 1 D} )$ 预测，比较模型差异。

(3）以双向LSTM模型（Bidirectional、LSTM）测试 LSTM可以像 $\mathrm{C N N-}$ -样使用多层的卷积层吗？答案是肯定的，但使用多层LSTM，必须注意一些重要的参数，可参阅 「Keras官网LSTM的说明」2

(1）return sequences：预设为 False，只传回最后一个神经元的输出 $( y )$ 若为True，则表示每一个神经元的输出 $( y )$ 都会传回。

(2) return state：预设为False，不会传回隐藏层状态
(Hidden State）及记忆状态（Cell State 反之，则会传回最后一个神经元的状态，当作下一周期或批次的输入。

(3）stateful：预设为False，前一批的隐藏层及记忆状态不会传给下一批训练，延续记 $\mathrm{Z}$ ，反之，则会传回前一批的隐藏层及记忆状态，作为下一批训练的输入。

(4）若两个LSTM 层相串连，前面的LSTM必须设定参数 return sequences-True，表示每一神经元的输出（y）都会传回， 使用批次（Batch）时，须设定
stateful-True，表示训练时每一批的最后输出会接到下一批的输入，同时每一个训练周期后要呼叫 reset states重置状态，避免接到上一个训练周期的输出。

以上的说明有些复杂，我们以实践帮助读者理解，这部分需要专 $\omicron\grave{\mathrm{L}}-\underline{{\Xi}}$ ，细致理解

## 12-3LSTM重要参数与多层LSTM

范例1.LSTM 重要参数测试。 (2）定义模型，内含一个LSTM，参数均为默认值。程序代码如 $\mathrm{F}$ :

执行结果：LSTM每一个神经元的输出均传回， $3$ 个输入就会有 $3$ 个输 $\mathtt{H}$ ，分别达[0.02329711],[0.06432742],
[0.11739781]]1.

## 请参阅程序：12 04LSTM_参数测试,.pynb

(1）加载相关库。程序代码如下

1#定义模型，参数均为预设值
2 model= Sequential()
3 model.add(LSTM(1, input shape=(3, 1)))
5#试数据
6 data = np.array $( [ 0. 1, 0. 2, 0. 3 ] )$ .reshape((1,3,1)) 7#预测：只传回最后的输出(y)
8 print(model.predict(data))

执行结果：LSTM输入3个数值，输出一个数值为
$$
[ [ 0. 1 0 5 6 3 3 5 2 ] ]_{\circ}
$$

(3）加一个参数return sequences-Tue。程序代码如下：

1#定义模型，参
$$
\not{\simeq} r e t u r n \_s e q u e n c e s=T r u e
$$
2 model = Sequential()
3 model.add(LSTM(1, input shape=(3, 1), return sequences=True)) 4
5#测试数据
6 ${\tt d a t a=n p. a r r a y ( [ 0. 1, \ 0. 2, \ 0. 3 ] )}$  $\# ( y )$ .reshape((1,3,1))
7#预测：传回每一节点的输
8 print(model.predict(data))

(4）加一个参数return state-Tue。程序代码如下： 执行结果：除 $7-$ 个输出外，还会传回隐藏层状态及记忆状态，总共 $3$ 个数值：[array ([-0.04487724], dtype-float32)
array ([[-0.04487724]], dtype-float32) array ([-0.07851784]], dtype-float32) $\rceil_{\mathrm{c}}$ 

注意：多个输出必须使用Function API，不能使用Sequentia 模型

$$
\mathrm{y n T :} \mathrm{( 5 ) ~ r e t u m \_~ s e q u e n c e s=}
$$
True, return state-True。程序代码

![](figures/761-3-FIGURE.jpg)

| 1 | #定义模型，参数return sequences=True return state=True |
| 2 | #多个输出必须使用Function API |
| 3 | from keras.models import Model |
| 4 |  |
| 5 | inputs1 = Input shape=(3,1)) |
| 6 | lstm1= LSTM(1, return sequences=True, return state=True)(inputs1) |
| 7 | — model = Model(inputs=inputs1, outputs=lstm1)  |
| 8 | Y 3  |
| 9 | #测试数据 |
| 10 | data = np.array([o.1, 0.2, 0.31).reshape((1,3,1)) |
| 11 | #预测：传回输出(y),state h,statec |
| 12 | print(model.predict(data)) |

执行结果：输出有3个数值，还有隐藏层状态（Hidden State）及记忆状态（Cell State），总共 $3$ 个数值：[array ([[-0.04487724]], ctype-float32) array
([[-0.04487724]],dtype-float32) array ([[-0.07851784]], dtype-float32)].

）模型包含两个LSTM神经层。程序代码如下： (6)

)第一个LSTM神经层的参数return sequences 须为Tue 否则会出现错误如下：

②执行结果：输出有 $3$ 个数值，还有隐藏层状态及记忆状态
总共 $3$ 个数值：[arr
$$
\mathrm{\ a y \ \ ( [ [-0. 0 0 4 7 1 7 5 5 ], [-0. 0 1 7 5 3 8 )}
$$
834]
 $\mathsf{I}$ -0.04042896]]], dtype=float32) array
$$
\mathrm{( [ [-0. 0 4 0 4 2 8 9 6 ] ], d t y p e=f l o a t s )}
$$
$$
\mathrm{a t 3 2} ) \ \, \mathrm{a r r a y} \ \, ( [ [-0. 0 7 5 6 6 8 4 8 ] ],
$$
dtype-float32)]o

由以上的实验，我们就能理解LSTM 参数的影响。接下来看另一个实例，使用多层LSTM，即所谓堆栈LSTM（Stacked
LSTM) 可以建立更多层的神经网络，与CNN的卷积层一样，其主要目的是提取特征，常被运用在语音识别方面。

范例 $2$ 。时间序列预测，以LSTM·算法预测航空 $\mathbf{Z}$ 司的未来营收，包括以下各种模型测试。

incompatible with the layer: expected ndim-3, found ndim-2.

(1）前期数据为X，当期数据为 $Y_{\epsilon}$ 
(2）取前 $3$ 期的数据作为 $\mathit{X}$ 。即以 $t-3$ 、 $\t-2$ 、t-1期预测琪期。 以1-3期（单期）预测琪。
(3)

1）前期数据为X，当期数据为 $Y_{\epsilon}$ 
P

(2）取前3期的数据作为X，即以t3、 $\underleftarrow{-} 2$ 、t-1期预测。

(3）以t3期（兰期）预测期请参阅程序： ${\bf1 2}$ 05 Stacked LSTM.ipynb，修改白Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras'l.

(2）加载航空公司的营收数据：这份数据年代久远，每月一笔数据，自1949年1月至1960年12月。程序代码如下：

(4）将每个周期再分多批训练

(5）Stacked LSTM：使用多层的LSTM

12.11所万元设计流程如图

![](figures/763-5-FIGURE.jpg)

图 $1 2. 1 1$ 设计流程

1）加载相关库。程序代码如下：

![](figures/763-8-FIGURE.jpg)

1 $^\sharp$ 载入测试数据
2 df2 = pd.read csv('./RNN/monthly-airline-passengers.csv 3 df2.head()

(3）绘图：透过图表可以发现，航空公司的营收除了有淡旺季之分以外，还有逐步上升的趋势。程序代码如 $\mathbf{F}$ :

(4）转换数据：以前期数据为 $X$ ，当期数据为 $Y,$ 以前期营收预测当期营收。因比训练数据和测试数据不采取随机切割，前面三分之二为训练数据，后面三分之一为测试数据，并对特征进行正态化。程序代码如下：

| 1 | # 绘图 |
| 2 | df2 2=0 df2.set index('Month') |
| 3 | df2.plot(legend=None) |
| 4 | plt.xticks(rotation=30) |

执行结果：如图12.12所示

![](figures/764-4-FIGURE.jpg)

$$
\boxed{\xi} ~ 1 2. 1 2 ~ ~ \Sigma\boxed{\xi} ~
$$

(5）模型训练与评估：由于训练数据较少，故训练较多执行周期（100) 模型为LSTM+Dense（1），会输出一个数值，即下期营收。LSTM的inout shape为（批数，落后期数，特征个数） 三维。程序代码如下：

(6）预测后还原正态化：绘制实际数据和预测数据的图表。 程序代码如下：

![](figures/765-2-FIGURE.jpg)

![](figures/765-3-FIGURE.jpg)

D执行结果：训练数据和测试数据的RMSE分别为22.84、 47.63.

2图表请参考程序，蓝色线条为实际值，橘色为训练数据的预测值，绿色为测试数据的预测值，如图12.13所示。

(7)Loopback 改为 $3$ ：X由前1期改为前 $3$ 期，即以-3、1 $2 \underbrace{}$ t1期预测期。程序代码如下

![](figures/766-3-FIGURE.jpg)

![](figures/766-4-FIGURE.jpg)

图12.13、实际数据和预测数据图表

(8）模型训练与评估：LSTM的参数input shape设为（1 3) 因为特征有 $3 \uparrow$ （期程序代码如下：

(9）预测后还原正态化：绘制实际数据和预测数据的图表。 程序代码如下：

![](figures/767-2-FIGURE.jpg)

执行结果：训练数据的特征（批数，落后期数，X维度) (92, $1, 3 )$ 

![](figures/767-4-FIGURE.jpg)

D执行结果：训练数据和测试数据的 RMSE 分别为22.83、 62.55，测试RMSE比只以一期为特征预测差，因为使用多期，有移动平均的效果，预测曲线会比较平缓，不容易受到激烈变化的样本点影响。

②图表请参考程序，蓝色线条为实际值，橘色为训练数据的预测值，绿色为测试数据的预测值，如图12.14所示

(10）改变落后期数（Time Steps）为 $3$ ：即以t-3期预测期程序代码如下：

![](figures/768-3-FIGURE.jpg)

![](figures/768-4-FIGURE.jpg)

图12.14绘制实际数据及预测数据图表

执行结果：训练数据的特征三维度（批数，落后期数， $\mathbf{X}$ 维店 $\xi) ~=~ ( 9 2, \, 3, \, 1 )$ 

(11）模型训练与评估：LSTM的参数input shape设为（3 1) 因为特征只有 $- \uparrow$ ，落后 $3$ 期（1-3) 程序代码如下：

(12）预测后还原正态化：绘制实际数据和预测数据的图表。 程序代码如 $\mathrm{F}$ :

![](figures/769-3-FIGURE.jpg)

![](figures/769-4-FIGURE.jpg)

2图表请参考程序，蓝色线条为实际值，橘色为训练数据的预测值，绿色为测试数据的预测值，如图12.15所示

（13）将每个周期细分多批训练：须设定LSTM参数
、表示训练时每一批的最后输出会接到 $\mathrm{J T-}$ 批的输 stateful=True,
 $\lambda$ ，故每一个训练周期要调用reset states 重置状态，避免接到上

![](figures/770-2-FIGURE.jpg)

D执行结果：训练数据和测试数据的RMSE分别为 $2 7. 1 9.$ 66.19，测试RMSE也比以前期为特征预测差。

![](figures/770-4-FIGURE.jpg)

图12.15$绘制实际数据与预测数据图表

一个训练周期的输出。LSTM的input shape 为（每批数，落后期数,特征个数 $) ~ ~ ~ \Xi$ 维，前置处理不变。程序代码如下：

(14）模型训练与评估：以批量设为1测试，也可以加大批量。程序代码如下：

(15）预测后还原正态化：绘制实际数据和预测数据的图表。 程序代码如下：

![](figures/771-3-FIGURE.jpg)

![](figures/771-4-FIGURE.jpg)

执行结果：训练数据和测试数据的RMSE 分别为25.91、
51. $7 4$ ，测试RMSE比不分批次略好一些，但不明显，这里主要是示范stateful参数的用法。

(16） Stacked LSTM：使用多层的LSTM，以下前置处理不变。程序代码如下

![](figures/772-2-FIGURE.jpg)

(17）模型训练与评估：因为LSTM 需接收上一个神经元的隐藏层输出 $( h_{t^{-}} 1 )$ ，因 $\operatorname{l o b c}$ ，若两个LSTM 层串连，前面的LSTM必须设定参数return sequences-True，表示每一个神经元的输出 $( y )$ 都会传给下一个LSTM。训练数据与测试数据预测之间也要重置状态，避免训练数据预测的状态传给测试数据预测。程序代码如下：

![](figures/773-1-FIGURE.jpg)

![](figures/774-0-FIGURE.jpg)

## (18）预测后还原正态化：绘制实际数据和预测数据的图表

程序代码如下：

![](figures/774-2-FIGURE.jpg)

个执行结果：训练数据和测试数据的 RMSE 分别为24.28、 86.85，测试RMSE比较差，应该是因为数据很单一，使用太复杂的网络结构反而没有帮助。

2图表请参考程序，蓝色线条为实际值，橘色为训练数据的预测值，绿色为测试数据的预测值，如图12.16所示

![](figures/775-2-FIGURE.jpg)

图12.16、绘制实际数据和预测数据图表

Gate Recurrent Unit (GRU）也是RNN 变形的算法，中
Kyunghyun Cho 在 2014年提出的，可参阅Empirical Evaluation of Gateol Recurrent Neural Networks ${\it o n}$ Sequence Modelingt，主要就是要改良LSTM的缺陷。

LSTM 是由遗忘阀与输入阀来维护记忆状态，然而因为这部分太过耗时，所以GRU废除记忆状态，直接使用隐藏层输出
0 $h_{t} )$ 2
，开二将前述两个阀改由更新阀替代，两个模型的架构比较如图12.17所示。

## 12-4 Gate Recurrent Unit

(1）LSTM计算过慢，GRU可改善训练速度

(2）简化LSTM模型，节省内存的空间

![](figures/776-5-FIGURE.jpg)

图12.17 LSTM与 GRU 内部结构的比较

(左图为LSTM，右图为GRU，图片来源：lustrated Guide to LSTMi's and GRUis: A step by step explanaion bi

虽然原作者提出效能测试图表，说明GRU的效能比LSTM $\sharp\mp$ ，不过，笔者实际测试的结果，差异并不明显，而且网络上也比较少提到GRU，大多仍以LSTM为主流，因此，我们就不详细研究 $\overline{{\jmath}}$ 。

白于LSTM与时间序列模型很类似，因此网络上有许多文章探讨以LSTM 预测股票价格， $\lfloor\mathcal{Y} .$ 下我们就来实践看看。

 $\mathbf{( h t t p s )} / / \mathbf{w} \mathbf{w} \mathbf{w}$ 
$$
\mathrm{t o c k-t i m e-s e r i e s-2 0 0 5 0 1 0 1-t o-}
$$
kaggle.com/szrlee/s 20171231）为例，也可以使用台股。

## 12-5 股价预测

范例。以LSTM/GRU算法预测股价。

请参阅程序：12 06 Stock Forecast.ipynb，修改自 FPredicting stock prices with LSTM l0

数据集：本范例使用亚马逊企业股票

12.18 所示设计流程如图

![](figures/778-7-FIGURE.jpg)

冬 $1 2. 1 8$ 设计流程

1）加载相关库。程序代码如下 ## (2）加载测试数据。程序代码如下

1#载人测试数据-- 亚马通
2 df = pd.read csv('./RNN/AMZN 2006-01-01 to 2018-01-01.csv 3 index col='Date', parse dates=['Date']) 4 df.head()

## 执行结果如下：

|  | Open | High | Low | Close | Volume |  | Date |  |  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 2006-01-03 | 47.47 | 47.85 | 46.25 | 47.58 | 7582127 |  |
| 2006-01-04 | 47.48 | 47.73 | 46.69 | 47.25 | 7440914 |  |
| 2006-01-05 | 47.16 | 48.20 | 47.11 | 47.65 | 5417258 |  |
| 2006-01-06 | 47.97 | 48.58 | 47.32 | 47.87 | 6154285 |  |
| 2006-01-09 | 46.55 | 47.10 | 46.40 | 47.08 | 8945056 |  |

## (3）绘图。程序代码如下：

1#只使用收盘价
2 df= df['Close']
3
4 #绘图
5 plt.figure(figsize = (12, 6))
6 plt.plot(df, label='Stock Price') $7$ plt.legend(loc='best')
8 plt.show()

执行结果：如图12.19所示

![](figures/780-0-FIGURE.jpg)

图 $1 2. 1 9$ 绘图结果

(4）参数设定：以过去40期为特征（X 一次预测10天 (y) 程序代码如 $\mathrm{T}$ :
测试数据量设定20期。

![](figures/780-3-FIGURE.jpg)

## (5）特征正态化。程序代码如下：

1#特征正态化
2 from sklearn.preprocessing import MinMaxScaler 3 scl = MinMaxScaler()
4 array = df.values.reshape(df.shape[0],1)
5 array = scl.fit transform(array)

## (6）前置处理函数：取得模型输入的格式。程序代码如下：

1 前置处理函数，取得模型输人的格式
2#Look back：特征(X)个数，forward days：日标(y）个数，jump：移动视皮 3 def processData(data, look back, forward days,jump=1):
4 ${\sf X}, {\sf Y}=\left[ \begin{array} {c} {{\sf I}} \\ \end{array} \right], \left[ \begin{array} {c} {{\sf I}} \\ \end{array} \right]$ 
5 for i in range(o,len(data) -look back -forward days +1, jump): 6 X.append(data[i:(i+look back)])
7 Y.append(data[(itlook back):(itlook back+forward days)]) 8 return np.array(X),np.array(Y)

## (7）数据切割成训练数据和测试数据。程序代码如下：

1#数据切割成训练数据及测试数据
2#一久预测 $1 0 \pi, \frac{\#} {\pi} 2 0$ 期
3 division = len(array) - num periods*forward days 4
5#再往前推 $4 \theta$ 天当第一笔的 $x$ 
6 array test = array[division-look back:]
7 array train = array[:division]

$$
{\bf x t r} : ( 8 ) \ {\bf x t r} \equiv{\bf x t r}, \ {\bf x t r} \geq{\bf x t r} )
$$
割成训练数据和验证数据。程序代码

![](figures/781-1-FIGURE.jpg)

## (9) 。程序代码如下训练模型。

1#训练模型
2 NUM NEURONS FirstLayer = 50
3 NUM NEURONS SecondLayer = 30
4 EPOCHS= 10
5
6#模型
7 model = Sequential()
8 model.add(LSTM(NUM NEURONS Firstlayer,input shape=(look back,1), return sequences=True)) 9 model.add(LSTM(NUM NEURONS SecondLayer,input shape=(NUM NEURONS FirstLayer,1)))
10 model.add(Dense(forward days))
11 model.compile(loss='mean squared error', optimizer-'adam')
12
13#训练
14 history = model.fit(X train,y train,epochs=EPOCHS,validation data=(X validate,y validate) 15 , shuffle=True,batch size=2, verbose=2)

## 10）绘制损失函数。程序代码如下

1#绘制损失函数
2 plt.figure(figsize = (12, 6))
3 plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val loss'], label='val loss' 5 plt.legend(loc='best')
6 plt.show()

执行结果： 12.20所示
如图

![](figures/782-0-FIGURE.jpg)

图 $1 2. 2 0$ 绘制损失函数

（1）若一次预测1天： $\mathrm{j u m p=1}$ 。程序代码如下：

![](figures/782-3-FIGURE.jpg)

执行结果：若一次只预测1天，结果尚可，如图12.21所示 (13）绘制测试数据的预测值与实际值的比较。程序代码如下：

![](figures/783-1-FIGURE.jpg)

$$
\boxed{\xi} 1 2. 2 1 ~ ~-\mathbb{X} 3 7 ( \mathbb{X} ) \mathbb{X}-\mathbb{F}
$$

(12) 天，移动窗口不重叠。程序代码如下
一次预测10 

![](figures/783-4-FIGURE.jpg)

执行结果： $( 2 0, \, 1 0 )$ 表示20期，每期预测10天

$$
\mathrm{T} :
$$

1 #绘制测试数据预测值
2 plt.figure(figsize = (12, 6))
3#绘制20条预测值，scl.inverse transform:还原正态亿
4 for i in range(o,len(Xt)):
5 plt.plot([x + i*forward days for x in range(len(Xt[i]))],
6 scl.inverse transform(Xt[i].reshape(-1,1)), color='r') 7
8#指定预测值 $l a b e l$ 
9 plt.plot(0, scl.inverse transform(Xt[i].reshape $(-1, 1 ) ) [ 0 ]$ |, color='r' 10 , label='Prediction')
11
12#绘制实际值
13 plt.plot(scl.inverse transform(y test.reshape $e (-1, 1 ) )$ , label='Target') 14 plt.legend(loc='best')
15 plt.show()

个执行结果：测试数据的预测值与实际值的比较如图12.22所 $\overline{{\Pi}}$ ，预测并不理想

第 $9$ 行程序只是要指定标签，并不是要画线，缺这一行的话，预测值图表会有 $2 0 \uparrow$ 

 $\mathrm{F} \! : \qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak\allowbreak6 9 6 9 6 9 6 9 6 9 6 9 6 9 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6$ 绘制测试数据的预测值与实际值的比较。程序代码如

![](figures/784-3-FIGURE.jpg)

图12.22测试数据与预测值的比较

(14）全部数据预测：将训练数据一并预测。程序代码如下：

![](figures/784-6-FIGURE.jpg)

$$
\mathrm{T} :
$$

执行结果：全部数据的预测值与实际值的比较如图12.23所 $\overline{{\Pi}}$ ，预测看似相当理想，但其实只是镜头拉远的效果而已

执行结果：测试数据的预测值与实际值的比较如图12.24所 $\overline{{\Pi}}$ ，在移动窗口之间画线就会造成错觉。

![](figures/785-2-FIGURE.jpg)

![](figures/785-3-FIGURE.jpg)

 $1 2. 2 3$ 全部数据的预测值与实际值的比较冬

16）拉近看，只观察测试数据的预测值。程序代码如下：

![](figures/785-6-FIGURE.jpg)

 $( 1 7 )$ 改用GRU模型：GRU的用法/参数与LSTM的完全相同。程序代码如下

(1）股价非稳态 $\mathrm{\# ~ ( N o n-s t a t i o n a r y )}$ ：每个时间点的股价平均数与标准差都不一致， $\mathcal{M}$ 图表观察，股价数据有一股趋势影响力， 并非随机跳动。因比，时间序列预测通常会将股价转换为收益率 (Return Rate 使数据呈现稳态。

(2）股价变化非常大，以长期历史数据预测未来股价并不合理。

![](figures/786-3-FIGURE.jpg)

图12.24 测试数据的预测值的比较

![](figures/786-5-FIGURE.jpg)

使用LSTM预测股价并不准确，有以下原因

（3）只预测 $1$ 天还算准确，但是意义不大，因为 $1$ 天股价涨跌幅度有限，预测微幅波动并无意 $\grave{\chi}$ ，假若要预测多天来掌握长期趋势，依上述测试，LSTM的预测结果并不准确

以LSTM预测股价，网络上有很多的讨论，有兴趣的读者可以自行搜导相关资料来阅读，并以回测（Back Testing）的方式测试个股，观察策略是否奏效，回测的方式可参考笔者撰写的「算法交易（Algorithmic Trading）实作」 $[ 7 ] \!-\grave{\chi}_{c}$ 

RNN ${\cal M}$ 之前的隐藏层状态取得上文的信息，LSTM则额外维护一条记忆线（Gell State），目的都是希望能借由记忆的方式来提高预测的准确性，但是，两种算法都局限于上文的序列顺序，导致越靠近预测二标的信息，权重越大。实际上当我们在阅读一篇文章时，往往会对文中的标题、人事时地物或强烈的形容词特别注意, 这就是所谓的注意力机制（Attention Mechanism），对于图像也是如此，如图12.25所示，比如图片左方，婴儿的脸部就是注意力热 $\o$ ，图右下方的纸尿裤也是注意力热区。

只要透过注意力机制，在预测时可以额外把重点单字或部位纳 $\lambda$ 考虑，而不只是上下文。注意力机制常被应用到神经机器翻译 (Neural Machine Translation, NMT），下面我们就来看看NMT 的做法。

机器翻译是一个序列生成的模型，它是一种Encoder-Decoder 的变形，称为序列到序列（Sequence to Sequence, Seq2Seq）模

## 12-6 注意力机制

![](figures/788-4-FIGURE.jpg)

图12.25人类的视觉注意力分布

(图片来源：「深度学习中的注意力机制（2017版〉」 型，结构如图12.26所示。图中的Context Vector是Encoder 输出的上下文向量，类似于CNN AutoEncoder提取的特征向量。也可以应用于对话问答，如图 12.27所示。

而注意力机制就是把要输 $\lambda$ 到译码器的词汇都乘上一个权重， 与Context Vector 混合计算成Attention Vector，以预测下一个词 $\grave{\lambda} \Gamma$ ，这个机制会应用到译码器的每 $- \imath\Xi$ ，如图12.28所示

![](figures/789-2-FIGURE.jpg)

图12.6 Sec2Seα模型 THow are you 翻译为「你好
吗」

![](figures/789-4-FIGURE.jpg)

图12.27对话问答（问「How are you 回答「lam
fine」

权重的计算的方式有两种，分别由Luong和 Bahdanau提出的乘法与加法的公式，如图12.29 所示。它利用完全连接层及
Softmax activation function，优化求得整个语句内每个词汇可能的概率。

$$
( 1 ) \ \ \mathrm{s c o r e}=\mathrm{F C}
$$
(tanh (FC (EO) +FC (H））），其中：
FC 为完全连接层：EO为Encoder输出（output) H为所有隐藏层输出；tanh为 tanh activation function.

![](figures/790-2-FIGURE.jpg)

冬 $1 2. 2 8$ 注意力机制

$$
\begin{array} {l l} {{\alpha_{l s}={\frac{\exp\left( \mathrm{s c o r c} ( \boldsymbol{h}_{l}, \bar{h}_{l} ) \right)} {\sum_{s^{\prime}=1}^{N} \exp\left( \mathrm{s c o r c} ( \boldsymbol{h}_{l}, \bar{h}_{l} ) \right)}}} & {{[ \mathrm{A t t e n t i o n ~ w e r g h t s} ]}} \\ {{{\bf c}_{l}=\sum_{s} \alpha_{s} \bar{\boldsymbol{h}}_{s}}} & {{[ \mathrm{C o n t e x t ~ v e c t o r} ]}} \\ {{{\bf a}_{t}=f ( {\bf c}_{l}, \boldsymbol{h}_{l} )=\operatorname{t a n h} ( \boldsymbol{W}_{e} [ {\bf c}_{l} ; \boldsymbol{h}_{l} ] )}} & {{[ \mathrm{A t t e n t i o n ~ v e c t o r} ]}} \\ {{\mathrm{s c o r c} ( \boldsymbol{h}_{l}, \bar{h}_{l} )=\left\{\begin{array} {l l} {{{\bf{\bar{h}}}_{s}^{-} \sinh( \boldsymbol{W}_{1} \boldsymbol{h}_{l}+\boldsymbol{W}_{2} \bar{h}_{s} \right)}} & {{[ \mathrm{L o n g ~ s u n t i t i o n t i v e ~ s t i v e} ]}} \\ {{\mathrm{s c o r c} ( \boldsymbol{h}_{l}, \bar{h}_{s} )=\{\begin{array} {l
$$

图12.29 Attention weight公式

虚拟程序代码如下。

(2) Attenion weighis - soitmax (score,axis-1) O

(3） Context vector= sum (A
$$
\mathrm{A t t e n t i o n \, w e i g h t s ~^* ~ E O, ~ a x i s=}
$$
1)

$$
( 5 ) \ \mathrm{A t t e n t i o n} \ \mathrm{V e c t o r} \, \circ\vspace{-0. 5 c m} \mathrm{v e c t o r} ) \ \ \circ
$$
- concat (embedding output, contex

推荐各位「浅谈神经机器翻译&月Transformer 与 TensorFlow $2$ 英翻中」网一文的流程图动画，比较两个模型的差异，从另一角度观察，会有更多的收获，如冬12.30 所示。

范例。使用 Sec2Seq架构，加上注意力机制，实践神经机器翻译（NMT)

请参阅程序：12 07机器翻译attention.ipynb，修改自 TensorFlow 官网所提供的范例「Neural machine translation with attention L10

(4）Embedding outpui-译码器的 input经嵌入层 (Embedding layer）处理后的输出。

![](figures/791-6-FIGURE.jpg)

图12.30 Seq2Seq模型加上注意力机制

 $($ 图片来源：同上9)）

接下来，我们就来实践神经机器翻译（NMT) O 数据集：自htp:/www.manyhings.org/anki/spa-eng.zip下载西班牙文/英文对照档spa-eng.zip.

设计流程如图12.31所示

![](figures/792-2-FIGURE.jpg)

图12.31设计流程

(1）加载相关库。程序代码如下

1#载人相关库
2 import tensorflow as tf
3 import matplotlib.pyplot as plt
4 import matplotlib.ticker as ticker
5 from sklearn.model selection import train test split 6 import unicodedata
7 import re
8 import numpy as np
9 import os
10 import io
11 import time

(2）数据前置处理函数。程序代码如下：

![](figures/793-0-FIGURE.jpg)

## (3）测试数据前置处理函数。程序代码如下

1#测试
2 en sentence = u"May I borrow this book?"
3 sp sentence = u"iPuedo tomar prestado este libro?"
4 print(preprocess sentence(en sentence))
5 print(preprocess sentence(sp sentence).encode("utf-8'))

执行结果如下：

<start> mayi borrow this book ? <end>

## b'<start> xc2xbf puedo tomar prestado este libro ? <end-

(4）读取训练数据文件，转成对照表。程序代码如下：

1 #读取训练数据文件，转成对照表
2 def create dataset(path, num examples):
3 lines = io.open(path, encoding='UTF-8").read().strip().split("\n') 4
5 word pairs = [[preprocess sentence(w) for w in line.split('\t')[0:2]] 6 for line in lines[:num examples]]
7
8 return zip(*word pairs)
9
10 #读取训练数据文件
11 path to file='./RNN/spa.txt'
12 en, sp = create dataset(path to file, None)
13
14 #显示最后一句对照
15 print(en[-1])
16 print(sp[-1])

执行结果如下： ## (5）分词，建立 Dataset。程序代码如下：

![](figures/794-1-FIGURE.jpg)

## (6）数据切割。程序代码如下：

| 1 | # 数据切割 |
| 2 | input tensor train, input tensor val, target tensor train, target tensor val 二 |
| 3 | train test split(input tensor, target tensor, test size=0.2) |
| 4 |  |
| 5 | len(input tensor train), len(input tensor val) |

## (7）参数设定。程序代码如下

| 1 | #参数设定 |  |
| 2 | BUFFER SIZE = len(input tensor train) | # Dataset的缓冲区大小 |
| 3 | BATCH SIZE=64 | #批量 |
| 4 | steps per epoch = len(input tensor train)//BATCH SIZE | # 每周期包含的步骤数 |
| 5 | embedding dim = 256 | # 成人层的输出维度 |
| 6 | units= 1024 | # GRU输出维度 |
| 7 | vocab inp size = len(inp lang.word index)+1 | # 原始语言的字汇表大小 |
| 8 | vocab tar size = len(targ lang.word index)+1 | ， 日标语言的字汇表大小 #  |

## (8）建立Dataset。程序代码如下 ## (9）建立模型编码器。程序代码如下

![](figures/795-1-FIGURE.jpg)

## 执行结果如下：

）编码器输出维度：（批量，语句长度，输出 (64, 19, 1024)

2编码器隐藏层输出维度：（批量，输出） -(64, 1024 O

(10）建立注意力机制：Bahdanau做法。程序代码如下：

![](figures/796-0-FIGURE.jpg)

## 执行结果如下:

Oattention维度：（批量，输出） -(64.1024 O

权重：（批量，语句长度， $1 ) ~=~ ( 6 4, 1 9, 1 )$ 。 2Attention

11）建立模型译码器。程序代码如下

(12）指定优化器、损失函数：语句长度不足补0的部 $\acute{\jmath}$ ，损失不予计算。程序代码如下：

(13）检查点存档：由于训练需要 $1 0$ 多分钟，避免中途宕机要重来，故设检查点存盘， ${\cal F}-$ 发生宕机的状 $\gg\Pi$ ，即可自最 $\pi-\uparrow$ 检查点还原。程序代码如下：

![](figures/797-2-FIGURE.jpg)

执行结果：译码器维度为（批量，字汇表尺寸） (64, 4807)

![](figures/797-4-FIGURE.jpg)

(15）训练模型：在笔者的 $\mathrm{P C}$ 上执行，每一个执行周期约需 80秒的时间。程序代码如下：

## 14）定义梯度下降函数。程序代码如下：

![](figures/798-2-FIGURE.jpg)

| 1 | | #模型训练 |
| --- | --- |
| 2 | EPOCHS=10 |
| 3 | for epoch in range(EPOCHS): |
| 4 | start= time.time() |
| 5 |  |
| 6 | enc hidden - encoder.initialize hidden state() |
| 7 | total loss=0 |
| 8 |  |
| 9 | for (batch, (inp, targ)) in enumerate(dataset.take(steps per epoch)): |
| 10 | batch loss = train step(inp, targ, enc hidden) |
| 11 | total loss t= batch loss  |
| 一 12 | 上 |
| 一 13 | if batch %100=0: |
| 14 | print(f'Epoch fepoch-1} Batch fbatchl Loss {batch loss.numpv():.4fl) |
| 15 | 〕孕丈他仁木豆辛衣卒儿矿貅貅G— |
| 16 | #每2个训练周期朝存档一丸 |
| 17 | if (epoch+1)%2=0: |
| 18 | checkpoint.save(file prefix=checkpoint prefix) |
| 19 | 一— |
| 20 | print(f'Epoch {epoch-1l Loss {total loss/steps per epoch:.4f') |
| 21 | — print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\n') |

## 执行结果如下：

![](figures/799-0-FIGURE.jpg)

## 16）定义预测函数。程序代码如下

![](figures/799-2-FIGURE.jpg)

(17）定义绘图函数。程序代码如下： attention weights 

## 18）定义翻译函数。程序代码如下

| 1 | #定义翻译函数 |
| --- | --- |
| 2 | def translate(sentence): |
| 3 | result, sentence, attention plot = evaluate(sentence) |
| 4 |  |
| 5 | print('Input:', sentence) |
| 6 | print('Predicted translation:', result) |
| 7 |  |
| 2 8 | attention plot - attention plot[:len(result.split("")), |
| 9 | :len(sentence.split(''))T |
| 10 | plot attention(attention plot, |
| 11 | sentence.split(''), result.split('")) |

## 19）翻译测试，并绘制热图。程序代码如下：

 $^1_{2}$ 上国新话区桥热区线电r aui.) translate(u'hace

## 执行结果： Google翻译查证测试结果。结果如下： 可自

Input: <start> hace mucho frio aqui <end>
Predicted translation: it s very cold here <end>

观察图12.32，如果原始语言与目标语言的单字是一一对照的，则热图的对角线应该是关联最大。

除了翻译功能之外，Seq2Seα模型还有其他各种型态和应用， 如图12.33所示。

$$
( 1 ) ~-\bar{x} \bar{y}-
$$
(one to one) 固定长度的输入与输出，即一般
的神经网络模型。例如影像分类,输 $\lambda-$ 张影像后，预测这张影像所属的类别。

(2）一对多（oneto many）：单一输 $\lambda$ ，多个输出。例如影像标题（Inage Captioning 输 $\lambda-$ 个影像后，接着检测影像内的多个对象，并一一给予标题，这称之为「Secuence Output

![](figures/801-3-FIGURE.jpg)

冬 $1 2. 3 2$ 绘制热力图

(20）用户可下载其他语言试试看，包括中英文对照档

![](figures/801-6-FIGURE.jpg)

图12.33 Seoc2Seaq模型的各种型态和应用

(图片来源：The Unreasonable Eifectiveness of Recurrent Neural Networksin

）多对一（many to one）：多个输 $\lambda$ ，单一输出。例如情
(3)
绪分析（Sentiment Analysis) 输 $\lambda-$ 大段话后，判断这段话是正面或负面的情绪表达，这称之为「Sequence input

(4）多对多（many to many) 多个输 $\lambda$ ，多个输出。例如语言翻译（Machine Translation) 输 $\lambda-$ 段英文句子后，翻译成中文，这称之为『Sequence input and sequence outout J

(5）另一种多对多（many to many）：多个输 $\lambda$ ，多个输出同步（Synchronize 例如视频分类（Video Classification），输 $\lambda$ 一段影片后，每一帧（Frame）都各产生一个标题，这称之为 Synced sequence input and output

这里大家先有个基本概念即可，在下一章介绍应用时，我们会再多看一些范例。

Google 的学者Ashish Vaswani等 $\mathcal{A}$ 于2017年依照 Seq2Seq 模型加上注意力机制，提出了Transformer 架构，如图12.34所示。架构一推出后，马上跃身为NLP近年来最受欢迎的算法，而 Attention ls All You Need」一文也被公认是必读的文章，各种改良的算法也纷纷出笼，如BERT、GP $\Gamma/ 2$ 、GPT3、XLNet、
ELMo、T5等， $\mathit{\Pi}$ 乎抢占了NLP大部分的版面。接下来我们就来好好认识Transformer 与其相关的算法。

Google 的学者Ashish Vaswani等 $\mathcal{A}$ 于2017年依照 Seq2Sea 模型加上注意力机制，提出了Transformer架构，如图12.34所 $\overline{{\Pi}}$ 。架构一推出后，马上跃身为NLP近年来最受欢迎的算法，而

## 12-7 Transformer架构

![](figures/803-3-FIGURE.jpg)

K12.34 Transformer 架构

(图片来源：Atinion s A Yu Ned la) RNN/LSTM/GRU 有个最大的缺点，因为要以上文预测目前的二标，必须以序列的方式，依序执行每一个节点的训练，进而导致执行效能过慢。而Transformer克服了比问题，提出自注意力机制 (Self-Attention Mechanism），能够并行计算出所有的输 $\mathtt{H}$ ，计算步骤如下,请同时参考图12.35。

DK：Key Vector，为Encoder隐藏层状态的键值，即上下文的词向量。

④故自注意力机制对应 $\boldsymbol{Q}$ 、 $\boldsymbol{K}.$ 、 $\boldsymbol{V},$ 共有三种权重，而单纯的注意力机制则只有一种权重- -Attention Weight

⑤利用神经网络优化可以找到三种权重的最佳值，然后，以输入向量分别乘以三种权重，即可求得 $\boldsymbol{Q}$ 、 $\boldsymbol{K},$ V向量。

## 12-7-1 原理 Transformer

(1）首先输入向量（Input Vector）被表征为Q、 $\boldsymbol{K}$ 、V向量。

QV: Value Vecior，为 Encoder隐藏层状态的输出值

 $\bigoplus_{\mathbf{3}} \mathbf{Q}$ ：Query Vector，为Decoder的前一期输出

(2）Q、 $\boldsymbol{K}$ 、V再经过下图的运算即可得到自注意力矩阵。

D点积运算： $\boldsymbol{K}$ ，计算输入向量与上下文词汇的相似度

②特征缩放： $\boldsymbol{Q}$ 、K维度开根 $\mp$ ，通常 $\boldsymbol{Q}$ 、K维度是64，故
$$
\sqrt{6 4} \,=8 \, \mathrm{{o}}
$$

今找出要重视的上下文词汇：以Value Vector乘以上述概率较大值为要重视的上下文词汇。

(3）自注意力机制是多头（Mlt-Head）的，通常是 $8$ 个头， 如图12.35 的机制，经过内积运算，串联这 $8$ 个头，如图 12.36所示。

BSotmax运算：将上述结果转为概率

![](figures/805-3-FIGURE.jpg)

图12.35「自注意力机制」运算

## (图片来源：Atention is A Yu eed I)

上图运算过程以数学式表达，即自注意力矩阵公式为

$$
A t t e n t i o n ( Q, K, V )=s o f t m a x ( \frac{Q K^{\mathrm{T}}} {\sqrt{d_{k}}} ) V
$$

式中： $d_{k}$ 为 Key Vector维度，通常是 $6 4$ (4最后加上其他的神经层，就构成了Transformer网络架构。

要了解详细的计算过程请参考「llustrated：Seli-Atenion Lal 一 $\grave{\chi}$ ，它还附有精美的动画；另外， The lltustratecl Transtormerli4 也值得一读。

$$
\div\mp\pi,
$$
自注意力机制就是要找出应该关注的上下文词汇，
举例来说：

其中的t是代表animal还是 street？透过自注意力机制，可以帮我们找出t与上下文词汇的关联度，进而判断出t所代表的是 animal，如图12.37所示。

![](figures/806-4-FIGURE.jpg)

图12.36、「自注意力机制」多头运算

(图片来A ：Ainion Is Al You Needil)

多头自注意力矩阵公式如下：

$$
\begin{array} {c} {M u l t i H e a d ( Q, K, V )=c o n c a t ( h e a d_{1} h e a d_{2} \dots h e a d_{n} ) W o} \\ {w h e r e, h e a d i=A t t e n t i o n ( Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V} )} \\ \end{array}
$$

The animal didn't cross the street because it was too tired. (图片来源：Thellustrated Transformer !)

![](figures/807-1-FIGURE.jpg)

图12.37「自注意力机制」示意图

依SCSelr-atention in L.P. i文中的实验，上述的
Transformer网络在一台 $8$ 颗NVIDIA P100 GPU的服务器上运行， 大约要3.5天才能完成训练，英/德文翻译的准确率（BLEU）约 28 $4$  $\acute{\jmath}$ ，英/法文翻译约41. $8$ 分。BLEU（Bilingual Evaluation
Understudy）是专为双语言翻译所设定的效果衡量指标，是根据n gram 的相符数目（不考虑顺序），乘以对应的权重而得到的分数，详细的计算可参考 $\Gamma\mathbf{A}$ Gentle Introduction to Calculating the BLEU Score for Tet in Python ll文。

由于，笔者没有这么好的设备可以训练模型，再往下钻研细节原理也没什么意 $\mathrm{X}$ ，因此，我们只简单阐述其精神，集中火力在应用层面。

、GPT最为普遍，GPT须在AWS执行，因此我
目前以BERT、
们只介绍 BERT与其应用。

## 12-7-2 BERT (Bidirectional Encoder Representations from
Transtormers）顾名思 $\mathcal{X}$ ，就是双向的Transformer，由Google Jacob Devin 等学者于2018年发表，参阅『BERT： Pretraining of Deep Bidirectional Transformers for Language Understanding l7 一文。

$$
\mathrm{W o r d 2 V e c / G l o v e-\uparrow\#}
$$
字只以一个词向量表示，但是，一词
多 $\grave{\chi}$ 是所有语系共有的现象。例如，Apple 是水果也可以是苹果 $\big<$ 司，Bank是银行也可能是岸边，BERT就能解决这个问题，它是 $\pm$ 下文相关（Gontext Dependent），输入的是一个句子，而不是一个单字，例如：

BERT算法比Transformer更复杂，要花夏长的时间训练，在这里进行介绍，有以下两点原因

(1）它跟CNN一样有预先训练好的模型（Pre-trained
model），可以进行转移学习。BERT分为两阶段，首先是一般的模型训练，之后依不同应用领域进行模型的效能微调（Fine
tuning），类似于CNN 预先训练模型接上自定义的完全连接层，就可以符合各应用领域的需求。

## 12-8 BERT

We goto the river bank.-→ bank 是岸边

need to go to bank to a deposit.-→ bank是银行
make

(2）BERT支持中文模型。 虽然没办法训练模型，但为了在实务上能灵活运用，我们需要理解BERT的运作原理，免得误用。

BERT使用以下两个训练策略

$$
\mathbb{O} \mathsf{M a s k e d} \mathsf{L M} \mathsf{( M L M )} \mathbf{\Delta}_{\circ}
$$

2Next Sentence Prediction (NSP) RNNVLSTM/GRU 都是以序列的方式，逐一产生输出，导致训练速度过慢，而Masked LM（MLM）则可以克服这个问题，训练数据在喂进模型前，有15%的词汇先以[MASK]符号取 $\div\indent$ ，即所谓的屏蔽（Mask），之后算法就试图用未屏蔽的词汇来预测被屏蔽的词汇。MaskedLM的架构如图12.38 所示。

(1）完全连接层（Full-connected layer 对Encoder的输出进行分类。

## 12-8-1 Masked LM

![](figures/811-3-FIGURE.jpg)

图12.3： Masked LM的架构

(图片来源：BERT Explained: State of the artlanguage model for NLPIi8

 $\grave{\imath}$ 算过程如下。

(2）上一步骤的输出乘以词嵌入矩阵，得到字汇表的维度 (Vocabulary Dimension

(3）以 Sotimax换算字汇表内每个词汇的概率 Next Sentence Prediction(NSP）如图 12.39所示。训练时会收到两个字句，NSP预测第 $2$ 句是否是第 $1$ 句的接续下文。训练时会取样正负样本各占 $5 0 \%$ ，进行以下前置处理。

(1）符号词嵌入（Token embedding [CLS]插在第 $1$ 句的前面，[SEP]插在每一句的后面。

(2）字句词嵌入（Sentence embedding) 在每个符号（词汇）上加注它是属于第1句或第 $2$ 句

(3）位置词嵌入（Posiional embedding 在每个符号（词 $\grave{\lambda} \Gamma)$ 上加注它是在合并字句中的第几个位置。

## 12-8-2 Next Sentence Prediction

[MASK] MASK]
Input rcusmy dog cute[SEP] he||likes play ##ing  [SEP] Token EicLs] Ee,. Eaute jMaNO
Embeddings
Sentence
Embedding

Positional E。E,
Transtormer E,Eo Embedding

3 12.39 Next Sentence Prediction (NSP)

(图片来源：BERT Explained: State of the art anguage
 $N \! L \! P^{[ 1 8 ]} )$ 
model for

这三种词嵌入就类似于前面自注意力机制的Q、 $\boldsymbol{K}$ 、 $\boldsymbol{V}_{\circ}$ 预测第 $2$ 句是否为第 $1$ 句的接续下文，处理步骤如下

这三种词嵌入就类似于前面自注意力机制的 $\boldsymbol{Q}$ 、 $\boldsymbol{K}.$ 、 $\boldsymbol{V}_{\circ}$ 

(1）所有输 $\lambda$ 序列导入Transtormer模型 BERT训练时会结合两个算法，目标是最小化两个策略的合并损失函数。

根据BERT GitHub'说明，模型训练在4~16个TPU的服务器上要训练 $4$ 天的时间，因此，我们还是下载预先训练好的模型然后，集中精力在效能微调 $\vdash$ ，比较实际。

(2）将字句进行简单的分类

(3) Softmax，判断是否为接续下文
使用
(IsNextSequence)

效能微调就是根据不同应用领域，加 $\lambda$ 各行业别的知识，使 BERT 能更聪明，有以下应用类型

(1) （Classification) 加一个分类层，进行情绪分析的
分类
判别，可参考BERT GitHub的程序run classifier.py。

(2）问答（Question Answering）：如 SQuAD 数据集，输入一个问题后，能够在全文中标示出答案的开头与结束的位置，可参考BERT GitHub的程序run souad.py。

(3）命名实体识别（Named Entity Recognition, NER) 输 $\lambda$ 一段文字后，可以标注其中的实体，如人名、组织、口期等。

(1）GLUE（httos:/lgluebenchmark.com/tasks）下载数据集，较有名的是Quora Question Pairs，它是科技问答网站的问题配对，标签是相似与否。

(2）下载预先训练模型BERT-Base，解压缩至一目录，如 BERT BASE DIR所指向的目录。

(3）效能微调：下列是Linux指令，Windows作业环境下可直接把变量带 $\lambda$ 。

## 12-8-3 BERT效能微调

以分类为例，测试处理程序如下。

(5）预测：参数do predict-true，输 $\lambda$ 放在 input/test.tsv, 执行结果则在 outout/test results.tsv。

问答（Question Answering）以 SQuAD 数据集为例，程序与上述程序类似。

注意：效能微调使用GPU时，BERT GitHub 建议为Titan X 或GTX1080，否则容易发生内存不足的情形。看到这里，可能很

![](figures/816-3-FIGURE.jpg)

## (4）得到结果如下：

| * | Eval results 水水零冰冰 |
|  | l accuracy - 0.845588 |
| eval loss = 0.505248 |
| global step = 343 |
|  | s=0.505248 |

![](figures/816-6-FIGURE.jpg)

多读者（包括笔者）脸上又不明白 $\overline{{\jmath}}$ ，还好，有一些库可以让我们直接实践，不须使用上述程序

Transtormers库的功能十分强大，它支持数十种模型，包括 BERT、GPT、T5、XLNet、XLM等架构，详情请参阅
Transformers GitHub2

接下来我们就拿Transformers这个库当例子，做一些实验它包含以下功能。

(8）特征提取（Feature Extraction 类似词向量，将文字转换为向量。

## 12-9 Transformers 库

(1）情绪分析（Sentiment analysis)
(2）文字生成（Text Generation）：限英文。
(3）命名实体识别（Named Entity Recognition, NER)
(4）问题回答（Question Answering)
(5）克漏字填空（Filing Masked Text
(6）文字摘要（Text Summarization）：将文章节录出大意 (7）翻译（Translation

(1）情绪分析（Sentiment analysis O

(2）文字生成（Text Generation）：限英文。

(3）命名实体识别（Named EnttyRecognion, NER O

(4）问题回答（Question Answering) O

(5）克漏字填空（Filin Masked Text O

(6）文字摘要 Summarization）：将文章节录出大意
(Text

 $( 7 )$ 翻译（Translation O

（2）加载模型：BERT有许多变型，下列指令默认下载
distilbert-base-uncased-finetuned-sst-2-englisn 模型，使用「The Stanford Sentiment Treebank」 (SST-2）数据集进行效能微调。 程序代码如下：

## 12-9-1 库范例 Transformers

先安装库，指令如 $\mathbf{F}$ :

$$
\mathrm{p i p ~ i n s t a l l ~ t r a n s f o r m e r s}
$$

范例 ${\bf1}$ .情绪分析

请参阅程序：12 08 BERT_情绪分析ipynb，修改自 Transiormers官网的「Quick tour」2l

1）加载相关库。程序代码如下

1 #载人相关库
2 from transformers import pipeline

(3）情绪分析测试。程序代码如下执行结果：非常准确，否定句也可以正确分类，不像之前的 RNN/LSTM/GRU 碰到否定句都无法正确分类。结果如下

执行结果：非常准确，就连否定句有可能是中性的这点也能够分辨，如don'thate 不讨厌，但不意味是喜欢，所以分数只有0.5

(5）多语系支援：BERT支持100多种语系，提供24种模型， $\mathcal{Y}$ BERT-base 的文件名 uncased L-12 H-768 A-12.zip为例来说明，L-12 表示 ${\bf1 2}$ 层神经层，H-768表示768个隐藏层神经元， $\mathrm{A^{-} 1 2}$ 表示12个头。

[{'label': 'POSITIVE score': 0.9997795224189758}] [{'label': NEGATIVE score': 0.9996869564056396}] [{'label': POSITIVE score': 0.999536395072937}]

(4）一次测试多批。程序代码如下

![](figures/820-5-FIGURE.jpg)

$$
\fbox{1 a b e 1 \! : \mathtt{P O S I T I V E}, \mathtt{w i t h} \mathtt{s c o r e : \mathtt{O. 9 9 9 9}}} \, \mathtt{l a b e l \! : \mathtt{N E G A T I V E}, \mathtt{w i t h} \mathtt{s c o r e : \mathtt{O. 5 3 0 9}}}
$$

(6）西班牙文（Spanish） 。程序代码如下
测试。

## 执行结果：不是很准确，分数都非极端值。结果如下 (2）加载模型：参数须设为question-answering。程序代码如下：

 $( 7 )$ 法文（French 。程序代码如下
测试。

## 执行结果：不是很准确，分数都非极端值。结果如下

[{'label': '1 star score': 0.631117582321167}] [{'1abel': '3 stars score': 0.5710769295692444}]

范例 $2$ .问题回答

请参阅程序： ${\bf1 2 \_0 9}$ BERT 问题回答.ipynb，修改自 Transformers官网「Sumna $\mathbf{y}$ ofthe tasks」的Exractive Question Answering 33l

1）加载相关库。程序代码如下

1#载人相关库
2 from transformers import pipeline

## (3）设定训练数据。程序代码如下

![](figures/821-8-FIGURE.jpg)

(4）测试两批数据。程序代码如下 ## 执行结果：非常准确，通常是从训练数据中节录一段文字当作回答。结果如下:

(httos/huggingtace.oransiormersiast tokenizers.html) 下百使用预设的分词器。

Answer: the task of extracting an answer from text given a question score: 0.6226 start: 33, end: 94 Answer: 'SQuAD dataset score: 0.5053 start: 146, end: 159

(5）结合分词：可自定义分词器，断句会比较准确，参阅 Using tokenizers from Tokenizersl

(6）载入分词器。程序代码如下

## (7）加载训练数据。程序代码如下

1#训练数据
2 text=r"
3 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose
architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet...) for Natural Language Understanding (NLU) and Natural 5 Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between
6 TensorFlow 2.0 and PyTorch.
7 江Ue

(s）设定问题。程序代码如下请参阅程序:12 10 BERT 填漏字.ipynb，修改自
Tansformers官网「Summary ofthe tasks」的 Masked Language Modeling 23

## (9）推测答案。程序代码如下

![](figures/823-2-FIGURE.jpg)

## 执行结果：非常准确。结果如下：

![](figures/823-4-FIGURE.jpg)

Question: Transformers provides interoperability between which frameworks? Answer: tensorflow 2. $\big.$ and pytorch

## 范例3。克漏字填空。

1）加载相关库。程序代码如下

(2）加载模型：参数须设为fl-mask。程序代码如下： 执行结果：列出前 $5$ 名与其分数，框起来的即是填上的字。结果如下：

## (3) 。程序代码如下测试。

![](figures/824-2-FIGURE.jpg)

## 果如下：

![](figures/824-4-FIGURE.jpg)

## (4) 合分词。程序代码如下

![](figures/824-6-FIGURE.jpg)

(5）推测答案。程序代码如下执行结果：列出前 $5$ 名与其分数，框起来的即是填上的字。结果如下：

范例 $\boldsymbol{4}$ 。文字生成：这里使用GPT-2算法，并非BERT，同属于Transformer算法的变形，目前已发展到 GPT-3。

请参阅程序：12 11_GPT2 文字生成.ipynb，修改自
Transiormers官网「Summary oi the tasks」的 Text Generation [24]

(3）测试：max length-s0 表示最大生成字数
do sample-False 表示不随机产生，反之为True时，则每次生成

![](figures/825-4-FIGURE.jpg)

(1）加载相关库。程序代码如下

![](figures/825-6-FIGURE.jpg)

(2）加载模型：参数须设为 text-generation。程序代码如下:

![](figures/825-8-FIGURE.jpg)

的内容都会不同，如聊天机器 $\mathcal{A}$ ，使用者会期望机器人表达能够有变 $\Psi$ ，不要每次都回答一样的答案。例如，问「How are you? 机器 $\mathcal{A}$ 有时候回答 $\Gamma_{\parallel}$ am fine」，有时候回笞「Great」「Not bad」。程序代码如 $\mathrm{T}$ :

(4）测试：do sample-True 表示随机产生，每次生成内容均不同。程序代码如下：

(5）结合分词：这里使用XLNet 算法，而非BERT，也属于 Transformer算法的变形。程序代码如 $\mathrm{F}$ :

(6）提示：针对短提 $\overline{{\Pi}}$ ，XLNet 通常要补充说明
$$
( \mathrm{P a d d i n g} )
$$
因为它是针对开放式（open-ended）问题而设计
的，但 GPT-2 则不用。程序代码如下：

![](figures/826-4-FIGURE.jpg)

## 执行结果：每次生成的内容均相同。结果如下：

[{'generated text': 'As far as I am concerned, I will be the first to admit that I am not a fan of the idea of a "free market." I think that the idea of a free market is a bit of a stretch. I think that the idea'}]

![](figures/826-6-FIGURE.jpg)

## 执行结果：每次生成的内容均不相同。结果如下：

[{'generated text': 'As far as I am concerned, I will not be using the name \'Archer\', even though it\'d make all of me cry!\n \n"I\'ll wait until they leave me, you know, on this little ship, of course, '}]

![](figures/826-9-FIGURE.jpg)

(6）提示：针对短提示，XLNet通常要补充说明 ## (7 推测答案。程序代码如下

1#推测答案
饰 $[ 0 ]$ , skip special tokens=True, inputs = tokenizer.encode(PADDING TEXT + prompt, add special tokens=False,
return tensors="tf")
clean up tokenization spaces=True)) outputs = model.generate(inputs, max length=250, do sample=True, top p=0.95,
top k=60)
8 generated = prompt + tokenizer.decode(outputs[0])[prompt length:]
9
10 print(generated)

## 执行结果如下：

## 范例5. ：预设使用CoNLL-2003 NER数据集。 命名实体识别：

请参阅程序：12 12 BERT NER.ipynb，修改自
Transiormers 官网「Summary of the tasks」的 Naned Eniiy Recognition 23s

(1）加载相关库。程序代码如下

(2）加载模型：参数须设为ner。程序代码如下:

D执行结果：显示所有实体（Entity） word 字段中以#彬开头的，表示与其前一个词汇结合也是一个实体，如#gging，前一个词汇为Hu，即表示Hu、Hugging 均为实体。

(3) 。程序代码如下测试。

![](figures/828-2-FIGURE.jpg)

Penty中段有以下实体类别。

0：非实体。
B-MISC：杂项实体的开头，接在另一个杂项实体的后面。
-MISC:杂项实体。
B-PER：人名的开头，接在另一个人名的后面。
I-PER：人名。
B-ORG：组织的开头，接在另一个组织的后面。
I-ORG：组织。
B-LOC：地名的开头，接在另一个地名的后面。
l-LO0：地名。

$$
\bullet\quad\mathrm{O : ~ \exists~ E \Rightarrow~ 1 ~ \hbar~ o}
$$

 $\bullet\quad\mathrm{I-M I S C :}$ 杂项实体。

B-PER：人名的开头，接在另一个人名的后面

B-ORG：组织的开头，接在另一个组织的后面

$$
\mathrm{~ H O R G : ~ \pm~ H \pm~ R ~ o}
$$

B-LOC：地名的开头，接在另一个地名的后面

 $\bullet\vdash\mathrm{L O C} :$ 地名 $\circ$ 

|  | word | score | entity | index | start |  |
| --- | --- | --- | --- | --- | --- | --- |
| 0 | Hu | 0.999511 | I-ORG | 1 | 0 |  |
| 1 | ##gging | 0.989597 | I-ORG | 2 | 2 |  |
| 2 | Face | 0.997970 | I-ORG | 3 | 8 |  |
| 3 | Inc | 0.999376 | I-ORG | 4 | 13 |  |
| 4 | New | 0.999341 | I-LOC | 11 | 40 |  |
| 5 | York | 0.999193 | I-LOC | 12 | 44 |  |
| 6 | City | 0.999341 | I-LOC | 13 | 49 |  |
| 7 | D | 0.986336 | I-LOC | 19 | 79 |  |
| 8 | ##UM | 0.939624 | I-LOC | 20 | 80 |  |
| 9 | ##BO | 0.912139 | I-LOC | 21 | 82 |  |
| 10 | Manhattan | 0.983919 | I-LOC | 29 | 113 |  |
| 11 | Bridge | 0.992424 | I-LOC | 30 | 123 |  |

## (4）结合分词。程序代码如下

1#载人相关库
Snmrtrenrnrnornaiation unonter import tensorflow as tf
4
5 #结合分词器(Tokenizer）
6 model name = "dbmdz/bert-large-cased-finetuned-conll03-english"
7 model = TFAutoModelForTokenClassification.from pretrained(model name) 8 tokenizer = AutoTokenizer.from pretrained("bert-base-cased")

## (5) 。程序代码如下测试。

![](figures/829-4-FIGURE.jpg)

范例 $\mathbf{6}$ 。文字摘要：从篇幅较长的文章中整理出摘要，测试的数据集是 CNN和 Daily Mail媒体刊登的文章

请参阅程序：1213文字摘要 $\mathbf{i p y n b}$ ，修改自Tanstormers官网的 Summarization !26

执行结果：代码说明可参照程序代码的第 $3 \mathrm{~^{3} ~} 1 1$ 行。结果如下：

[('[CLS]', 'O'), ('Hu', 'I-ORG'), ('##gging' 'I-ORG'),('Face','I-ORG'), ('Inc','I-ORG'),（ 'O'),('is', o'),'a'
'0'), ('company', 'O'), ('based', 'O"), ('in" 'O'),（'New' I-LOC"), ("York', 'I-LOC'), ('City' 'I-LOC'),（ O'),（'It
'O'),（"headquarters 0'） are O "in' '0', 'I-LOC'),("##UM', 'I-LOC'), ('##BO', 'I-LOC'), '0'), ' therefore' 'O'),（'very' 'O'),('##c' 'O'),('##lose ,('to', '0'), ('the', 'O'), ("Manhattan' I-LOC'),（'Bridge I-LOC'), '0'), ("[SEP]', 'O')]

1）加载相关库。程序代码如下

(2）加载模型：参数须设为 summarizaion。程序代码如下：

1#载入模型
 $2$ 
$$
{\mathsf{s u m m a r i z e r}} \,=\, {\mathsf{p i p e l i o n e}} \, {\bigl(}^{n} {\mathsf{s u m m a r i z a t i o n "}} {\bigr)}
$$

## (3) 。程序代码如下测试。

1#测试数据
2 ARTICLE = """ New York (CNN) hen Liana Barrientos was 23 years old, she got married in Westchester County, New York.
3 A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband. 4 Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared "I do" five more times, sometimes onl 5 In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her "fir 6 Barrientos, now 39, is facing two criminal counts of "offering a false instrument for filing in the first degree," referring 7 2010 marriage license application, according to court documents.
8 Prosecutors said the marriages were part of an immigration scam.
9 On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who de 10 After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking 11 Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurrin 12 All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four 13 Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after 14 Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.
15 The case was referred to the Bronx District Attorney\'s Office by Immigration and Customs Enforcement and the Department of 16 Investigation Division. Seven of the men are from so-called "red-flagged" countries, including Egypt, Turkey, Georgia, Pakis 17 Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism lg i. convicted, Barrientos faces up to four yers in prison. er net court apeane i sheuld for ay 18.
18
20
21 #推测/答案
22 print(summarizer(ARTICLE, max length=130, min length=30, do sample=False))

执行结果：摘要内容尚可。结果如下：

(4）结台Tokenizer： T5是 Google Text-To-Text Transier Transformer的模型，它提供一个框架，可以使用多种的模型、损失函数、超参数，来进行不同的任务（Tasks），如翻译、语意接受度检查、相似度比较、文字摘要等，详细说明可参阅 $\bigskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\medskip\ cm \ cm \ cm \ cm \ cm \ cm \ cm \ cm \ cm \ cm \right$ Transter Learning with 75: the Text-To-Text Transfer
Transtormert?l。

执行结果：T5最多只可输入 $5 1 2$ 个词汇，故将多余的文字截断，产生的摘要也还可以看得懂。结果如 $\mathrm{F}$ :

范例 $\mathbf{7}$ .翻译功能：使用WMT English to German dataset (英翻德数据集)

请参阅程序：12 14 T5 翻.pynb，修改自Transformers官网的Translation 2

![](figures/831-4-FIGURE.jpg)

![](figures/831-5-FIGURE.jpg)

1）加载相关库。程序代码如下 ## (2）加载模型：参数设为 translation en to de 表示英翻德程序代码如下： ## 2）加载模型：参数设为translaion en to de 表示英翻德。

## (3) 。程序代码如下测试。

1#测试数据
2 text = "Hugging Face is a technology company based in New York and Paris" 3 print(translator(text, max length=40))

## 执行结果如下：

## ）结台Tokenizer。程序代码如下： (4)

![](figures/832-6-FIGURE.jpg)

## 执行结果如下： Transformers 也提供了效能微调的功能，可参阅 Transtormers 官网Training and'fie-tuning”的网页说明，我们现在就来练习整个程序。Transformers 效能微调可使用以下三种方式。

请参阅程序：12 15 BERT Train.ipynb，修改自参考资料 [30]

## 12-9-2 Transiormers 库效能微调

$$
( 1 ) ~ \mathrm{\ T e n s o r F l o w \lor2_{o}}
$$
$$
( 2 ) \ \, \mathrm{P y T o r c h}_{\circ}
$$
$$
\mathrm{( 3 ) ~ \operatorname{T r a n s f o r m e r s} ~ \''~} \mathrm{T r a i n e r}_{\diamond}
$$

$$
( 1 ) ~ \mathrm{\ T e n s o r F l o w \lor2_{o}}
$$

$$
( 2 ) \mathrm{~ P y T o r c h}_{\circ}
$$

$$
\mathrm{( 3 ) ~ \operatorname{T r a n s f o r m e r s} ~ \sharp~ \sharp~ \operatorname{T r a i n e r}_{\circ} ~}
$$

范例1，直接以Transformers的 Trainer进行效能微调

12.40所示设计流程如图

![](figures/833-9-FIGURE.jpg)

冬 $1 2$ 40 设计流程

(1）安装Datasets 库：GLUEBenchmark
(https:/gluebenchmark.com/tasks）包含许多任务与测试数据

(4）加载数据集、效果衡量指标：每个数据集有不同的效果衡量指标。程序代码如下：

(5）显示Dataset 数据内容：Dataset数据类型为
DatasetDict，可参考Transformers 官网的「DatasetDict 说明文件」
(nttps://huggingface.co/docs/datasets/package reference/main classes.html#datasetdict) 程序代码如下

(htts/huggingtace.codos/datasetspackage reference/main classes.html#datasetdict）。程序代码如下：

执行结果：训练数据8551批，验证数据1043批，测试数据 1063批。结果如下

$$
\sharp_{\circ}
$$

 $\mathrm{\ p i p}$ install datasets

(2）定义GLUE所有任务。程序代码如下：

![](figures/834-7-FIGURE.jpg)

）指定任务为cola。程序代码如下: (3)

![](figures/834-9-FIGURE.jpg)

![](figures/834-10-FIGURE.jpg)

![](figures/834-11-FIGURE.jpg)

![](figures/835-0-FIGURE.jpg)

(6）显示第一批内容。程序代码如下：

## 执行结果：为正面/负面的情绪分析数据。结果如下：

(7）定义随机抽取数据函数。程序代码如下

![](figures/835-4-FIGURE.jpg)

## (s）随机抽取10批数据查看。程序代码如下

 $^1_{2}$ #随机抽取10批数据查看
show random elements(dataset"train"])

执行结果如下： 执行结果：包含准确率（Accuracy） F1、Pearson关联度 (Correlation) Spearman 关联度、Matthew关联度。结果如下

）模型分词：前置处理以便于测试，可取得生字表
(11)
(Vocabulary) 设定 use fast-True 就能够快速处理。程序代码

|  | idx | label | sentence |
| --- | --- | --- | --- |
| 0 | 2722 | unacceptable | Abicycle lent to me |
| 1 | 6537 | unacceptable | Who did you arrange for to come? |
| 2 | 1451 | unacceptable | The cages which we donated wire for the convicts to build with are strong |
| 3 | 3119 | acceptable | Cynthia munched on peaches. |
| 4 | 3399 | acceptable | Jackie chased the thief |
| 5 | 4705 | acceptable | Nina got Bill elected to the committee |
| 6 | 1942 | unacceptable | Every student who ever goes to Europe ever has enough money |
| 7 | 5889 | acceptable | Bob gave Steve the syntax assignment |
| 8 | 4162 | acceptable This  | his Government have been more transparent in the way they have dealt with public finances than any previous government. |
| 9 | 1261 | acceptable | I know two men behind me |

(9）显示效果衡量指标。程序代码如下：

$$
\mathrm{F} \! :
$$

gritme"ge" fatrn: $\{$ predictions': Value(dtype-'int64', id-None), Compute GLUE evaluation metric associated to each GLUE dataset.
Args:
predictions: list of predictions to score.
Each translation should be tokenized into a list of tokens.
references: list of lists of references for each translation.
Each reference should be tokenized into a list of tokens.
Returns: depending on the GLUE subset, one or several of:
"accuracy": Accuracy
"f1": F1 score
"pearson": Pearson Correlation
"spearmanr": Spearman Correlation
"matthews correlation": Matthew Correlation

10）产生两批随机数，测试效果衡量指标。程序代码如下：

1#产生两批随机乱数，测试效果衡量指标
2 import numpy as np
3
4 fake preds = np.random.randint(O, 2, size=(64,))
5 fake labels = np.random.randint(o, 2, size=(64,))
6 metric.compute(predictions=fake preds, references=fake labels) 执行结果·Ourfriends won't buythis analysis let alone the next one we propose.

## 如下：

## 12）测试两批数据，进行分词。程序代码如下：

![](figures/837-3-FIGURE.jpg)

## （13）定义任务的数据集字段。程序代码如下

![](figures/837-5-FIGURE.jpg)

## 14）测试第一批数据。程序代码如下

| 1 | #测试第一批数据 |
| 2 | sentencel key, sentence2 key - task to keys[task] |
| 3 | if sentence2 key is None: |
| 4 | print(f"Sentence: {dataset['train'][o][sentence1 key]}") |
| 5 | else: |
| 6 | print(f"Sentence 1: {dataset['train'][o][sentence1 key]}") |
| 7 | print(f"Sentence 2: {dataset['train'][o][sentence2 key}") |

1s）测试 $5$ 批数据分词。程序代码如下

(17）定义训练参数：可参阅Transformers 官网的
TrainingArguments 说明文件
https://uggingtace.co/transiormers/main classes/trainer.htmi#ra nsformers.TrainingArguments。程序代码如下：

httos:/huggingiace.co/transformers/main classestrainer.htm#tra nsformers. rainingArguments。程序代码如 $\mathrm{F}$ :

## 执行结果如下：

{'input ids': [[101, 2256, 2814,218 $\begin{array} {l} {{9, ~ 1 0 0 5, ~ 1 0 0 6, ~ 1 9 9 5, ~ 2 0 0 3, ~ 4 1 0 6, ~ 1 0 0 3, ~ 1 0 1 0, ~ 2 0 0 1 3, ~ 2 0 1 0 1 3, ~ 1 0 1 3, ~ 1 0 1 3, ~ 1 0 1 3, ~ 1 0 1 3, ~ 1 0 0 1 3, ~ 1 0 0 1 3, ~ 1 0 0 1 3, ~ 1 0 0 1 3, ~ 1 0 0 1 3, ~ 1 0 0 1 3, ~ 1 0 0 1 3, ~ 1 0 1 3, ~ 1 0 1 3, ~ 1 0 1 3, ~ 1 0 1 3, ~ 1 0 1 3, ~ 1 0 1 3, ~ 1 0 1 3, ~ 1 0 1 3, ~ 1 0 1 3, ~ 1 0 1 3, ~ 1 0 0 3, ~ 1 0 0 3, ~ 1 0 0 3, ~ 1 0 0 3, ~ 1 0 0 3, ~ 1 0 0 1 3, ~ 1 0 0 1 3, ~ 1 0 0 1 3, ~ 1 0 0 0, ~ 1 0 0 2, ~ 1 0 0 2, ~ 1 0 2, ~ 1 0 2, ~ 1 0 2, ~ 1 0 3 3, ~ 1 0 2, ~ 1 0 2, ~ 1 0 2, ~ 1 0 3, ~ 1 0 3, ~ 1 0 0 3, ~ 1 0 0, ~ 1 0 0, ~ 1 0 0, ~ 1 0 0, ~ 1 0 0, ~ 1 0 1 3, ~ 1 1, ~ 1, 1, 1, ~ 1, 1, ~ 1, 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1, ~ 1$ 2],[101,2028,2062,18404,2236，3
030,1045,1005,1049,3228,2039,1
1012，102],[101，2154， 2011，2154，
1,1,1,1,1,1,1,1,1,1，1，1，
1],[1, 1, 1,1, 1, 1, 1,1, 1, 1,1

## 16）加载预先训练的模型。程序代码如下

![](figures/838-5-FIGURE.jpg)

## (17）定义训练参数：可参阅Transiormers官网的

s说明文件」

![](figures/838-8-FIGURE.jpg)

18）定义效果衡量指标计算的函数。程序代码如下

(19）定义训练者 $( \mathrm{T r a i n e r} )$ 对象：参数包含额外增加的训练数据。程序代码如下

(20）在预先训练好的模型基础上继续训练，即是效能调整， 在笔者的PC上训练了20小时。程序代码如下

| 1 | #定义效果衡量指标计算的函数 |
| --- | --- |
| 2 | def compute metrics(eval pred): |
| 3 | predictions, labels = eval pred |
| 4 | if task != "stsb": |
| 5 | predictions = np.argmax(predictions, axis=1) |
| 6 | else: |
| 7 | predictions = predictions[:, 0] |
| 8 | return metric.compute(predictions=predictions, references=labels) |

![](figures/839-3-FIGURE.jpg)

## 执行结果如下：

| Epoch | Training Loss | Validation Loss | Matthews Correlation | Runtime | Samples Per Second |
| --- | --- | --- | --- | --- | --- |
| 1 | 0.519900 | 0.484644 | 0.437994 | 301.078100 | 3.464000 |
| 2 | 0.352600 | 0.519489 | 0.505773 | 299.051900 | 3.488000 |
| 3 | 0.231000 | 0.538032 | 0.556475 | 1863.316700 | 0.560000 |
| 4 | 0.180900 | 0.733648 | 0.515271 | 241.590500 | 4.317000 |
| 5 | 0.13070 | 0.787703 | 0.538738 | 242.532000 | 4.300000 |

## 训练时间统计如下：

(21）模型评估。程序代码如下

(22）新模型存盘：未来就能透过from pretrained(加载此效能调整后的模型进行预测。程序代码如下

## 执行结果如下：

'eval loss': 0.5380318760871887,
eval matthews correlation': 0.5564748164739529 'eval runtime': 229.9053,
'eval samples per second': 4.537,
'epoch': 5.0,
eval mem cpu alloc delta': 507904,
eval mem gpu alloc delta': o,
eval mem cpu peaked delta': $\omicron$ 
eval mem gpu peaked delta': 20080128}

1#模型存档
$$
\begin{array} {c} {2} & {\mathrm{t r a i n e r. \, s a v e \_\ m o d e l \, (^{\, \, "}. / c o l a^{\, \prime} )}} \end{array}
$$

）新数据预测。程序代码如下： (23)

![](figures/840-5-FIGURE.jpg)

## 执行结果：每笔以最大值作为预测结果。结果如下：

Predictionoutput(predictions=array([[-0.55236566, 0.32417056],
[-1.5994813 1.4773667 ]], dtype=float32), label ids=None, metrics={'test runtime': 4.0388, "test samples per second': 0.495, "test mem cpu alloc delta': 20480, "test mem gpu alloc delta': 0, "test mem cpu peaked delta': 0, "test mem gpu_peaked d elta': 609280})

124）之后可进行参数调校，笔者不再继续进行

以上只就官方的文件与范例介绍，Transiormers 模型的功能非常多，要熟悉完整功能，尚待读者后续努力实验，BERT的变形不 $\ ~$ ，这些变形统称为BERTology，预先训练的模型可参阅「官网 Pretrained modelsy

(htios:/huggingface.co/transformers/pretrained models.html 有的提供轻量型模型，如ALBERT、TinyBERT，也有的提供更 $\vec{\pi}$ 整的模型，如GPT-3，号称有1750亿个参数，更多内容可参阅 $\Gamma_{\bf A I}$ 天 142期报导311
趋势周报第

Transformer 架构的出现已经完全颠覆了NLP 的发展，过往的 RNN/LSTM 模型虽然仍然可以拿来应用，但是，遵循Transformer 架构的模型，它们在准确率上确实有比较明显的优势，因此，推测后续的研究方向应该会逐渐转移到Transformer架构 $\underbar{l}$ ，而且它不仅可以应用于NLP，也开始将触角伸向影像辨识领域，口此可见 Transformers 套件变得日益重要，详情可参阅 $\Gamma_{\bf A I}$ 趋势周报第167 期报导」3。

## 12-9-3后续努力

这一章我们介绍了处理自然语言的相关模型与其演进过程，包括RNN、LSTM、GRU、注意力机制、Transformer、BERT等， 同时也实践许多范例，如情绪分析、神经机器翻译（NMT）、字句相似度的比对、问答系统、文字摘要、命名实体识别 $\mathrm{( N E R )}$ 、时间序列预测等。相信各位对于NLP应用应该有了基本的认识，若要能灵活应用，还是需要找些项目或题二实践，毕竟成功源于细节。提醒 $- \mathrm{T}$ ，由于目前BERT系列的模型在准确度方百已经超越 $\overline{{\jmath}}$ RNN/LSTM/GRU，所以如果是项目应用，建议应优先采用
BERT模型。

## 12-10总结

这几年NLP 的应用范围相当广泛，如聊天机器人
(ChatBot）， $\mathit{\Pi}$ 乎每一家企业都有这方面的需求， ${\cal M}$ 售前支持 (Pre-sale）、销售（Sales）到售后服务（Post Services）等方面，用途一分多元，而支持系统功能的技术则涵盖了NLP、NLU、 NLG，既要能解析对话（NLP）、理解问题（NLU），又要能回答得体、幽默、周全（NLG），技术范围几乎整合了上一章所有的范例。另外，如果能结合语音识别，用说话代替打字，这样不论身处何时何地，人们都能够更方便地用手机与机器沟通，或是结合其他的软/硬件，如社群软件、智能音箱等，使得计算机可以更贴近用户的需求，提供人性化的服务，以往只能在电影里看见的各种科技场景正逐渐在我们的日常生活中成为现实，如图13.1所示。

话说回来，要开发一个功能完善的 ChatBot，除了技术之外更要有良好的规划与设计作为基础，当中有哪些重要的技巧和窍门呢？现在就跟大家来一探究竞

## 第13章聊天机器人

## 第13章

## 聊天机器人

![](figures/843-5-FIGURE.jpg)

图13.1 ChatBot商业应用

广义来说，ChatBot不一定要具备 AI的功能，只要能自动应答信息，基本上就称为ChatBot。通常一说到聊天机器 $\mathcal{A}$ ，大家直觉都会想到苹果 $\big< \lambda$ 司的Siri，它可以跟使用者天南地北地聊天，不管是天气、金融、音乐、生活信息都难不倒它，但是，对于一般中小企业而言，这样的功能并不能带来商机，他们需要更直接的支持功能，因此我们把ChatBot分为以下类别。

(1）不限话题的机器 $\mathcal{A}$ ：可以与 $\mathcal{A}$ 天南地北地闲聊，包括 $\big<$ 开信息的查询与应答，如温度、股市、播放音乐等，也包含口常寒暄，不需要精准的答案，只要有趣味性、实时回复。

（2）任务型机器 $\mathcal{A}$ ：如专家系统，具备特定领域的专业知 $\grave{\i} \P$ ，服务范围如医疗、驾驶、航行、加密文件的解密等，着重在复杂的算法或规则式（Rule Based）的推理，需给予精准的答案，但不求实时的口复。

(3）常见问答集（Frequently Asked Questions, FAQ）：客服中心将长年累积的客户疑问集结成知识库，当客户询问时，可快速搜寻，找出相似的问题，并将对应的处理方式回复给客户，答复除了要求正确性与遵循话题之外，也讲究内容是否浅显易懂和详细严密，避免重复而空泛的回答，引发客户不耐烦与不满。

(4）信息检索：利用全文检索的功能，搜寻关键词的相关信 $\boldsymbol{\Theta}$ ，如Google搜导，不需要完全精准的信息，也不要单一的答案，而是提供所有可能的答案，由使用者自行做进一步判断。

## 13-1 ChatBot美别

(5）数据库应用：借白SQL指令来查询、筛选或统计数据如旅馆订房、餐厅订位、航班查询/订 $\overrightarrow{\Omega}$ 、报价等，这是最传统的需求，但如果能结合NLP，让输出输入接口更友善，如语音输 $\lambda_{l}$ 输 $\uplus$ ，可以带来新一波的商机。

以上这五种类别的 ChatBot 各有不同的诉求，功能设计方向也因而有所差异，所以，在开工之前，务必要先搞清楚需求，以免开发出来不符合需求。

上一节谈到的 ChatBot种类非常多元，如果就每一种应用都详细介绍的话未免过于烦琐，所以本节仅对共同的关键功能进行说明。

(1）订订目标：根据规划的目标，选择适合的ChatBot类别，可以是多种类别的混合体。

(2）收集应用案例（Use Gase）：收集应用的各种状况和场景，整理成案例，以航空机票的销售来举例，包括了每日空位查询、旅程推荐、订票、付款、退换票等，分析每个案例的现状与导 $\lambda$ ChatBot 后的场景与优点。

(3）提供的内容：现在营销是内容为王（Content is king）的时代，有内容的信息才能吸引人潮并带来商业潮，这就是大家常听到的内容营销（Content Marketing）。因此，要评估哪些信息是有效的，又该如何生产，并以何种方式呈现（如Video、PodCast、 部落文等）

 $\bigoplus$ 软件包：现在已有许多厂商提供某些行业级别的解决方案，如金融、保险等各行各业，技术也从传统的「VR顺势转为 ChatBot，提供更便利的使用接口。

## 13-2 ChatBot设计

ChatBot 的规划要点如下。

(4）挑选开发平台，有以下四种方式供选择。

 $\bullet$ ChatBot 平台：许多大型系统厂商都有提供 ChatBot 平 $\overleftrightarrow{\Pi}$ ，他们利用独有的NLP技术以及大量的NER信息，整合各种社群软件，用户只要直接设定，就可以在云端使用 ChatBot并享有相
DialogFlow、微软QnAMaker等。 关的服务。厂商包括Google

 $\bigoplus$ 开发二具：许多厂商提供开发工具，方便工程师快速完成一个ChatBot,如Microsoft Bot Framework、Wit.ai等，可参阅 $1 O$ Best Chatbot Development Frameworks to Build Powerful Bots 叫，另外 Google、Amazon智能音箱也都提供了 SDK。

(5）部署平台：可选择云端或本地端，云端可享有全球服务或以微服务的方式运作，以使用次数计费，可节省初期的高额支 $\uplus$ ，因此，若ChatBot 不是数据库交易类别的话，会有越来越多企 $\gnu$ 采用云端方案。

(6）用户偏好（Preference）与面貌 $( \mathrm{P r o f i l e} )$ 考虑要存储哪些与业务相关的用户信息。

(1）技能Klil 例如银行的技能包含存提款、定存、换 $\grave{\lambda} \Gamma.$ 、基金购买、房贷等，每一个应用都称为一种技能。

自行构建：可以利用库加速开发，如TextBlob、 Gemsim、SpaCy、Transforms等

NLP 函数库，或是Fasa、ChatterBot等ChatBot Open Source.

ChatBot的术语定义如下

(2）意图（Intent）：技能中每一种对谈的用意。例如，技能是旅馆订房，意图则是查询某日是否有空的双人房、订房、换日期、退房、付款等。

(3）实体 $( \mathrm{E n t i t y} )$ ：关键的 $\mathcal{A}$ 事时地物，利用前面所提的命名实体识别（NER）找出实体，每一个意图可指定必要的实体，如旅馆订房必须指定口期、房型、住房天数、身份证号码等。

(4）例句（Uterance) 因为不同的人表达同一意图会有各种不同表达方式，所以需要收集大量的例句，训练、ChatBot，如 「我要订 $3$ 月21日双人房」「明天、双人房一间」等

(5）行动（Action）：所需信息均已收集完整后，即可作出响 $\overrightarrow{\Psi}$ （Response）与相关的动作，如订房，若已确定日期、房型、 住房天数、身份证号后，即可采取行动，为客人保留房间，并且响应客户「订房成功」

(Welcome 问候语（Greeting）等，通常要有一些例句供随机使用，避免一成不变，流于枯燥。

(1）对话管理：有两种处理方式，有限状态机（Finite-State Machine,FSM）和槽位填充
（Slot Filling)

 $\bigoplus$ 有限状态机：传统的自动语音应答系统 $\mathrm{( I V R )}$ 大多采取这种方式，事先设计问题顺序，确认每一个问题都得到适当的回答，才会进到下一状态，如果中途出错，就退回到前一状态重来 (6）开场白（Opening Message 如欢迎词

对话设计的注意事项如下。 银行 ATM 计算机报修专线等都是这种设计方式，如图13.2
操作、
所示。

 $\bigoplus$ 槽位填充：有限状态机的缺点是必须按顺序回答问题，并且每次只能回答一个信息，而且要等到系统念完问题才能回 $\overbrace{\Xi}^{\root[ 3 ] {\Xi}}$ ，对娴熟的使用者来说会很不耐烦，若能引进NLP技术，就可以让使用者用自然对谈的方式提供信息。例如「我要订3月21日双人房」，客户说一句话，系统就能够直接处理，若发现信息有欠缺， 系统再询问欠缺的信息即可，就像与真人客服对谈一样，不必像往常一样，「普通话请按 $1$ ，闽南话请按 $2$ ，客语请按 $3$ ，英语请按 4」，引起客户不满。

(2）整合社群媒体，如Line、Facebook Messager、Tite 等，用户不需额外安装软件，且不用教学，直接在对话群组加入官方贝 $\b< \frac{\Xi} {\Xi}$ ，即可开始与 ChatBot对话。

(3）人机整合：ChatBot 设计千万不能重复问相同的问题， $\grave{\varPsi}$ 须设定跳脱条件，一旦察觉对话不合理，就应停止或转由客服人员处理，避免引起使用者不快，造成反作用，使用有限状态机设计方式，常会发生这种错误，若状态已重复两次以上，就可能是Bug.

![](figures/849-4-FIGURE.jpg)

图13.2 ATM提款的有限状态机（FSM

 $\mathit{\Pi}$ 年前，微软聊天机器人Tay，推出后不到24小时，就因为学会骂人、讲脏话，导致微软紧急将她下架，就是一个血淋淋的案例。

除了技术层面的问题之外，ChatBot也称为 Conversational $\mathbf{A} \mathbf{I}$ ，因此对话的过程，需注意使用者的个人信息保护，包含对话文件的存取权、对话中敏感信息的保护，并且让使用者清楚知道 ChatBot的能力与应用范围。

(2）加载训练数据：数据来自于「anto buid yourfirs chatbot using NLTK &Keras」 (https://data-
flair.training/blogs/python-chatbot-project) 程序代码如下：

## 13-3 ChatBot实践

这一节我们先以自行制作ChatBot为出发点，看看几个范例。

范例.NLP加上相似度比较，制作简单的ChatBot

请参阅程序:13 01 simple chatbot.ipynb

1）加载相关库。程序代码如下

1#载人相关库
2 import spacy
3 import json
4 import random
5 import pandas as pd

![](figures/851-7-FIGURE.jpg)

(4）定义前置处理函数：去除停用词、词形还原。程序代码如下：

(5）测试：相似度比较，为防止选出的问题相似度过低，可订立相似度下限，低于下限即调用 say not understand()，回复 「我不懂你的意 $\P$ ，请再输 $\lambda-i \pi$ 」，高于下限，才回答问题。程序代码如 $\mathrm{F}$ :

执行结果：例句个数有 $4 7 \ \uparrow$ ，意图（Inten）个数有 $9 \uparrow$ 0

(3）载入词向量。程序代码如下

![](figures/852-4-FIGURE.jpg)

）执行结果：经过程序调校后，响应的结果比较满意。结果如下:

![](figures/853-1-FIGURE.jpg)

3将回答转成Pandas DataFrame，便于筛选与抽样 (Sample），针对相同问题，可做不同的回复。

利用前一章所学的知识，只要短短数十行的程序代码，就可以完成一个具体而微的 ChatBot，当然，它还可以再加强的地方有很多，举例如 $\mathbf{F}$ 。

(2）可视化接口：可以利用Stramlit、Flask、Django等库制作网页，提供使用者测试

(4）使用更完整的语料库，测试ChatBot效果：目前使用 SpaCy的分词速度有点慢，应该是词向量的转换和前置处理花 $7-$ 些时间，可以改用NLTK试试看

 $( 5 )$ 整合数据库：例如查询数据库，检查旅馆是否有空房、 保留订房等

![](figures/854-4-FIGURE.jpg)

 $( 1 )$ 中文语料库测试。

(3）整合社群软件：例如LINE，直接在手机上测试 (6）利用NER提取实体：有了 $\mathcal{A}$ 事时地物的信息，可进一步整合数据库。

网络上有许多的 ChatBot工具框架，技术架构也很多样化，笔者测试的一些框架如下。

thtps/gihub.comngunthercoxcChatterBotitee/3ecceddzal4eo caaeff12df7fa 68513a464a00) 采配接器模式（Adapter
Pattern) 是一个可扩充式的架构，文持多语系。

(Template）语法制定各式的样板，接着再制定变量嵌入样板中除了原本内建的样板外，用户也可以自定义样板和变量，来扩充 ChatBot的功能。

(3） Rasa (https:/rasa.com/）：以 Markdown 格式制定意图、故事、响 $\overrightarrow{\Psi}$ 、实体与对话管理等功能，用户可以编辑各个组态文件 $( \star. \mathrm{y m l} )$ 重新训练后，就可以提供给ChatBot 使用

## 13-4 ChatBot 工具框架

## (1)ChatterBot

## (2) ChatBotAl

$$
\mathrm{( h t t p s : / / g i t h u b. c o m / a h m a d f a i z a l b h / C h t )}
$$
atbot）：以样板

ChatterBot采配接器模式，内建多种配接器（Adapters），主要分为Logic adapters、Storage adapters 两类，也能自制配接器，是一个扩充式的架构，也支持多语系。它本身并没有NLP的功能，只是单纯的文字比对功能。

(2）训练：将后一句作为前一句的回答。例如，使用者输入 Hello 后，ChatBot则回答「Hithere!」 它会使用到NLTK的语料库。程序代码如下：

## 13-4-1 ChatterBot 实践

ChatterBot测试， 范例.

请参阅程序:13 02 ChatterBot test.ipynb

(1）加载相关库。程序代码如下

| 1 | 车载人相关库 |
| 2 | om chatterbot import ChatBot |
| 3 | om chatterbot.trainers import ListTrainer |

![](figures/857-7-FIGURE.jpg)

(3）简单测试。程序代码如下执行结果：由于「Good morning!」不在训练数据中，所以 ChatBot就从过往的对话中随机抽一批数据出来回答。

(4）测试另一句在训练数据中的句子：ChatBot 通常会回答后一句，偶尔会回答过往的对话。程序代码如下：

OMathematicalEvaluation：数学式运算，检视原始码后发现它是使用 mathparse 区数库

Dstorage adapter参数指定对话记录存储在 SQL数据库或 MongoDB.

![](figures/858-4-FIGURE.jpg)

(5）加 $\lambda$ 内建的配接器

$$
\mathrm{( h t t p s : / / g i t h u b. c o m / g u n t h e r c o x / r}
$$
nathparse) O

PTmeLogicAdapter：有关时间的函数

3BestMatoh：从设定的句子中找出最相似的句子

$$
\pi\equiv\Xi\pm[ \mp, \exists\pm] \mp\mp
$$

![](figures/858-10-FIGURE.jpg)

## ，测试时间的问题：问现在的时间。程序代码如下： (6)

## D问法可检视原始码ime adapter.py

![](figures/859-2-FIGURE.jpg)

②执行结果：回笞「The current time is 04: 37PM O

(7）数学式测试。程序代码如下：

1#数学式测试
2# $7+7$ 
3 response = bot.get response("What is 7 plus 7?")
4 print(f'回答：{response}")
5
6 $\# \textbf{8}-\textbf{7}$ 
7 response = bot.get response("What is 8 minus 7?")
8 print(f'回答：{response}"）
9
10#50 * 100
11 response = bot.get response("What is 50 * 100?")
12 print(f'回答：{response}')
13
14 #50 * (85 / 100)
15 response = bot.get response("What is 50 * (85 / 100)?") 16 print(f'回答：{response}"）

## 执行结果如下：

回答： $7$ plus $7 \ =\ 1 4$ 
回答： $8$ minus $7 \ =\ 1$ 
回答： $5 0 \, * \, 1 0 0 \,=\, 5 0 0 0$ 
回答： $5 0 ~ * ~ ( \; \; 8 5 \; \; / \; \; 1 0 0 \; \; ) ~=~ 4 2 \,. 5 0$ 

(8）加入自定义的配接器：自定义配接器为my adapter.py 类别名称为 MyLogicAdapter。程序代码如下：

D ：会回答「订位日期、时间及人数？」或「哪一
执行结果：
天？ $\mathit{\Pi}$ 点？人数呢？ 这是程序中随机指定的。

can process：设定何种问题由此配接器处理，笔者设定的条件为 statement.text.find（『订位』 >-0

以上范例是一个很简单的架构，虽然没有太多NLP的功能， 提问的句子还必须与训练数据完全相同，但是，它提供了一个可扩充式的架构，让读者可以利用各自的配接器开发技能，再结合相似度辨识，找出意图最相似的问题，最后做出对应的回答，程序就更实用 $\overline{{\jmath}}$ 。

![](figures/860-4-FIGURE.jpg)

(9）测试自定义配接器。程序代码如下

| 1 | # 测试自定义配接器 |
| 2 | response = bot.get response("我要订位"） |
| 3 | print(f·回答：{response}'） 一 |

②自定义配接器必须实现以下三个函数

init：初始化对象

 $\bullet$ process：处理口答的函数

ChatBotAI 同时提供AI的功能与对话管理的架构，可以透过 REST API与社群软件进行整合，如Facebook Messenger，并且以样板语言为主，提供学习（Learn）、记忆（Memory）设定、条件判断（Conditional $\mathrm{S w i t c h} )$ 、主题式对话（Topic-based
Conversation Handling）等功能，如图13.3所示。

## 13-4-2 Chatbot Al实践

![](figures/861-2-FIGURE.jpg)

图13.3 ChatBot Al的架构图

(图片来源：chatbotAI官网

范例.ChatBot AI测试。

请参阅程序:13 03 chatbotA test.ipynb

1）加载相关库

(2）功能展示。程序代码如下： 执行结果：>后面为使用者输入的问题，一般问题的回答还算得体，要结束的话需输入quit。执行结果如下

(3）使用维基百科作为语料库，需安装 wikioedia 库。程序如下:

这个库可搜寻维基百科的数据,输 $\lambda$ 关键词后，就可以进行多功能的查寻，相关使用说明可参考 wkipedia 库的官网
(https:/github.com/goldsmith/Wikipedia)

(4）注册可接收的关键词及负责响应的模块为维基百科。程序代码如下：

![](figures/862-4-FIGURE.jpg)

$$
\mathrm{p i p ~ i n s t a l l ~ w i k i p e d i a}
$$

S（Do you know about what is who is'tell me about 可接收的问句开头。

执行结果：询问一些比较专业的问题，都可以应答无碍。结果如下：

![](figures/863-2-FIGURE.jpg)

## (5）指定样板，开始对话。样本文件内容如下

| {% | {% block %} |  |
|  | {% client %}(Do you know about what | ) (?P<query>.*){% endclient |
|  | {% response %}{% call whoIs: %query |  |
| {% | {% endblock %} |  |

Toien：使用者

ChatBot的回应。

Gcalwhols: %cuery：指定注册的whols模块响应

$$
\star\Xi\equiv\pm\pm\exists\pm\pm\pm\Gamma:
$$

## 执行结果：中文关键词（who is 杨振宁?）能够正确回答。结果如下：

(7）记忆模块定 $\grave{\chi}$ ：下面使用变量来记忆一个字符串或累计值，如访客人数，并且使用 key/value进行存储。程序代码如下：

## (6）使用中文关键词发问。程序代码如下

1 first question="你好吗?
2 Chat("chatbot data/Example.template").converse(first question)

你好吗？
〉好
Please tell me more.
> whois杨振宁?
Yang Chen-Ning or Chen-Ning Yang (Chinese: 杨振宁； pinyin: Ying Zhénning; born 1 October 1922), also known as C. N. Yang or by t he English name Frank Yang, is a Chinese theoretical physicist who made significant contributions to statistical mechanics, int egrable systems, gauge theory, and both particle physics and condensed matter physics. He and Tsung-Dao Lee received the 1957 N obel Prize in Physics for their work on parity nonconservation of weak interaction. The two proposed that one of the basic quan tum-mechanics laws, the conservation of parity, is violated in the so-called weak nuclear reactions, those nuclear processes th at result in the emission of beta or alpha particles. Yang is also well known for his collaboration with Robert Mills in develo ping non-abelian gauge theory, widely known as the Yang-Mills theory.

![](figures/864-5-FIGURE.jpg)

(8）记忆设定测试。程序代码如下 ChatBotAI也提供 $7-$ 个扩充性的架构，可透过注册的模块和样板，以外挂的方式衔接各种技能。

![](figures/865-1-FIGURE.jpg)

Dchat.converse()：内含用法说明。

increment <name>：变量值加 $1_{\circ}$ 
show <name>：显示变量值。
remember <name> is <value>: 记忆变量与对应值。
tellme about <name>：显示变量对应值

increment <name> 变量值加1

show <name> 显示变量值。
冷量

remember <name> is <value>：记忆变量与对应值。

$$
\bullet\quad\mathrm{t e l l ~ m e ~ a b o u t ~ < n a m e} \! > : \ \Xi, \Xi
$$
六变量对应值。

②执行结果如下

![](figures/865-9-FIGURE.jpg)

Rasa 是一个Open Source 的工具软件，也有付费版本，相当多的文章提到了 $\overrightarrow{\mathrm{E}}$ 。它以Markdown格式设定意图、故事、响 $\overrightarrow{\Psi}$ 、 实体以及对话管理等功能，用户可以编辑各个组态文件 $( \star. \mathrm{y m l} )$ 重新训练过后，就可以提供给ChatBot 来使用。

安装过程依照Rasa官网指示操作，会出现错误，正确的安装指令如下。

**-uer参数：会让Rasa 被安装在用户目录 $\mathrm{F_{o}}$ 若不加此选项，则会出现权限不足的错误信息，表示Python site-packages目录不允许安装。

在Windows操作系统下，Rasa安装成功后，程序会放在 C:users
kuser name>tappdatarcaminglpythontoython38Ascripts.。接着测试步骤如下。

## 13-4-3 Rasa 实践

## .Windows操作系统

pip instal rasa-ignore-instaledruame.lyaml--use

## 2. Lnux/Mac操作系统

pio install
$$
\mathrm{r a s a-i g n o r e-i n s t a l l e d \ r u a m e l. y a m}
$$
1

## 1。新增一个项目

$$
\mathrm{C : \ u s e r s l}
$$
kuser name>\appdataroaming\oython\python38\scriptsrasa.exe init --no-prompt

(2）会依据以上项目文件，同时进行训练，完成后建立模型文件，存储在 models 子目录内

Bstories.yml：包含各项故事情节，描述多个意图和行动的顺序

$$
\mathrm{C : \ u s e r s l}
$$

（1）产生一个范例项目，子目录和文件列表如下：

$$
\operatorname{a c t i o n s}
$$
data
$$
\mathrm{m o d e l s}
$$
$$
\mathsf{t e s t s}
$$
$$
| \operatorname{c o n f i g. y m} |
$$
$$
\fbox{} \mathrm{c r e d e n t i a l s. y m}
$$
$$
| \dim\operatorname{a i n} \! \mathrm{. y m} |
$$
$$
\fbox{\ e n d p o i n t s. y m}
$$

(3） data子目录内有以下几个重要的文件。

Dnlu.yml：NLU 训练数据，包含各类的意图和例句

包含各项规则的意图和行动。

(4）根目录的文件如下

Oe 包含Bot各项的回应

2config.yml:NLU训练的管线（Pipeline）与策略 (Policy)

## 2.测试 <user name> appdatalroamingloython\python38\scriptslrasa.exe shell

对话过程如下，并没有太大的弹性，必须完全照着nlu.yml间问题

$$
\begin{array} {c} {\mathrm{C. \ u s e r s y}} \\ {\mathrm{< u s e r \_n a m e > \ a p p d a t a V o u m i}} \\ {\mathrm{v i s u a l i z e}} \\ \end{array}
$$
ing\python\python38\scripts\rasa.exe

$$
\mathrm{C : \ u s e r s l}
$$

![](figures/868-4-FIGURE.jpg)

## 3。故事情节可视化

$$
\mathrm{C : \ u s e r s l}
$$

执行结果：如图13.4月所示，对立stories.ynl的内容 <user name>tappdata roaming\oython\oyhon38sriptsrasa.exe train

范例。建立自定义的行动（Gustom action 下面新增一个行动，询问姓名，并单纯回答「Hello <name>.

![](figures/869-2-FIGURE.jpg)

 $1 3. 4$ 故事情节可视化图

## 4。训练模型

可以修改上述的.yml文件后，重新训练模型

$$
\mathrm{C : \ u s e r s \ l}
$$

 $( 1 )$ 安装Rasa SDK: pip install rasa core sdk

 $( 1 )$ 安装 Rasa SDK:

$$
\mathrm{p i p ~ i n s t a l l ~ r a s a \_~ c o r e \_~ s d k}
$$

(4)在data stories.yml的每一段action: utter ask name 后面增加下列内容：

(2）在domain.ym1 文件内增加以下内容，请参阅范例文件。

$$
\big( \big) \mathrm{i n f o r m} \not\Xi\not\Xi_{\circ}
$$

2entities、actions

$$
\begin{array} {c} {\mathrm{e n t \, i t \, i e s :}} \\ {-\mathrm{~ n a m e}} \\ \end{array}
$$

$$
\begin{array} {c} {\mathrm{a c t i o n s :}} \\ {-\mathrm{a c t i o n \_s a v e \_n a m e}} \\ \end{array}
$$

s增加以下内容：

$$
\begin{array} {c} {{\mathrm{u t t e} \, r_{-} w e l c o m e :}} \\ {{-\ t e x t : \mathrm{~ ` `} W e l c o m e :}} \\ {{\mathrm{u t t e} \, r_{-} a s k_{-} n a m e :}} \\ {{-\ t e x t : \mathrm{~ ` `} w h a t^{\prime} s \mathrm{~ ~ y o u r ~ ~ n a m e} \, r^{\prime\prime}}} \end{array}
$$

(3）在atanuyml增加以下内容：

intent: inform
examples:
my name is [Michael] (name)
[Philip] (name)
[Michelle] (name)
[Mike] (name)
I'm [Helen] (name)

intent: inform
entities:
name: name
action: action save name

(5）在endointsyml 解除批注

$$
\begin{array} {c} {\mathrm{a c t \, i o n}_{-} \mathrm{e n d p o i n t} \colon} \\ {\mathrm{u r l} \colon\mathrm{h t \, t p} \colon\ / / \mathrm{l o c a l h o s t} \colon5 0 5 5 / \mathrm{w e b h o o k}} \\ \end{array}
$$

(7）启动action 程序：CAusers\
<user name>lappdatalroaming\oython\python38\scriptsirasa.exe run actions.

(8）重新训练模型：C:users
<user name>lappdataroaminglpythontoython38\scriotsirasa.exe train.

(9）测试：C:users
<user name>lappdata roaminglpythonloython38\scriptsirasa.exe shell

Rasa 比较像传统的 AIML ChatBot，是以问答例句当作训练数据。算是相对僵硬的方式，且需要大量的人力维护，但好处是可以精准控制回答的内容。

(6）增加一段action处理程序。程序代码如下

![](figures/871-5-FIGURE.jpg)

## (7）启动action程序：CAusers\

对话过程如下，确实有回应「Hello <nane> O

| Your input | ut -> hello |
| What's you | our name? |
| Your input Hello.r mic | ut 8 -> michae ichael!  |
| Hey! How a | are you? |

现在已经有许多厂商都推出了成熟的 ChatBot产品，只要经过适当的设定，就可以上线 $\overline{{\j}}$ ，如 Google Dialogflow、Microsot QnA Maker、Azure Bot Service、IBM Watson Assistant等。以下我们以DialogFlow为例介绍整个流程。

依据Dialogflow的官网说明料，它是一个NLU平台，可将对话功能整合至网页、手机、语音响应接口，而输入/输出接口可以是文字或语音。就笔者实验结果而言，它主要是以槽位填充为出发点， 并搭配完整的 NER功能，如时间，可输入 today、tomorrow、
rightnow，系统会自动转换为日期，另外全世界的城市也能辨识， 算是一个可轻易上手的产品。

它有两个版本：Dialogfiow CX和Dialogflow ES，前者为进阶版本，后者为标准版，可免费试用，我们用免费版本测试。两者功能的比较表可参阅：

(1） Agent：即 ChatBot，每个 $\angle\ >$ 司可建立多个ChatBot，各司其职。

训练的词组（Training Phrases 定义使用者表达意图的词组，不必列举所有可能的词组，Dialogflow 有内建的机器学习

## 13-5 Dialogflow 实践

htps:/iclouclgogl.comcdialogflowdoseitions.

Dialogilow的术语定义如下

(2）意图：与之前定义相同，但更细致，包含如下。

 $\bigoplus$ 参数：定义槽位填充所需的信息，包括必填的和选填的参数，Dialogflow 可以从使用者的表达中找出对应的实体，如图13.5 所示。

(3）实体：Dialogfilow 已内建许多系统实体（System
Enity）类别，包括日期、时间、颜色、Emal等，还包括多国语系的实体。

(4）上下文：如图13.6所示，Dialogflow从第一句话中察觉意图是「查询帐户信息」（Checkinglnfo），接着会问「何种 $1 \vec{\Xi}$ 息」，用户回答「账户余额」后，Dialogflow即将余额告诉使用者。Dialogfow 会先辨识意图，再根据缺乏的信息进一步询问，直到所有信息都满足为上，才会将答案回复给使用者，这就是槽位填 $\overrightarrow{\pi}$ (Sot illin）的机制

智能，会自动加入类似的词组

 $\bigoplus$ 行动：ChatBot接收到意图后采取的行动

 $\clubsuit$ 回 $\overrightarrow{\Psi}$ ：行动完毕后，响应用户的文字或语音

![](figures/873-6-FIGURE.jpg)

图13.5 Dialogilow可从意图中找出时间和地点

(图片来源：Dialogilow的官网说明）

(5）追问意图（Folow-up intent）：可依据使用者的回答定义不同的回答方式，以追问意图，通过比功能可以建立有限状态机，对于复杂的流程有很大的帮助，Dialogflow 也内建了追问意图的识别，如Yes/No。例如 $\mathrm{y e s}$ ，回笞sure、exact $\mathbf{y}$ 也可以，如图 13 $7$ 所示。

(6）履行（Fufillment) ChatBot除了响应文字之外，也能够与数据库或社群软件整合，开发者可以撰写一个服务，整合各种软硬件，如图13.8所示。

![](figures/874-2-FIGURE.jpg)

图13.6 Dialogilow上下文对话的流程

(图片来源：Dialogilow 的官网说明）

![](figures/874-5-FIGURE.jpg)

图13.7追问意图

![](figures/875-0-FIGURE.jpg)

图13.8履行

(图片来源：Dialogfiow的官网说明印）

(2）新增GCP的项目：参阅「Dialogfilow Quickstart Setup」，选择「新建项目」选项，并单击「建立」按钮，如图 13.9所示。

(4）建立服务账户：单击「建立服务账号」按钮，如图13.10 所示。

## 13-5-1 Dialogflow 安装

Dialogflow 不需安装，只需开通，程序如下。

(1）开通：首先要申请Gmail账 $\mp$ ，接着再申请GCP (Google Cloud Platiform）云端服务。

![](figures/876-5-FIGURE.jpg)

图13.9新增GCP项目

）授权项目使用Dialogilow APl：单击「启用AP」 按钮 (3)

(s）新增密钥 $( \mathrm{k e y} )$ ：选择「电子邮件」选项，如图13.11 所示。

![](figures/877-1-FIGURE.jpg)

图 $1 3. 1 0$ 建立服务账号

![](figures/877-3-FIGURE.jpg)

![](figures/877-4-FIGURE.jpg)

图13.1新增密钥

$$
\pi: \quad( 6 ) \pm\pm\Pi\pm\pi\equiv\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm\pm
$$
可密钥路径。在Powershel 内输入如

$env: GOOGLE APPLICATION CREDENTIALS- 「<密钥路径>」

(9）安装Dilolio intiarary pi insal goole-cloud-dialogilow.

例如： $\Gamma\mathrm{F} \colon\eta\omicron$ ABooksl以100张图理解深度学习 \quickstart.json

 $( 7 )$ 安装Cloud SDK

(8）测试SDK：会打印出密钥（是一堆乱码) O

gcloud athppicaion-deiaul printacess-oken

(1）建立 Agent：输入相关信息，支持多语系，包括中文单击「CREATE」按钮即可，如图13.12所示。

 $\bullet$ Default Fallback ntent：不理解使用者的意图时，会归属于此，通常会要求使用者再输入其他用语。

(3）建立意图：单击「Create Intent」按钮，输入意图名称并单击「Add Training Phrases 超链接,就可以输 $\lambda$ 多组问句与响应，如图13.13所示。

## 13-5-2 Dialogilow 基本功能

至此开通完成，即可进入Dialoglow设定相关画面

![](figures/879-5-FIGURE.jpg)

 $1 3. 1 2$ 建 $\overrightarrow{\Psi}$  $\mathsf{A g e n t}$ 冬

(2）内建意图：Dialogilow会预先建立两个意图

 $\bullet$ Detault Welcome ntent: Agent的欢迎词回应：单击「ADDRESPONSE」超链接，如图13.14所示

(4）测试：存盘，并确定训练完成的信息出现之后，即可在画百右侧测试，亦可直接用语音输 $\lambda$ .

①输入「What is your name ，如图13.15月所示

![](figures/880-3-FIGURE.jpg)

图 $1 3. 1 3$ 建 $\overrightarrow{\Psi}$ 意图

![](figures/880-5-FIGURE.jpg)

$$
\boxed{\b^{g}} ~ 1 3. 1 4 ~ ~ ~ \boxed{\b y}
$$

存档：单击 $\Gamma\mathrm{S A V E}$ 按钮

(5）参数：输入的例句如果包含内建的实体，则会被解析出来，当作参数，可进一步设定参数属性。单击画面左侧Intent旁的 +J

D例如输入「Iknow English Dialogflow 检测到
「English」是内建的语言Enily (@sys.language) 系统会自动新增一个参数，如图13.17所示。

![](figures/881-2-FIGURE.jpg)

图13.15 测试

2输入Hello ，如图13.16所示

![](figures/881-5-FIGURE.jpg)

$$
\stackrel{\hline} {\S} 1 3. 1 6 ~ ~ \mathrm{i n l l i t} ~ 2
$$

 $\bullet$ Prompts：若输入的问句或回答欠缺此参数，Agent会显示此提 $\overline{{\Pi}}$ ，询问用户，如图13.18 所示。

![](figures/882-1-FIGURE.jpg)

图13.17参数

②可针对参数设定属性

 $\bullet$ Requred：是否必要输入

：参数名称。 Parameter Name

Entty：选Entr $\mathbf{y}$ 类别，可修改为其他类别。

Value：参数的名称，响应可以此名称取得参数值

ls List：参数值是否为List，即一参数含多个值

![](figures/882-9-FIGURE.jpg)

图 $1 3. 1 8$ 询问用户

3输入响应：「Wow! ldidn't know you knew $language. 其中 $language 会白用户的问包取得变量值。

）测试：输入「Ispeak english 响应的 $language-
(6)
English，如图13.19所示。

$$
\textcircled{4} \not{\Xi} \sharp\sharp\mathbb{I}_{\circ}
$$

![](figures/883-3-FIGURE.jpg)

$$
\boxed{\S} 1 3. 1 9 \quad\mathrm{i d l i t}
$$

(7）可建立自定义的实体：单击画面左侧Enities 旁的 +

D输 $\lambda$ Entty和同义字

2存档 13.20所示
如图

![](figures/883-8-FIGURE.jpg)

13.20存档冬

3双击「JavaScript」按钮，选择选项。

1参数language 若选择「Required」，会出现「whatis the language?J 如图13.21所示

(8）使用 Engish 的实体：在原来的「set-language」 Entiy，输入例句（Training phrase)

$$
\emptyset\mathrm{I} \mathrm{~ k n o w ~ J a v a S c r i p t.}
$$

$$
\textcircled{2} \mathrm{l} \mathrm{~ w r i t e ~ t h e ~ l o g i c ~ i n ~ P y t h o n.}
$$

④输入响 $\overrightarrow{\omega}$ ：Slagugrpogaming isancllent programming language.

$$
\textcircled{5} \subsetneq\sharp\sharp\flat_{\circ}
$$

(9）测试：输入「you know js? O

![](figures/884-8-FIGURE.jpg)

 $1 3. 2 1$ 选择参数图

2全部参数若都不选择「Required 则会出现「JavaScript is an excellent programming language.」 如图123.22所示

(10）追问意图
$$
( \mathrm{F o l l o w-u p ~ l n t e n t} )
$$
：若需考虑上文回答，可
追问详细意图

![](figures/885-2-FIGURE.jpg)

 $1 3. 2 1$ 选择参数续图

![](figures/885-4-FIGURE.jpg)

图13.22不勾选所有参数

D测试：修改回应为「Wow! l didn't know you knew Slanguage. How long have you known Slanguage?

②加追问意图：单击画面左侧「intents」选项，将鼠标移至 a 会出现「Addlflw-up intent 单击即可， set-language」
会增加追问意图,如图 13.23所示。
set-language - customy

の寿命（Lifespan）：一般意图的预设寿命为 $5$ 个对话，追问意图的预设寿命则为 $2$ 个对话，超过20分钟，所有意图均不保留，即相关的对话状态会被重置。

![](figures/886-2-FIGURE.jpg)

图 $1 3. 2 3$ 加追问意图

③单击「 输入例句（Training set-language - custom」
phrase）如下：

$$
\begin{matrix} \bullet& {3 \operatorname{y e a r s}} \\ {\bullet} & {\operatorname{a b o u t} 4 \operatorname{d a y s}} \\ {\bullet} & {\operatorname{t o r} 5 \operatorname{y e a r s}} \\ \end{matrix}
$$

$$
\bullet\quad3 \, \mathrm{y e a r s}
$$

$$
\bullet\quad\mathrm{a b o u t \, 4 \, d a y s}
$$

$$
\bullet\quad\mathrm{f o r ~ 5 ~ y e a r s}
$$

⑤测试：加入响应 $\Gamma_{\parallel}$ you've known #set-
can't believe
language-iolowup.language for Sduration!

$$
\bullet\Gamma_{\Gamma}^{\Gamma} \#_{\perp} : \frac{\pi_{\pm}} {\Xi_{\pm}} \boxed{\Xi_{\pm}} \Gamma_{\pm} \Gamma_{\pm} \Longrightarrow\Xi_{\pm} \pm\zeta_{\pm}
$$

r 意图。 #|

「』：参数值。 D输入「Iknow French ，如图13.24月所示

$$
( 1 1 ) \setminus\i[ \mathrm{i d} \pm\mathrm{i d} \mathrm{i d} ] \pm\mathrm{i d} \mathrm{i d} \mathrm{i d} \mathrm{i d} \mathrm{i d} \mathrm{i d} \mathrm{i d} \mathrm{i d} \mathrm{i d} \mathrm{o}
$$

![](figures/887-2-FIGURE.jpg)

图 $1 3. 2 4$ 输 $\lambda$ 1响应

2输 $\lambda$ 「for 5 years ，如图13.25月所示

![](figures/887-5-FIGURE.jpg)

 $1 3. 2 5$ 输 $\lambda$ 2
图 2响应

履行是在搜集完整信息后采取的行动，撰写程序完成商业逻辑和交易，可与社群软件、硬件整合。Dialog提供以下两种履行类型。

(1）Webhook：撰写一个网页服务（Web Service) Dialog 通过POST请求送给 Webhook，并接收响应。

(2）Inline Editor：通过GCP 建立 Cloud Functions，使用 Node.js执行环境，这是比较简单的方式，不过如果是正式的项目开发，还是要选择 Webhook。

范例. nline Editor须建立 GCP付费账 $\mathbf{\Xi}$ ，才能使用，下面针对Webhook 实践。

(1）建 $\vec{\Sigma}-$ 个新的意图：输 $\lambda$ 两个例句「order a room in Tainan at 2021-2-5」 want a double room in Taipei at 2021-01-01」，如图13.26所示。

## 13-5-3 履行

![](figures/888-6-FIGURE.jpg)

 $1 3. 2 6$ 建立新的意图图

(2）参数 $\Gamma_{\mathrm{g e o-c i t y} \rfloor}$ 「date-time」均设为 $\grave{\varPsi}$ 要字段，如图 13.27所示。

(4）撰写 Webhook程序：可使用多种语言撰写，这里我们使用Python加上Flask库，撰写 Web 程序，完整程序请参考
dialogflow\webhookiapp.py，程序代码后续说明

(5）程序必须部署到Internet $\b=$ ，Dialogflow 才能存取到 app.py。可以使用 ngrok.exe将内部网址对应到外部网址，这样就可以先在本机测试，等到测试成功后，再将程序部署到Heroku 或其他网站测试，Heroku 是免费的网站部署平台，试用后可升级为付费账户。

(8）接着设定Fulfillment：单击画百左侧的Fufillment旁的 +」按钮，启用 Webhook，并设定上一步骤所取得的htips网

![](figures/889-4-FIGURE.jpg)

 $1 3. 2 7$ 设置参数图

(3）履行：启月用N「anal wobokalfor this intent O

(6）启动app.y：预设网址为htto $\sharp/ 1 2 7. 0. 0. 1$ :500

(7）执行下列指令，取得对应的htps网址。 址，后面须加上/webhook，单击下方「Save」按钮即可，如图 13.28所示。

(9）测试：在画百右侧输 $\lambda$ 「order a room in Tainan at 2021-2-5」 如图13.29所示。

(10）再查看 dialogilowwebhooktest.db SQLite数据库，就可以看到每重复执 $\acute{\Pi}-\lambda\bar{\lambda}$ ,tainan/2021-02-05 的订房数
(roon count）就会加 $1$ 。而输 $\lambda$ 不同的城市或口期则会新增一批

![](figures/890-3-FIGURE.jpg)

图13：28设定Fulfillmnt

![](figures/890-5-FIGURE.jpg)

图13.29测试

记录。SQLite 数据库可使用 SQLitespy.exe 或其他工具软件开启如图13.30所示。

(4）定义函数：必须为@app.route（'webhook, methods-['POST]) 可取得请求、意图、ntit $\mathbf{y}$ 会程序代码如 $\mathrm{F}$ :
等。

| File Name | Edit | View | Execute Opti | ons Hel | lp |  |
|  |  | Type | 国雷 g | grocery |  |
| 日门 白 中 | main - 面 1 由乐 Colla | 1 Tables (1 hotelk tions(7 | F:\OAI | delete | e from hotels whe | re city = 'tainan' |
|  |  |  |  | id | name | created at |
|  |  |  |  | 1 | mik | 2019-10-2406:00:26.285108 |
|  |  |  |  | 2 | cereal-chocolate fiavor | 2019-10-24 10:27:37.293991 |
|  |  |  |  | 3 | bread | 2019-10-24 10:43:11.071314 |

 $1 3. 3 0$ 查看数据库图

dalogilowehoocap.y)程序代码说明如下

## (1）安装库。

$$
\mathrm{p i p ~ i n s t a l l ~ f l a s k}
$$

$$
\mathrm{p i p ~ i n s t a l l ~ s q a l c h e m y}
$$

(2）加载相关库。程序代码如下

4#载入相关库
5 from flask import Flask, request, jsonify, make response 6 from sqlalchemy import create engine

(3）声明Flask对象。程序代码如下

8#声明Flask 对象
9 $\mathrm{a p p} \,=\, \mathrm{F I} \mathrm{a s k} \, \mathrm{I}$ name

(6）更新数据库：先根据城市、日期查询数据，若记录存在，则订房数加1；反之，则新增一批新的记录，最后传回OK信息。程序代码如下：

(7）以上只是示范程序，在实际情况中，我们必须做例外处理，包括程序代码错误、意图、Enity 检查等

Dialogiow 还可以整合语音交换机、社群媒体、Spark等，详情可参阅「Dialogflow ntegrations 说明」 另外，Dialogflow也大建了许多应用程序，可参阅「Dialogflow Prebuilt Agents 说明

## (5）开启数据库联机。程序代码如下

| 34 | engine = create engine(" sqlite:///test.db convert unicode=True) |
| 35 | con = engine.connect() |

| 37 | if intent == 'booking': #根据城市、日期查询 sql cmd = f"select room count from hotels " sql cmd t= f"where city - '{entityCity}' and order date = "{entityDate}'" result = con.execute(sql cmd) list1 = result.fetchall() A海0汽二  |
| 38 |
| 39 |
| 40 |
| 41 |
| 42 |
| 43 汽 |
| 44 R一 | #增加汇求 限百汽门店 |
| 45 A一 | it len(list1)>0:#副房数加1 0115275240 |
| 47 | 广一LL salcmd t= f"where citv = 'fentitvCityi' and order date = "fentitvDatel' |
| 48 | 一一 result = con.execute(sal cmd)  |
| 49 |  联一 else:#新增一批记灵  |
| 50 | sql cmd = "insert into hotels('city', 'order date', 'room count')" |
| 51 | sql cmd += f" values ('fentityCity}', '{entityDate}', 1)" |
| 52 | 一 result = con.execute(sql cmd)  |
| 53 |  |
| 54 | #回应 |
| 55 | response = f'{entityCity}, {entityDate} oK." |
| 56 | return make response(jsonify({ 'fulfillmentText': response })) |

上述Dialogilow介绍的概念，也几乎是业界的标准对于大部分的企业来说，聊天机器人的实用度相当高，可以提供售前支持、销售甚至是售后服务等多方面的服务，但如何整合既有的流程及系统，使得ChatBot能无缝接轨，员工也能快速上手， 是系统设计的一大课题。最后，设置时也不要忘记系统自我学习的使命，让系统随着服务的经验，越来越「聪明」。

## 13-6总结

近几年随着社会发展，影像、语音等的自然用户接口（Natural User Interface, NUI）有了突破性的发展，如 $\mathbf{A}$ pple Face D 以脸部辨识登录，手机、智能音箱可以使用语音输 $\lambda$ ，这类操作方式大幅降低输入的难度，尤其针对中老年人。根据统计，人们讲话的速度约为每分钟 $1 5 0 {\sim} 2 0 0$ 字，而打字输 $\lambda$ 大概只有每 $6 0$ 字分，如果能提高语音识别的能 $\mathcal{H}$ ，语音输入就会逐渐取代键盘打字 $\overline{{\jmath}}$ ，此外， 键盘在携带方便性与亲和力方面也远不及语音。由此可见，要消弭 $\mathcal{A}$ 类与机器之间的隔阂，语音识别是关键技术，接下来我们就来好好认识它的发展。

(1）说话者的个体差异：包括口音、音调的高低起伏，如男性和女性的音频差异就很大

(2) 、各种环境会有不同的背景音源，因此辨识前
环境噪声：
必须先去除噪声

(3）语调的差异：人在不同的情绪 $\mathrm{T F}$ ，讲话的语调会有所不同，比如悲伤时讲话速度可能较慢，声音较小而低沉： $\nabla\overrightarrow{\chi}$ ，兴奋时，讲话速度快，声音较大。

光是「No」一个简单的词，不同的人说就有各式各样的声波， 如图14.1所示。

## 第14章语音识别

## 第14章

## 语音识别

回归现实，语音识别并不简单，必须要克服以下挑战。

因此，要能辨识不同人的声音，计算机必须先对收到的信号做前置处理，之后，才能再运用各种算法和数据库进行辨识，而这过程中所需的基础知识包括如下。

看到这里大家应该有点犯难了，所以笔者试着以简驭繁，将焦点放在实践上。

![](figures/896-2-FIGURE.jpg)

冬14.1 -千种No的声波的部分提取

(图片来源：哥伦比亚大学语音识别课程讲文中）

(1）信号处理（Signal Processing)
(2）概率与统计（Probability and Statistics）
(3）语音和语意学（Phonetics:linguistics)
(4）自然语言处理（Natural language Processing, NLP) (5）机器学习与深度学习

1）信号处理 O

(2）概率与统计（Pbliy an Saistios O

(3）语音和语
$$
\imath\equiv\d\# \, \mathrm{~ ( P h o n e t i c s ; ~ \ l i n g u i s t i c s ) ~}
$$

(4）自然语言 $\boldsymbol{\imath}$ 小理（Natural language Processing, NLP) O

(5）机器学习与深度学习

以说话为例，人类以胸腔压缩和变换嘴唇、舌头形状等方式， 使空气产生压缩与伸张的效果，形成声波，然后以每秒大约340米的速度在空气中传播，当此声波传递到另一个人的耳朵时，耳膜就会感受到一伸一压的压 $\Box1 \Xi\Xi$ ，接着内耳神经再将此信号传递到大脑，并白大脑解析与判读，分辨此信号的意 $\mathcal{X}$ ，详细说明可参阅 Audio Signal Process $i n g$ and Recogntior'.

声音的信号通常是不规则的（见图14.2），所以必须先经过数字 $\Psi$ ，才能交由计算机处理，做法是每隔一段时间衡量振幅，得到一个数字，这个过程称为取样，如图14.3所示。之后，再把所有数字记录下来变成数字音频，而这个过程就是所谓的将模拟
(Analog）信号转为数字（Digital）信号。

## 14-1 语音基本认识

![](figures/897-3-FIGURE.jpg)

图14.2声音信号

![](figures/897-5-FIGURE.jpg)

 $1 4. 3$ 声音取样冬

信号可由波形的振幅（Amplitude) 频率（Frequency）及相位（Phase）来表 $\overline{{\Pi}}$ ，如图14.4所示。

(3）相位：描述信号波形变化的度量，通常以度（角度）作为单 $\overrightarrow{\nu}$ ，也称为相角或相。当信号波形以周期的方式变化，波形很环一周即为360度。

请参阅程序：14.01_振幅、频率及相位 $\mathbf{i p y n b}$ ，做一简单的测试。

频率是以赫兹（Hz）为单 $\imath\vec{\Omega}$ ，赫兹为信号每秒振动的周期数通常人耳可以听到的频率约在20 Hz~20 kHz，但随着年龄的增长，
，如冬14.5所示。
人们会对高频信号越来越不敏感，

(图片来源：台湾大学普通物理实验室母）

）振幅：指波的高度，可以形容声音的大小。 (1)

(2）频率：为一秒波动的周期数，可以形容声音的高低。

![](figures/898-7-FIGURE.jpg)

图14.4 振幅与波长

根据奈奎斯特 $\mathrm{( N y q u i s t )}$ 定理，重建信号是取样频率
(Sample Rate）的一半，以传统电话为例，通常接收的音频约为 4kHz，因比取样频率通常是8kHz，其他常见装置的取样步频率如下。

(4）其他装置的取样频率可参阅「Sampling（signal
processing）维基百科」（htps:/len.wikipecia.org/wiki/Sampling (signal processing) #Sampling rate)

将信号转为数字时，若数字的精度不足会造成更多的损失，因此，也可分为 $8$ 位、16 $\imath\vec{\Omega}$ 、32位不同的整数精度，这个过程称为量 $\Psi$ （Quantization 传统电话采用8 $\overrightarrow{\omicron}$ ，网络电话采用16 $\overrightarrow{\omicron}$ 。

信号经过数字化后，通常会把它存盘或通过网络传输给另一端，接着再把数字信号转回模拟 $\imath\rightleftharpoons\Xi$ ，即可原音重现，如图14.6所

![](figures/899-4-FIGURE.jpg)

14.5、动物可听见的音频范围

(图片来源：Audio Signal Processing 

1）网络电话：163H2

(2）CD：单声道为22.05K1z，立体（双）声道为44.1k12

(3） DVD：单声道为48Hz，蓝光DVD为96KHz $\overline{{\Pi}}$ 。这时就涉及到信号压缩的问题，如何以最小的数据量存储或传输，就是所谓的编码机制

(1）脉冲编码调变（Puse-code Modulation,PCM 直接将每一个取样的振幅存档或传输至对方，这种编码方式效率不高。

(2）非线性PCM（Non-linearPCM 因人类对高频信号较不敏感，故可以把高频信号以较低精度编码； $\nabla\overrightarrow{\vphantom{\vphantom{p}}}$ ，低频信号采较高精度，可降低编码量。

(3）可调变PCM（Adaptive PCM 由于信号片段高低不因此不必统一编码，可以将信号切成很多段，并把每一段都分 、进行正规化（Regularization）后，再做PCM编码。
别编码，

最常见的语音文件应该是 wav ，它支持各式的精度与编
文件
码，最常见的是16位精度与PCM编码

以上的过程可通过示波器（Oscillocope）观察，如图 14.7所 $\overline{{\Pi}}$ ，也可以直接以程序实践。

![](figures/900-6-FIGURE.jpg)

图14.6、信号的数字化与重现

(图片来源：
$$
\mathrm{T F i l e : ~ C P T-S o u n d-A D C-D A C. s v g \perp~^{[ 5 ]} ~ )}
$$
常见的编码方式如 $\mathrm{T}$ 。

常见的编码方式如下。

(1）加载相关库：Jupyter Notebook 本身就支持影像显示、 语音播放。程序代码如 $\mathrm{F}$ :

hitos:/github.com/maxifjavedsample-iles, autoplay 设定为True 时，执行即会自动播放，不需另外按PLAY键。程序代码如下

![](figures/901-2-FIGURE.jpg)

图14.7 示波器

(图片来源：台湾大学普通物理实验室印）

范例 ${\bf1}$ .音频文件解析

请参阅程序：14-02_音频文件解行.ipynb.

1#载人相关库 2 import IPython

(2）播放音频文件：文件来源为

![](figures/901-8-FIGURE.jpg)

执行结果：可中 $\ab k$ 显示文件长度有33秒

![](figures/901-10-FIGURE.jpg)

(3）取得音频文件的属性：可使用Python 内建的模块 Wa $\mathrm{v e}$ ，取得音频文件的属性，相关说明可参阅 wave 说明文件 (https:/dos.python.org3/ibrary/wave.html）。程序代码如下：

①执行结果：取样频率 $= \! 8 0 0 0$ ，帧数 $= \! 2 6 8 2 3 7$ ，声道 $= 2$ ，精度 $= 2$ ，文件秒数-33.53。

(4）使用 $\mathrm{P y A u d i o}$ 函数库串流播放：每读一个区块，就立即播放。PyAudio 在 Windows操作系统下不能使用pipinstal
PyAudio 顺利安装，请直接至Unofficial Windows Binaries for Python Extension Packages

(htps:/ww.id.uciedu-gohke/pythonibs/#pyaudio）下载 PyAudio-0.2.11-cp38-cp3[X]-win amd64.whl，再执行 pip instal PyAudio-0.2.11-cp38-cp3[X]-win and64.whl。程序代码如 $\mathrm{F}$ :

| #取得音频文件的属件 |
| import wave |
|  |
| f=wave.open(wav file) |
| print(f'取样频率={f.getframerate()}，帧数={f.getnframes()}, |
| — f'声道-{f.getnchannels()}，精度={f.getsampwidth()}, |
| f'文件秒数={f.getnframes() / f.getframerate():.2f}') |
| — f.close()  |

)：取样频率

音频文件总帧数。

$$
\bigoplus_{\mathrm{4 g e t n c h a n n e l s ( ) \! :}} \sharp\Xi_{\circ}
$$

$$
\protect\mathrm{( 5 g e t s a m p w i d t h ( ) \colon\) \equiv\langle\hbar\hbar\pm\hbar\hbar\) \Xi_{\circ}}
$$

⑥文件秒数 $=$ 音频文件总帧数/取样频率

(6）绘制波形：由于多声道wav文件格式是交错存储的，故先说明比较单纯的单声道 wav文件读取。程序代码如下：

![](figures/903-1-FIGURE.jpg)

(5）调用函数播放。程序代码如下

| 1 | 靥放音频文件 |
| 2 | layAudio(wav f file, -1) |

执行结果：每秒区块数 $= 7. 8 1 2 5$ ，总区块数
$$
= 1 3 0. 9 7 5 0 9 7 6 5 6 2 5_{\circ}
$$

| 1 | #绘制波形 |
| 2 | import numpy as np |
| 3 | import wave  |
| 4 | import sys |
| 5 | import matplotlib.pyplot as plt |
| 6 |  |
| 7 | #单声道绘制波形 |
| 8 | def DrawWavFile mono(filename): |
| 9 | #开启音频文件 |
| 10 | f = wave.open(filename, "r") |
| 11 |  |
| 12 | #字串转换整数 |
| 13 | signal = f.readframes(-1) |
| 14 | signal = np.frombuffer(signal, np.int16) |
| 15 | fs = f.getframerate() |
| 16 | 口 |
| 17 | #非单声道无法解板 |
| 18 | if f.getnchannelsOs1: |
| 19 | 口 Time =np.linspace(o, len(signal) / fs. num=len(signal)) |
| 厂厂口9 20  |
| 21 | #绘图 |
| 22 | plt.figure(figsize=(12,6)) |
| 23 | plt.title("Signal Wave...") |
| 24 | plt.plot(Time, signal) |
| 25 | plt.showO |
| 26 | else: |
| 27 | print('非单声道无法解析'） |

## (7 。程序代码如下测试。

1 wav file = './audio/down.wav $2 ~$ DrawWavFile mono(wav file)

执行结果：如图14.8所示

![](figures/904-4-FIGURE.jpg)

图14.8、测试结果

 $( 8 )$ 多声道绘制波形函数。程序代码如下 (10）将前面的单声道、多声道函数整合在一起。程序代码如下：

![](figures/905-1-FIGURE.jpg)

## (9) 。程序代码如下测试。

 $1_{2}$ wav file = './audio/WAV 1MG.wav DrawWavFile_stereo(wav file)

14.9所示执行结果：双声道如图

![](figures/905-5-FIGURE.jpg)

图14.9 测试多声道绘制波形

| 1 | t多声道绘制波形 |
| --- | --- |
| 2 | lef DrawWavFile(wav file): |
| 3 | f=wave.open(wav file) |
| 4 | channels = f.getnchannels()#声道 |
| 5 | f.close() |
| 6 |
| 7 | if channels = 1: |
| 8 | DrawwavFile mono(wav file) |
| 9 | else: |
| 10 | DrawwavFile stereo(wav file) |

## 11) 。程序代码如下测试。

1 $\mathrm{w a v_{\tt-f i l l e ~=~'~. / a u d i o / d o w n. w a v \,'' ~.}}$ 2 $\mathsf{D r a w w a v F i l e ( w a v \_f i l e ),}$ 
3 $\mathrm{w a v \quad\textsf{f i l} e=\Omega` `. / a u d i l} \, \le\% \mathsf{A V},$ 1MG.wav 4 DrawWavFile(wav file)

执行结果： 14.10所示
如图

![](figures/906-4-FIGURE.jpg)

14.10 整合单声道、多声道函数测试结果冬

![](figures/906-6-FIGURE.jpg)

执行结果：取样频率=44100，帧数 $= 9 9 9 9 9$ ，声道 $\natural1$ ，精度 $= 2$ ，文件秒数 $= 2. 2 7$ ，与设定一致。

## 续图14.10，整合单声道、多声道函数测试结果

(12）产生音频文件：以随机数生成音频文件，随机数介于 $(-3 2 7 6 7, 3 2 7 6 7 )$ 。程序代码如 $\mathrm{F}$ :

![](figures/907-3-FIGURE.jpg)

执行结果：产生音频文件random.wav，并播放。

13）取得音频文件的属性。程序代码如下

| 1 | #A 取得音频文件的属件 |
| 2 | f=wave.open(wav file) |
| 3 | print(f'取样频率={f.getframerate()}，帧数-{f.getnframes()}, |
| 4 | — f·声道={f.getnchannels()},精度={f.getsampwidth()}, |
| 5 | * f.getnchannels(O):.2f}') f·文件秒数={f.getnframes()/(f.getframerate)  |
| 6 | f.close() |

14）双声道音频文件转换为单声道。程序代码如下执行结果：取样频率 $= \! 8 0 0 0$ .帧数=268237，声道 $= 2$ ，精度 $= 2$ ，文件秒数-33.53。

除了读取文件之外，要如何才能直接从麦克风接收音频或是录音存盘呢?我们马上就通过下一个范例来看看该怎么做。

![](figures/908-2-FIGURE.jpg)

## 15 。程序代码如下测试。

| #取得音频文件的属性 |
| import wave |
|  |
| f=wave.open(wav file) |
| print(f'取样频率={f.getframerate()}，帧数={f.getnframes)}, |
| —— f'声道={f.getnchannels()}，精度={f.getsampwidth(O}, |
| 8 f'文件秒数={f.getnframes() / f.getframerate():.2f}') |
| f.close(  |

范例2.麦克风接收音频与录音存盘

请参阅程序：1403录音Iipynb

(1）SpeechRecognition库提供了麦克风收音的功能，并支持语音识别，首先要进行安装：pip install SpeechRecognition.

(2）另外，文字转语音（Text To Speech, TTS）的技术也已非常成熟，因此需一并安装pyttsx3库，下面程序代码会使用到： pip install pyttsx3a

执行结果：注意有些说话者擅长说英文或中文，不过笔者实际测试后发现，其实他们两种语言都可以讲。结果如下

(5）指定说话者：每台计算机安装的说话者均不同，请以D $\mathcal{M}$ 中指定一位。程序代码如下：

(3）麦克风收音，并进行语音识别。程序代码如下：

1#裁人相关库
2 import speech recognition as sr 3 import pyttsx3

(4）列出计算机中的说话者（Speaker 。程序代码如下：

1#列出计算机中的说话者(Speaker）
2 speak = pyttsx3.init()
3 voices = speak.getProperty('voices")
4 for voice in voices:
5 print("Voice:")
6 print(" - ID: %s" % voice.id)
7 print(" Name: %s" % voice.name)
8 print(" - Languages: %s" % voice.languages) 9 print(" - Gender: %s" % voice.gender)
10 print(" - Age: %s" % voice.age)

![](figures/909-7-FIGURE.jpg)

(6）麦克风收音：含文字转语音（Tex To Speech, TTS) 程序会等到持续静默一段时间（预设是0.8秒）后才结束。详细可参阅 SpeechRecognition 官方说明

$$
\pi_{\mathrm{T}} :
$$
echRecogniton2.2）。程序代码如

(8）语音识别：需以参数language指定要辨识的语系。程序代码如下：

①笔者念 $7-$ 段新闻，内容为：「受台风影响，北台湾今天下午大雨特报，有些道路甚至发生积淹，曾文水库上游也传来好消息。

$$
\mathrm{T} \! :
$$

![](figures/910-5-FIGURE.jpg)

## (7）录音存档。程序代码如下：

| 1 | #录音存档 |
| 2 | wav file = "./audio/woman.wav" |
| 3 | with open(wav file, "wb") as f: |
| 4 | f.write(audio.get v wav data(convert rate=16000)) |

![](figures/910-8-FIGURE.jpg)

》执行结果为：「受台风影响北台湾今天下午大雨特报有些道路甚至发曾记殷曾文水库上游也传来好消息。J

执行结果：取样频率=16000，帧数 $= 1 7 3 1 2 8$ ，声道 $\natural1$ ，精度 $= 2$ ，文件秒数 $= 1 0. 8 2$ ，与设定一致。

(）读取音频文件，转为 SpeechRecognition音频格式，再进行语音识别。程序代码如 $\mathrm{F}$ :

3结果大部分是对的，错误的文字均为同音异字。

(9）检查输出文件：播放录音。程序代码如下：

| 上 2 | import IPython |
| 3 | # autoplay=True:自动播放，不须按 PLAY 筵 |
| 4 | IPython.display.Audio(wav file, autoplay=True) |

10）取得音频文件的属性。程序代码如下

| #取得音频文件的属件 |
| # https://docs.python.org/3/library/wave.htmL |
| import wave |
|  |
| f=wave.open(wav file) |
| print(f'取样频率{f.getframerate()},帧数={f.getnframes()},' |
| f'声道={f.getnchannels()},精度={f.getsampwidth()},' |
| 限 * f.getnchannels(O):.2f}' f'文件秒数={f.getnframes()/(f.getframerate()  |
| f.close(  |

| 1 | import speech recognition as sr |
| 2 |  |
| 3 | #读取音频文件，转为音频 |
| 4 | r=sr.Recognizer() |
| 5 | with sr.wavFile(wav file) as source: |
| 6 | audio = r.record(source) |
| 7 |  |
| 8 | #语音辨识 |
| 9 | try: |
| 10 | text=r.recognize google(audio, language='zh-tw') |
| 11 | print(text) |
| 12 | except e: |
| 13 | pass |

执行结果为：「受台风影响北台湾今天下午大雨特报有些道路甚至发曾记殷曾文水库上游也传来好消息。」与麦克风来源一致。

受台风影响台湾今天下午大雨特报有些道路甚至发曾记殷曾文水库上游野传来好消息

受台风影响台湾今天下午大雨特报有些道路甚至发生技烟曾文水库上游野传来好消息

受台风影响台湾今天下午大雨特报有些道路甚至发生记烟曾文水库上游野传来好消息

受台风影响台湾今天下午大雨特报有些道路甚至发生气烟曾文水库上游野传来好消息

受台风影响台湾今天下午大雨特报有些道路甚至发生记燕曾文水库上游野传来好消息

(12）显示所有可能的辨识结果及信赖度。程序代码如下：

1#显示所有可能的辨识结果及信赖度
2 dictl=r.recognize google(audio, show all=True, language='zh-tw') 营中热品热区城A和新和A
if i == 0:
print(f"信赖度-{item['confidence']}, {item["transcript']}") 7 print(f"{item['transcript']}")

$$
\pm\pm\pm\pm\pm\pm\pm
$$

$$
( \pm x ), \bar{\Psi}=0. 8 9 8 2 0 5 8 8,
$$

所有可能的辨识结果如下：

另外，有一个非常棒的语音处理库不得不提，那就是
Librosa，它可以将音频做进一步的解析和转换，我们会在后面实践相关功能，更多内容请参阅Librosa 说明文件
(https://librosa.org/doc/latest/tutorial.htm)

在开始测试之前，还有一些关于音频的概念需要我们先了解。 由于音频通常是一段不规则的波形，很难分析，因此，学者提出傅里叶变换（Fourier Transform 可以把不规则的波形变成多个规律的正弦波形（Sinusoidal）相加，如图 14.11 所示

## 14-2 语音前置处理

![](figures/913-3-FIGURE.jpg)

图14.11傅里叶变换

(图片来源：「ntroduction Basic Audcio Feature Extraction
[6])

(图片来源：「Introduction Basic Audio Feature Exiraction

每个正弦波形可以被表示为

$$
\textsc{s}_{( A, \omega, \varphi)} \quad( t ) \ =\texttt{A g s i n} \left( 2 \pi\ \ ( \omega t-\varphi\right) \ )
$$

式中： $\boldsymbol{A}$ 为振幅：α为频率： $\phi$ 为相位如图14.12所示，可以观察到振幅、频率、相位是如何影响正弦波形的。

(图片来源：Introduction Basic Audio Feature xtacionlb

转换后的波形振幅和频率均相同，原来的X轴为时域（Time $\mathrm{D o m a i n} )$ 就转为频域 $\mathrm{( F r e q u e n c y}$ Domain 如图14.13所示。

(图片来源：Audio Data Analysis Using Deep Learning with Python (Part1) l7)

(图片来源：Audio Data Analysis Using Deep

![](figures/914-5-FIGURE.jpg)

14.12 正弦波形的振幅、频率与相位图

![](figures/914-7-FIGURE.jpg)

图14.13：傅里叶变换将时域转为频域

$$
\boldsymbol{P y t h o n} ~ ( \boldsymbol{P a r t \, 1} ) ~ ~ \stackrel{\lbrack\gamma]} {\Gamma}
$$

不同的频率混合在一起称之为频谱（Spectrum) 而绘制的冬表就称为频谱图（Specrogram) 通常X轴为时间，Y轴为频率， 可以从图表中观察到各种频率的能量，如图14.14所示。

为了方便做语音识别，与处理影像一样，我们会对音频进行特征提取，目前有FBank（Filter Banks) MFCC(Mel-frecuency Cepstral Coefficients）两种，特征提取前须先对声音做前置处理， 如图14.15所示。

(1）分帧：通常每帧是25ms，帧与帧之间重叠10ms，避免边界信号的遗漏，如图14.16所示。

![](figures/915-3-FIGURE.jpg)

图14.14频谱图（Spectrogram

![](figures/915-5-FIGURE.jpg)

图14.15 音频前置处理

(3）加窗 $( \mathrm{W i n d o w} )$ 目的是消除各个帧的两端信号可能不连续的现象，常用的窗函数有方窗、汉明窗（Hamming Window 等。有时候为了考虑上下文，会将相邻的帧合并成一个帧，这种处理方式称为帧叠加（Frane Stacking

在计算频谱时，会将以上的前置处理，包含分帧、加窗、离散傅里叶变换（Discrete Fourier Transform, DFT）合并为一个步骤， 称为短时傅里叶变换（Shor-Time Fourier Transtorm, STFT)
SciPy支持此功能，函数名称为 tt

请参阅程序：14 05 spectrogram.py，由于是以动画呈现， 无法在Jupyter Notebook上展 $\overline{{\Pi}}$ ，故以Python 文件执行。另外

![](figures/916-3-FIGURE.jpg)

$$
\boxed{\S} ~ 1 4. 1 6 ~ \not\cong1 0 5
$$

(2）信号加强：针对高频信号做加强，使信号更清楚

(4）去除噪声（(denoising or noise reducion O

范例1。频谱图实时显示 14-04 waves.py可显示实时的波形。这两支程序均源自于Python audio spectrum analyzer

(3）频谱图实时显示：调用 signal.spectrogram()，显示频谱图，设定显示满100个图表即停止，可依需要弹性调整。程序代码如 $\top$ :

1）加载相关库。程序代码如下

| 3 | import pyaudio |
| 4 | import struct |
| 5 | import matplotlib.pyplot as plt |
| 6 | import numpy as np |
| 7 | from scipy import s signal |

(2）开启麦克风，设定收音相关参数。程序代码如下：

![](figures/917-5-FIGURE.jpg)

![](figures/917-6-FIGURE.jpg)

执行结果：如图14.17所示范例 $2$ 。音频前置处理：利用Librosa函数库了解音频的前置处理程序。

(2）载入文件：调用 ibrosa.load)，传回数据与取样频率可设定参数如下

![](figures/918-2-FIGURE.jpg)

14.17 频谱图实时显示图

(4）关闭所有装置。程序代码如下

![](figures/918-5-FIGURE.jpg)

：14 音讯前置处理.pynb 请参阅程序: 06

(1）加载相关库。程序代码如下

![](figures/918-8-FIGURE.jpg)

1#载人相关库
2 import IPython
3 import pyaudio
4 import struct
5 import matplotlib.pyplot as plt 6 import numpy as np
from scipy import signal
8import librosa
9 import librosa.display #一定要加 10 from IPython.display import Audio Cho-True，表示加载时采用高质量模式（high-qualiy mode)

$$
\overline{{2}} \mathrm{s r} \!=\! 4 4 1 0 0
$$
，指定取样频率。

Bres type-kaiser fast $\Gamma$ ，表示快速载入文件。

$$
\pi\equiv\Gamma\pm\pi\pm\pm\pm\Gamma\Gamma:
$$

执行结果：取样频率-22050，总样本数-（739329) O

(3）绘制波形。程序代码如下：

1#绘制波形
$$
2 \quad\mathrm{1 i b r o s a. \, d i s p l a y \,. w a v e p l o t \, ( d a t a, ~ s r )}
$$

执行结果： 14.18月所示
如图

![](figures/919-8-FIGURE.jpg)

图14.18、绘制波形

(4）显示频谱图：先调用 melspectrogram()取得梅尔系数 (Me) 再调用power to db()转为分贝（db） 最后调用 specshow()显示频谱图。程序代码如下：

(5）存档： $\mathrm{v 0. 8}$ 版本后已不支持 brosa.outut.write wav函数，因此须改用soundfile库。程序代码如下

![](figures/920-2-FIGURE.jpg)

执行结果：如图14.19所示

![](figures/920-4-FIGURE.jpg)

14.19 显示频谱图冬

![](figures/920-6-FIGURE.jpg)

(6）接着进行特征提取的实践，可作为深度学习模型的输入

(7）短时得里叶变换（Short-Time Fouriertransform 包括分帧、加窗、离散傅里叶变换，合并为一个步骤。程序代码如下

(10）接着说明Librosa 内建音频加载的方法。程序代码如下:

![](figures/921-2-FIGURE.jpg)

①传回一个矩阵 $\boldsymbol{D}$ 、其中包含频率、时间

②执行结果： 1025,1445) complex64.

 $( 8 ) \, \ \mathrm{M F C C} \! :$ 参数n_micc 可指定每秒要传回几个MFCC frame，通常是 $1 3$ 、40 $\uparrow$ 。程序代码如下：

1 $\# \ \ m f c c$ = librosa.feature.mfcc(y=data, sr=sr, n mfcc=40) 2mfcc
3mfcc.shape

执行结果：（40,1445) O

$$
( 9 ) \mathrm{\ L o g-M e l \ S p e c t r o g r a m_{o}}
$$
程序代码如下

1# $L o g-M e l$ Spectrogram
2 melspec = librosa.feature.melspectrogram(data, sr, n fft=1024,
3 hop length=512, n mels=128) 4 logmelspec = librosa.power to db(melspec)
5 logmelspec.shape

执行结果：（128, 1445) O

$$
\mathrm{T} :
$$

 $^1_{2}$ 53030区5路区40m10 librosa.util.list

执行结果如下：

 $( 1 4 )$ 音频处理与转换：Librosa 支持多种音频处理与转换功能，我们逐一来实验。

 $( 1 5 )$ 重取样（Resampling）：从既有的音频重取样，通常是 ${\cal M}$ 高质量的取样频率，通过重取样，转换为较低取样频率的数据。 程序代码如下：

## （11）加载Librosa默认的内建音频文件。程序代码如下：

1#载入 $l i b r o s a$ 内建音频
2 y, sr = librosa.load(librosa.util.example audio file()) 3 print(f'取样频率-{sr}，总样本数-{y.shape}')

执行结果：取样频率-22050，总样本数-（1355168） O

12）播放：利用IPython模块播放音频。程序代码如下：

1#播放
2 Audio(y, rate=sr, autoplay=True)

13） 加载内建音频文件。程序代码如下： 指定D

1 # hq=True :high-quality mode
2#勃拉姆斯 牙利舞曲
3 y, sr = librosa.load(librosa.example('brahms', hq=True)) 4 print(f'取样频率={sr}，总样本数={y.shape}'）
5 Audio(y, rate=sr, autoplay=True)

(16）将和音与打击音
$$
\not\hspace{6 p t} \not\equiv\hspace{6 p t} \mathrm{( H a r m o n i c / P e r c u s s i v e}
$$
Separation) 调用 ibrosa.effects.hpss()可将和音与打击音分离， ${\cal M}$ 打击音可以找到音乐的节奏 $( \mathrm{T e m p o} )$ 程序代码如 $\mathrm{F}$ :

![](figures/923-1-FIGURE.jpg)

执行结果：如图14.20所示

![](figures/923-3-FIGURE.jpg)

图14.20和音与打击音分离

(17）取得打击音每分钟出现的样本数。程序代码如下：

1 #打击音每分钟出现的样本数
 $~ 2$ print(librosa.beat.tempo(y, sr=sr))

执行结果：143.5546875

$$
( 1 8 ) ~ ~ \exists\arraycolsep. 5 p t \mathrm{~ \pm~ J ~} \ddag\ddag\ddag\ddag\ddag\ddag\lines\ddag\hfil\hfil\hfil\hfil\f\dag\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\hfil\ 7 \hfil\ 7 \hfil\ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ 7 \ \ 7 \ 7 \ \ 7 \ \ 7 \ \ 7 \ \ \ 7 \ \ 7 \ \ \ 7 \ \ \ 7 \ \ 7 \ \ \ 7 \ \ \ \ 7 \ \ \ 7 \ \ \ \ 7 \ \ \ \ \ 7 2 \ \ \ \ \ \ \ \ 7 2 2 2 \ \ \ \ \ \ \ \ \ \ \ \ \ 7 2 7 2 2 2 2 2 2 \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 7 2 7 2 7 2 7 2 2 2 2 2 2 2 2 2 \ 7 2 2 \ 7 2 2 2 2 2 2 2 2 2 2 2 \ 7 2 \ 2 2 \ 7 2 2 2 2 \ 2 2 2 2 \ 2 7 2 7 2 7 2 7 2 7 2 7
$$
程序代码如下

执行结果：如图14.21所示，y轴显示 $1 2$ 个半音，pitch 是有周期的循环。

(20）可任意分离频谱，例如将频谱分为 $8$ 个成分
(Component) 以非负矩阵分解法（NMF）分离频谱，NMF类 $\mathit{u l l}$ 于主成分分析 $\left( \mathrm{P C A} \right)$ 程序代码如下：

## (19）绘制色度图（Chromagram) chroma 为半音 (semitones 可提取音准（pitch）信息。程序代码如下：

![](figures/924-3-FIGURE.jpg)

![](figures/924-4-FIGURE.jpg)

图 $1 4. 2 1$ 绘制色度图

![](figures/924-6-FIGURE.jpg)

(21）显示成分与Activations。程序代码如下： (22）再以分离的Components 与 Activations重建音频。程序代码如下：

执行结果：播放与原曲一致，这部分的功能可用于音乐合成或修改。

![](figures/925-2-FIGURE.jpg)

执行结果：X轴为 $8$ 个成分，如图14.22所示

![](figures/925-4-FIGURE.jpg)

图14.22显示成分与Aciations

![](figures/925-6-FIGURE.jpg)

(23）只以第一Component 与 Activation 重建音频：播放效果与原曲大相径庭。程序代码如下：

(24）Pre-emphasis：用途为高频加强。前面说过，人类对高所以能利用此技巧，补强音频里高频的部分。程频信号较不敏感，
序代码如下：

![](figures/926-2-FIGURE.jpg)

![](figures/926-3-FIGURE.jpg)

执行结果：如图14.23所示。可以很明显看到高频已被补强。

(25）正态化：在导入机器学习模型之前，我们通常会先进行特征缩放，除了能提高准确率外，也能加快优化求解的收敛速度， 具体方式就是直接使用 Scikit-Learn 的 minmax scale函数即可。 程序代码如下：

![](figures/927-1-FIGURE.jpg)

图14.23 Pre-emphasis

![](figures/927-3-FIGURE.jpg)

执行结果：如图14.24所示

![](figures/927-5-FIGURE.jpg)

除了Librosa 库之外，也有 python speech features库，提供
、包括MFCC/FBank等，安装指令如读取音频文件特征的功能，
下：

## 图14.24正态化

pip instal phon sgeech eatures

范例 $\textbf{3}$ .特征提取MFCC、Filter bank向量。

请参阅程序:14.07 python speech features.ipynb

(1）加载相关库。程序代码如下

(2）加载音乐文件。程序代码如下

1#载人音乐文件
$$
2 : s r \,, \; \; d a t a \,=\, w a v {\mathrm{f \, i \, l e \,. \,}} r e a d {\mathrm{( ~^{n} \,. \, / \, a u d 1 o \, / \, w A V_{\mathrm{_{\small{~ 1 N G \,. \, w a v^{\small{n}} ~}} ~ )}}}}
$$

(3）读取 $\mathrm{\mathsf{M F C}}.$ ilter bank特征。程序代码如下：

1#读取 MFCC、Filter bank 特征
2 mfcc features = mfcc(data, sr)
3 filterbank features = logfbank(data, sr)
4
5 # Print parameters
6 print('MFCC 维度：'，mfcc features.shape）
7 print("Filter bank 维度:', filterbank features.shape)

$$
\pm\hbar\hbar\pm\L\pm\hbar\hbar:
$$

DMFCC维度：（6705,13) O

BfIterbank维度： $( 6 7 0 5, 2 6 )$ 。

有关音频的转换还有另一个选择，读者可以自
http:/fimpeg.org/download.html下载fimpeg工具程序，它支持的功能非常多，包括裁剪、取样频率、编码等，详细说明可参阅 fimpeg官网（Http:/fimpeg.org/documentation.himl）。下面介绍将input.wav 转为 output.wav，并改变取样频率、声道、编码： fimpeg.exe -i output.wav -ar 44100 -ac 1-acodec pom sl6le
output.wav

$$
\mathrm{( 4 ) ~ \mathrm{M F C C, ~ F i l t e r ~ b a n k} ~ \sharp\Delta~} \! \stackrel{\star} {=} \! \stackrel{\star} {\Delta} \! \stackrel{\star} {\gg} \! \circ
$$
程序代码如下

1#绘图
2 plt.subplot(2,1,1)
3 mfcc features = mfcc features.T
4 plt.imshow(mfcc features, cmap=plt.cm.jet,
5 extent=[0, mfcc features.shape[], $\big8$ , mfcc features.shape[o]], aspect='auto")
6 plt.title('MFCC')
7
8 plt.subplot(2,1,2)
g filterbank features = filterbank features.T
10 plt.imshow(filterbank features, cmap=plt.cm.jet,
11 extent=[o, filterbank features.shape[1], $\big8$ , filterbank features.shape[0]], aspect-'auto') 12 plt.title('Filter bank')
13 plt.tight layout()
14 plt.show()

执行结果：如图14.25所示

![](figures/929-4-FIGURE.jpg)

$$
\boxed{\xi} 1 4. 2 5 \quad\Sigma^{\Delta} \boxed{\xi}
$$

了解前面音频处理与转换的内容后，我们算是做好热身 $\overline{{\jmath}}$ ，接下来，就要正式实践几个深度学习相关的应用

范例1。音乐曲风的分类。我们利用上一节特征提取的 MFCC 向量，导入到CNN模型，就可以分类曲风了

htto:/marsyas.info/downloads/datasets.html，提供各种不同大小的数据集，本范例采用最大的数据集

(http:/oihpic.cvicasoundlgenres.tar.gz），共有 $1 0$ 个类别， 每个类别各有100首歌，每首歌的长度均为 $3 0$ 秒。10个类别分别为：Blues、Classical、Country、Disco、Hiphop、Jaz、Metal Pop、Reggae、Rock.

## 14-3 语音相关的深度学习应用

1408音乐曲风分类.ipynb 请参阅程序： 08

数据集：MARSYAS GTZAN Genre Collection

程序修改自Audio Data Ar
$$
\mathit{n a l y s i s} \, \mathit{U s i n g \, \mathit{D e e p \, L e a r n i n g \, w i t}}
$$
h
Python (Part2) 

(1）加载相关库。程序代码如下 3CNN输入需要四维数据（批数、宽度、高度、颜色） 因此，将训练数据转为四维

## (2）加载音乐文件：文件目录如图1426所示

| lassica |
| country |
| disco |
| pnop azz  |
| metal |
| Dop |
| eggae |
| Ae  |

冬 $1 4. 2 6$ 加载音乐文件

调用lbrosa.load )加载音乐文件

调用 ibrosa.ieature.mioc()将数据转为MFCC向量
|

## 程序代码如下：

![](figures/931-7-FIGURE.jpg)

执行结果： (800, 40, 1077,1) . $( 2 0 0, \, 4 0, \, 1 0 7 7, \, 1 )$ 切割为训练数据800批、测试数据200批

(5）CNN模型：大部分的文章使用 AveragePooling2D取代 MaxPooling2D，不过笔者实测后发现，两者效果并没有太大差异，所以这里两种模型都进行列出。程序代码如 $\mathrm{F}$ :

执行结果： $( 1 0 0 0, 4 0, 1 0 7 7, 1 )_{\circ}$ 

）特征缩放：数据正态化。程序代码如下： (3)

## (4）将数据切割为训练数据和测试数据。程序代码如下

1#数据切割
2 from sklearn.model selection import train test split
3 y = np.array(y)
4 Xtrain, Xtest, $y$ train, $y$ test - train test split(X norm, y, test size=.2) 5 X train.shape, X test.shape

![](figures/932-6-FIGURE.jpg)

(6）模型训练、评分。程序代码如下：

训练准确率高达99.899%，而验证准确率只有
执行结果：
 $5 2. 5 0 \%$ ，表示模型有过度拟合（Overitting）的现象。测试准确率只有46.00%， ，如图14.27所示。
也偏低，

执行结果：训练20周期，准确率为 $4 6 \%$ ，并不高

(7）对训练过程的准确率绘图。程序代码如下

![](figures/933-3-FIGURE.jpg)

![](figures/933-4-FIGURE.jpg)

图 $1 4. 2 7$ 准确率绘图

Music Genre Recognition using Convolutional Neural Networks （CNN)-Part $\boldsymbol{1}$ 10一文提到以下两个改善的方向

(1）将音乐数据分段（Segmentation）：每一段视为一组数据，使用pydub 库调用 AudioSegment.from wav()载入文件，即可切割。

(2）数据增补（Data Augmentation）：将音乐存储为 png $\grave{\chi}$ 件，再利用 mageDataGenerator类别进行数据增礼 $\vdash$ ，产生更多的数据。

综合多篇文章的实验结果来看，数据增补并无太大帮助，但数据分段的方法却能大幅提高准确率，训练70周期后，训练准确率高达99 $5 7 \%$ ，而验证准确率也达到了89.03 $\not\supset$ 。笔者对数据分段进行的实验如 $\mathrm{F}$ 。

(1) 将每个文件分为10段，训练数据变成10000
数据分段：
批。程序代码如下：

(2）之后的处理程序都一样，不再赘述，实验结果如图14.28 所示。

| 1 | #戴人音乐文件 |
| 2 | x=None |
| 3 | y=[] |
| 4 | for i,g in enumerate(genres): |
| 5 | pathlib.Path(f'./GTZAN/genres//{g}').mkdir(parents=True, exist ok=True) |
| 6 | for filename in os.listdir(f'./GTZAN/genres/{g}'): |
| 7 | songname = f'./GTZAN/genres/{g}/{filename} |
| 8 | data, sr = librosa.load(songname, mono=True, duration=25) |
| 9 | try: |
| 10 | if i==$0: |
| 11 | segment length = int(data.shape[o] / 10) |
| 12 | for jin range(1o): 89 |
| LD 1A | segment - udtdlJ r segmenc-lengtn: tyti) r segmenc-lengtnj #65十-60m6十-5001  |
| 1+ 1C | #pucnc(SEgmeic·ope mEAA-1ihAcs fAstunA mE-A -cAdmantcn-cnn mEA--A0Y |
| 一 16 | 555555 #nnint data chane. mfec.shane)  |
| 一 17 | 厂 ifXis None:  |
| 18 | X=mfcc.reshape(1. 40.-1.1) |
| 19 | else:  |
| 20 | X= np.concatenate((X, mfcc.reshape(1, 40,-1,1)),axis=0) |
| 21 | y.append(i) |
| 22 | except:  |
| 23 | print(i) |
| 24 | raise Exception('' |
| 25 | print(X.shape, len(y)) |

执行结果：训练准确率高达 $8 7. 1 2 \%$ ，而验证准确率为
69.44 $\not\sim$ ，测试准确率也有 $6 7. 8 0 \%$ ，模型有了明显改善。如果要再提高，可参阅上文，进行数据增补

接着再来看另一个范例，Google 收集短指令的数据集，包括常用的词汇，如 stop、play、up、down、right、lett等，共有30 个类别，如果辨识率很高，我们就能将其应用到各种场域，如玩游戏、控制简报等。

范例 $2$ 。短指令辨识。一样使用MFCC向量，导入CNN模型，即可进行分类。同时笔者也会结合录音实测，示范如何控制录音与训练样本一致（Alignment) 其中有些技巧将在后面说明。

数据集：Google's Speech Commands Datase
(http://download.tensorfilow.org/data/speech commands test set v0.02.tar.gz)

每个文件长度约为 $1$ 秒，无杂音。它也附一个有杂音的目录与无杂音的文件混合在一起，增加辨识的困难度。

![](figures/935-5-FIGURE.jpg)

图14.28训练与验证准确率绘图

请参阅程序：14 09_短指令辨识ipynb (2）任选一个文件测试，该文件发音为 happy。程序代码如下：

1）加载相关库。程序代码如下

![](figures/936-2-FIGURE.jpg)

1#载人相关库
2 import pandas as pd
3 import numpy as np
import matplotlib.pyplot as plt
5import librosa
6 import librosa.display
import os
8 import pathlib
9 import csv
10 import tensorflow as tf
11 from tensorflow.keras import layers

$$
\mathrm{T} :
$$

1#任选一文件测试，发音为 hoppy
2 train audio path = './GoogleSpeechCommandsDataset/data/'
3 data, sr = librosa.load(train audio patht'happy/0ab3b47d nohash O.wav') 4
5#绘制波形
6 librosa.display.waveplot(data, sr)
7 print(data.shape)

执行结果：如图14.29所示，数据长度为19757.

![](figures/936-7-FIGURE.jpg)

图 $1 4. 2 9$ 测试文件

(3) 一个文件测试，该文件发音也为 happy。 再选

·执行结果：与图14.29相比较，同样有两段振幅较大的声波，这代表happy的两个音节，但是，因为录音时每个人的起始发音点不同，因此，波形有很大差异，必须收集够多的训练数据才能找出共同的特征。

②数据长度为22050，与上一笔音频文件的数据长度亦不相同，如图14.30所示。

$$
\begin{array} {c} {( 5 ) \boxplus( \pm\pm\pm\pm\mp\mp\mp\pm\pm\pm\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\end{array}
$$
属性：每个文件的长度不等，但都接近

![](figures/937-3-FIGURE.jpg)

![](figures/937-4-FIGURE.jpg)

图 $1 4. 3 0$ 测试文件

(4）播放文件，观察其中的差异。程序代码如下

![](figures/937-7-FIGURE.jpg)

执行结果：取样频率 $= \! 1 6 0 0 0$ ，帧数-14336，声道 $= 1$ ，精度 $= 2$ ，文件秒数 $= 0. 9 0_{\circ}$ 

(6）只取三个短指令测试：只放 $\lambda\Xi\prime$ 子目录的文件于测试目录内。程序代码如 $\mathrm{T}$ 中

执行结果：如图14.31所示，文件数目略有不同。文件数一 [1713, 1733, 1742].

![](figures/938-3-FIGURE.jpg)

1#取得子日录名称
2 labels=os.listdir(train audio path) 3 labels

执行结果： $\lfloor$ 'beda .『catu, Thappy l

(7）统计子目录的文件数。程序代码如下

![](figures/938-6-FIGURE.jpg)

の以ibrosa加载音频文件，白于每个音频文件的长度 $\pi-,$ 因此，使用np.pad 与 np.resize将其统一长度为 $1$ 秒

3最后将每个类别的文件合并存储为npy文件格式，之后如果要重新测试，就可以直接加载，省去一一读取和解析文件的时间。

![](figures/939-2-FIGURE.jpg)

图14.31统计子目录文件数

$$
( 8 ) \jmath\jmath\jmath\b\equiv\b\imath\b\imath\b\imath\imath\b\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\imath\cdot\imath\imath\tjmath\imath\imath\imath\imath\imath\imath\imath\cdot\imath\cdot\cdot\cdot\cdot\cdot\tjmath\imath\imath\imath\imath\jmath\imath\imath\circ\imath\circ\cdot\cdot\cdot\t7 \circ\circ\t7 \circ\end{( \circ\mathbf\circ
$$

D扫描每个目录的文件。

$$
\pi\equiv\pi\pm\pi\pm\pi\pm\Gamma:
$$

执行结果：以直方图的方式统计文件的长度，发现大部分的音频文件是接近于1秒钟，但有少数音频文件的时间长度非常短，事实上这会影响训练的准确度，故统一长度有其必要性。

![](figures/940-1-FIGURE.jpg)

X轴为音频文件长度，Y轴为笔数 14.32所示
如图

![](figures/940-3-FIGURE.jpg)

图 $1 4. 3 2$ 加载音频文件

(9）载入npy文件的测试。程序代码如下： 执行结果：加载npy文件的速度很快，一瞬间就将所有音频文件全部载入 $\overline{{\jmath}}$ 

(10）特征缩放：MFCC·会有标准化的效果，后面会有测试因此这里不需进行特征缩放。

(1）计算每个音频文件的 $\mathrm{\mathsf{M F C}}$ ，同时转换成 CNN模型的输入格式。程序代码如 $\mathrm{F}$ :

(13）建立CNN模型，与前一个范例相同，除了最后一层 $\not\! y \vert$ ，类别数量改为 $3_{\circ}$ 程序代码如下：

![](figures/941-4-FIGURE.jpg)

| 1 | #计拿 MFCC |
| 2 | MFCC COUNT=40 |
| 3 | X=None |
| 4 | for class wave in all wave: |
| 5 | for data in class wave: |
| 6 | mfcc = librosa.feature.mfcc(y=data, sr=len(data), n mfcc=MFCC COUNT |
| 7 | # print(data.shape, mfcc.shape) |
| 8 | if Xis None: |
| 9 | X= mfcc.reshape(1, MFCC COUNT,-1,1) |
| 10 | else: |
| 11 | X=np.concatenate((X, mfcc.reshape(1, MFCC COUNT, -1, 1)), axis=o |
| 12 | print(X.shape) |
| 13 | print(X.shape, len(y)) |

执行结果：输入数据维度为158. 40,32,1) O

 $( 1 2 )$ 数据切割。程序代码如下：

| # 数据切割 |
| from sklearn.model selection import train test split |
| test size=.2) X train, x test,y train,y test- train test split(X,y,t  |
| X train.shape, x test.shape |

![](figures/942-0-FIGURE.jpg)

## 14）模型训练与评分。程序代码如下

![](figures/942-2-FIGURE.jpg)

执行结果：准确度高达 97.88%

 $( 1 5 )$ 对训练过程的准确率绘图。程序代码如下

![](figures/942-5-FIGURE.jpg)

## 执行结果：如图14.33所示

![](figures/942-7-FIGURE.jpg)

 $( 1 7 )$ 任选一个文件进行预测，该文件发音为bed。程序代码如下：

## 图14.33准确率绘图

(16）定义一个预测函数，功能包括如下
统一文件长度，右边补 $0$ ，过长则截掉。
转为 MFCC.
③以MFCC预测词汇。
过程中显示原始波形和 $\mathrm{\mathsf{M F C}}$ 散点图。

16）定义一个预测函数，功能包括如下。

D统一文件长度 $0$ ，过长则截掉。
右边补

2转 $\grave{\aleph}$ I O MFCO

$$
\textcircled{3} \downarrow\lambda\mathrm{~ M F C C ~ i f f} \downarrow\pi\imath\pi\downarrow\Xi\imath\Gamma_{\circ}
$$

の过程中显示原始波形和 MFCO·散点图

$$
\star\Xi\equiv\pm\pm\exists\pm\pm\pm\Gamma:
$$

![](figures/943-9-FIGURE.jpg)

$$
\flat\sqcap\mathrm{F} :
$$

执行结果： $[ [ 0. 9 9$ ，0.，0.01]，正确判断为bed。如图14.34 第二张图所示，除了起始的信号为很大的负值以外，其他都介于
$$
(-1 0 0, \, 1 0 0 )
$$

）再任选一个文件测试，该文件发音为cat。程序代码如 (18)
下；

(19）接着再任选一个文件测试，该文件发音为happy。程序代码如下：

![](figures/944-3-FIGURE.jpg)

图14.34、预测结果

1 $^\sharp$ 在选一文件测试，该文件发音为cat
2 predict(train audio path+'cat/Oac1Sfe9 nohash O.wav')

执行结果：[[O.， 1.，0.]，正确判断为 cat，如图14.35所示。

![](figures/944-7-FIGURE.jpg)

图 $1 4. 3 5$ 测试文件

执行结果：[., .，1.]。正确判断为 happy，如图14.36所示。

最后的参数为存档的文件名。程序录音长度设为 $2$ 秒，再利用上述predict欧数，截取中间1秒钟的音频。

执行结果： $[ \rfloor1$ ., 0.， 0. $\lbrack\ ]$ ，正确判断为 happy，下面第一张图为原录音波形，下面第二张图为截取中段的波形，如图14.37所 $\overline{{\Pi}}$ 。

![](figures/945-3-FIGURE.jpg)

图 $1 4. 3 6$ 测试文件

(20）自行录音测试：笔者开发一个录音程序 14-10 record.py，用法如下：

python 14-10 record.py
GoogleSpeechCommandsDataset/happy.wav

GoogleSpeecCommandsDalasethappy.wav

）测试：该文件发音为hapy。程序代码如下 (21)

1# 7 $h a p p y$ 
测试，该文件发音为
2 predict('./GoogleSpeechCommandsDataset/happy.wav') 执行结果： $\amalg[ 1., \ 0.$ ，0.]，正确判断为bed，如图14.39所示。

![](figures/946-1-FIGURE.jpg)

图14.37自行录音测试1

(22）再录音测试cat.。程序代码如下：

![](figures/946-4-FIGURE.jpg)

执行结果： $[ [ 0., ~ 1.$ ，0.]1、正确判断为oat，如图14.3所示

![](figures/946-6-FIGURE.jpg)

图14.38自行录音测试2

(23）再录音测试,bed。程序代码如下

1#测试，该文件发音为bed
2 predict('./GoogleSpeechCommandsDataset/bed.wav')

总地来说，如果用训练数据进行测试的话，准确率都还不错， 但若是自行录音则准确率就差强人意 $\overline{{\j}}$ ，可能原因有两点：第一是笔者发音久佳，第二是录音的处理方式与训练方式不同。因此，建议还是要自己收集训练数据为宜，也建议读者发挥创意，多做实 $y_{\Sigma}$ 。

以上是参酌多篇文章后修改而成的程序，之前笔者有发表类似的程序在Blog $\models$ ，许多网友对这方面的应用非常感兴趣，提出很多的问题，读者可参阅「Day 25：自动语音识别（Automatic
Speech Recognition）观念与实践」，此外，Kaggle 上也有一个关于这个数据集的竞赛，可参阅「TensorFlow Speech
Recognition Challengea lt2.

笔者另外还做了一个实验，以支持向量机（Support Vector Machine, $\mathrm{S V M}$ 算法与主成分分析 $\left( \mathrm{P C A} \right)$ ，对上述数据训练和预测。测试结果与CNN模型相同， $\P$ 是训练速度更快，读者可参阅14 11 SVM_短指令辨识.ipynb，处理程序与14 09短指令辨识.ipynb类似，细节不再赘述。白于SVM 输入只接受二维，而 MFCC 本身即为二维，需重置为一维，又因变量过多，因此使用 PCA降维，使其符合输 $\lambda$ 条件。

![](figures/947-3-FIGURE.jpg)

14.39自行录音测试 $3$ 冬

上述实验只能辨识短指令，假使要辨识一句话，或者更长的一段话，那就力不从心了，因为讲话的方式千变万化，很难收集到完整的数据来训练，所以解决的办法则是把辨识目标再细化，以音节或音素（Phoneme）为单 $\overrightarrow{\Upsilon}$ ，再使用语言模型，并考虑上下文才能精准预测，下一节我们就来探讨相关的技术。

自动语音识别（Automatic Speech Recognition, ASR）的目标是将人类的语音转换为数字信号，之后计算机就可进一步理解说话者的意图，并做出对应的行动。例如，指令操控，应用于简报上下页控制、车辆和居家装置开关的控制、产生字幕与演讲稿等。

英文的词汇有数万个，假如要进行分类，模型会很复杂，需要很长的训练时间，准确率也不会太高，因为，相似音太多，因此自动语音识别多改为将音素（Phoneme）作为预测目标，依据维基百科的的说明如 $\mathrm{F}$ 。

音素，又称音位，是人类语言中能够区别意义的最 $\imath\rfloor\omicron$ 声音单 $\overrightarrow{\nu}$ ，一个词汇由一至多个音节所组成，每个音节又白一至多个音段所组成,音素类似于音段,但音素定义是要能区分语义

举例来说，bat 由 $3$ 个音素 $| \mathrm{b} /$ 、e/、/t/所组成，连接这些音素，就是bat的拼音（Pronunciation），然后按照拼音就可以猜测到一个英文词汇，当然，有可能发生同音异字的状 $\gg\Pi$ ，这时就必须依靠上下文做进一步的推测了。如图14.40所示，Human 单字被切割成多个音素HH、 $\mathbf{Y}$ 、UW、M、AH、N。

## 14-4自动语音识别

![](figures/949-5-FIGURE.jpg)

(图片来源：Indian Accent Speech Recognition
hitos://anandai.medium.com/indian-accent-speech-recognition-
2d433eb7edac)

各种语言的音素列表可参考「Amazon Polly支持语言的音素 (nttps://docs.aws.amazon.com/zh tw/oolly/latest/dg/ref-
phoneme-tables-shell.tml），以英文/美国（en-US）为例，音素列表主要包括无音和辅音，共约40~50个，而中文则另外包含声调 (一声、二声、三声、四声和轻声）

(1）信号处理与特征提取：先将语音信息进行傅里叶变换、 去杂音等前置处理，接着转为特征向量，如MFCC、LPC（Linear Predictive Coding

（2）声学模型（Acousic Model) 通过特征向量，转换成多个音素，再将音素组合成拼音，然后至拼音字典（Pronunciation $\mathrm{D i c t i o n a r y} )$ 里比对，找到对应的词汇与得分

至14.40音素辨识示意图

141所示，自动语音识别的流程可分成以下四个步骤如图

![](figures/950-6-FIGURE.jpg)

图14.41自动语音识别架构

(3）语言模型（Language Model 依据上一个词汇，猜测目前的词汇，事先以n-gram 为输 $\lambda$ ，训练模型，之后套用此模型，计算一个语言的得分。

(4）解码搜寻（Decoding Search 根据声学得分和语言得分来比对搜寻出最有可能的词汇。

经典的 GMM-HMM算法是过去数十年来语音识别的主流，直到2014年 Google学者使用双向LSTM,以GTC（Conmectionis Temporal Classification）为目标函数，将音频转成文字，深度学习算法就此涉足这个领域，不过，目前大部分的工具箱依然以GMM-HMM算法为主，因此，我们还是要先来认识 GMM-HMM 的运作原理。

自动语音识别的流程可用贝式定理（Bayes'Theorem）来表示：

(1）W就是我们要预测的
$$
\exists\! \Ji\Gamma~ ( W_{1}, ~ W_{2}, ~ W_{3}... )
$$
0O是音频
的特征向量。

$$
( 2 ) ~ P ~ ( \mathit{W I O} )
$$
已知特征向量，预测各个词汇的概率，故以
argmax找到获得最大概率的W，即为辨识的词汇。

$$
\begin{array} {l} {W=\operatorname{a r g} \operatorname* {m a x} P ( W \, | \, O )} \\ {W=\operatorname{a r g} \operatorname* {m a x} \frac{P ( O \, | \, W ) P ( W )} {P ( O )}} \\ {W=\operatorname{a r g} \operatorname* {m a x} P ( O \, | \, W ) P ( W )} \\ \end{array}
$$

式中各项说明如下。

(3）公式中的 $P \ \left( O \right)$ 不影响W，可省略

$$
( 5 ) ~ ~ P ~ ~ ( \boldsymbol{W} )
$$
语言模型，以隐藏式马尔可夫模型（Hidden
Markov Model，HMM）算法建构。

高斯混合模型是一种非监督式的算法，假设样本是白多个正态分布混合而成的，则算法会利用最大似然估计法（MLE）推算出母体的统计量（平均数、标准差），进而将数据分成多个集群
(Clusters），如图14.43所示。其中会应用到声学模型，就是以特征向量作为输 $\lambda$ ，算出每个词汇的可能概率。

以声学模型推测出多个音素后，就可以比对拼音字典，找到相对的词汇，如图14.44所 $\overline{{\Pi}}$ 。左边是词汇，右边是对照的音素，最 (4）P $( O | W )$ ：通常就是声学模型，以高斯混合模型 (Gaussian Mixture Model,GMM）算法建构如图14.42所示。

![](figures/952-4-FIGURE.jpg)

图14.42、一维高斯混合模型示意图

(图片来源：GCasian Mixture Models Explainedu LHl)

![](figures/952-7-FIGURE.jpg)

图14.43”二维高斯混合模型示意图

隐藏式马尔可夫模型（Hidden Markov Model，HMM）系利用前面的状态 $( k \!-\! 2, ~ k \!-\!... )$ 预测目前状态（k）。应用到语言模型就是以前面的词汇为输 $\lambda$ ，预测下一个词汇的可能概率，即前面提到 NLP的n-gram 语言模型。声学模型也可以使用HMM，以前面的音素推测日前的音素，如一个字的拼音为首是 $\rightharpoondown$ ，那么接着久就绝对不会出现。

又比方，and、but、cat三个词汇，采用bi-gram 模型，我们就要根据上一个词汇预测下一个词汇的可能概率，并取其中概率最大者，如图14.45 所示。

后两个词汇同音，故标示#1、#2

下雨xia4 ii 3
今天jinl t ianl
会hui4
北京b ei3 jingl
去qv4
吗mal
天气tianlqi4
怎么样 $z$ en3 m o5 ii ang4 旅游Iv3iiou2
明天m ing2 tianl
的de5
还是h ai2 sh i4
中zh ongl#1
忠zh ong1 #2

图14.44拼音字典示意图

(图片来源：「语音识别系列2--基于 WFST译码器 u012361418的博客-程序员宅基地」15)

$$
P ( w )=\prod_{k=1}^{K} P ( w_{k} \mid w_{k-1}, \cdots w_{1} )
$$

最后，综合GMM和HMM模型所得到的分数，再白译码的方式搜寻最有可能的词汇。小型的词汇集可采用维特比译码（Viterbi Decoding）进行精确搜索（Exact Search），但大词汇连续语音辨识就会遭遇困难，所以一般会改为采用光束搜寻（Beam
Search）、加权的有限状态转换机（Weighted Finite State
Transducers, WFST）或其他算法，如图 14.46所示。如要获得较完整的概念可参阅「现阶段大词汇连续语音辨识研究之简介文的说明。

![](figures/954-1-FIGURE.jpg)

图14.45 bigram 语言模型结合HMM的示意图

(图片来源：「爱丁堡大学语音辨识课程」第11章四

![](figures/954-4-FIGURE.jpg)

大词汇连续语音识别的流程图14.46

Kaldi是目前较为流行的语音识别工具箱，它囊括了上一节所介绍的声学模型、语言模型及译码搜寻的相关函数库实践，由
Daniel Povey等研究人员所开发，源代码为C++，其安装程序较复杂，必须安装许多 $\big< \g$ 用程序和第三方工具，虽然可以在Windows 操作系统上安装，但是许多测试步骤均使用Shell脚本（*.sh） 因此，最好还是安装在Linux环境上，由于笔者手上并没有相关设备，只好跳过这方面的实践，请读者见谅。

(1下载giclone tpsillgithub.oakaliakrkaigi kaldi -origin upstream

④使月configure进行配置，编译Kaldi。其中OpenFst 是加权有限状态转换器 $\mathrm{( W F S T )}$ 的函数库，Sclite 是计算错误率

## 14-5 白动语音识别实践

以下仅列举Kaldi的相关资源，供各位参考

(2）安装NVidla GPU卡驱动程序和CUPA Tookit

(3)安装 ali.

$$
\bigoplus_{\mathrm{c d}} \mathrm{k a l d i_{o}}
$$

2检查相关的开发工具是否已安装，若有缺少，必须补齐

® 包括OpenFst、CUB、Solite、
安装第三方工具，
Sph2pipe、IRSTLM/SRILMKaldi Im 语言模型工具 OpenBLAS/MKL矩阵运算函数库。

安装完成之后就可以进行一些测试了，相关操作说明可参考 Kaldi官网文件（htt:kadi-asr.org/doc/index.html) 内容相当多，需投入不少精力来研读，请读者自行参阅。

另外，各大学的机电系都有开设整学期的课程，名称即为语音识别（Speech Recognition) 网络上有许多公开的教材，包括影片和投影片，有兴趣的读者可前往搜导。

$$
\mathit{)}_{\circ}^{\mathit{( h t t p s : / w w w. e e. c o l u m b i a. e c}}
$$
ui-stanchential2e6870/outline.html

最 $\sqrt{\Pi}$ ，语音识别必须要收集大量的训练数据，而OpenSLR就有提供非常多可免费下载的数据集和软件，详情可参考官网说明
$$
\mathrm{( h t t p s : / / w w w. o p e n s l r. o r g / r e s t )}
$$
ources.php），其中也有中文的语音
数据集 $\mathrm{C N-C e l e b}$ ，它包含了1000位著名华人的三十万条语音， VoxCeleb 则是知名 $\mathcal{A}$ 一的英语语音数据集，其他较知名的数据集如下。

(1)TIMIT thtts:/catalog.ldc.upenn.edu/LDC93S1）：美式英语数据集。

$$
\mathrm{( W o r d \ E r r o r \ R a t e, W E R ) \it~ a l j \equiv~ \hbar~ \# ~ \hbar~ \neq~ \circ~}
$$

## 1）台湾大学李琳山教授

ntp:ispechn
$$
\mathrm{e e. n t u. e d u. t w / D S P 2 0 1 9 S p r i n g} / \mathrm{\Delta_{\circ}}
$$

$$
( 2 ) \enskip\exists\linebreak\mathrm{e l t h} \amalg\rangle\n=\mathrm{E E C S} \propto6 8 7 0
$$

 $( 3 )$ 爱丁堡大学AUTOMATIC SPEECH RECOGNITION
$$
\mathrm{( h t t p s : / / W W W}
$$
inf.ed.ac.uk/teaching/courses/asr/lectures-
2019.html)

(2）LibriSpeech：电子书的英语朗读数据集，可在 OpenSLR下载。

(3）维基百科语音数据集
(https://en.wikipedia.org/wiki/Wikipedia: WikiProject Spoken Wikipedia)

 $( 3 )$ 维基百科语音数据集

(htts:/en.wikipedia.orgwkiWikipedia:

WikiProject Spoken Wikipedia O 语音除了可以拿来进行语音识别之外，还有许多其他方面的应用，举例如下。

(1）声纹辨识：从讲话的声音分辨是否为特定 $\mathcal{A}$ ，属于生物识别技术（Biometrics）的一种，可应用在登录（Sign in）、犯罪侦测、智能家庭等领域。

(2）声纹建模：模拟或创造特定人的声音唱歌或讲话，如 Siri.

(3) 未来也许能够使用语音搜寻，类似现在的程似性比较:
文字、图像搜导。

各位读者看到这里,应该能深刻了解到，文字、影像、语言辨识是整个人工智能应用的三大基石，不论是自动驾驶车、机器人、 ChatBot、甚至是医疗诊断通通都是建构在这些基础技术之上的。 如同前文提到，现今的第三波AI 浪潮之所以不会像前两波一样后继无力，不了了之，有一部分原因是要归功于这些基本技术的开 $\overrightarrow{\mathcal{K}}$ ，这就像盖房子的地基，技术的研发与应用实践并进，才不会空有理论，最后成为空中楼阁。相信在未来这三大基石还会有更进一步的发展，而到时候又会有新的技能被解放，我们能透过 $\mathbf{A} \mathbf{I}$ 完成的任务也更多 $\overline{{\jmath}}$ ，换言之，我们学习的脚步永远不会有停上的一天，笔者与大家共勉之。

## 14-6总结

(4）音乐方面的应用：比如曲风辨识、模拟歌手的声音唱歌、编曲/混音等

强化学习（Reinforcement Learning, RL）相关的研究少说也有数一年的历史 $\overline{{\jmath}}$ ，但与另外两类机器学习相比，并不受瞩曰，直到2016年以强化学习为理论基础的 AphaGo，先后击败了世界围棋冠军李世艺、柯洁等 $\mathcal{A}$ 才开始备受世界瞩，强化学习才因此一炮而红，学者专家纷纷投入研发，接下来我们就来探究其原理与应用。机器学习分类如下图所示。

## 第五篇强化学习

![](figures/959-2-FIGURE.jpg)

图机器学习分类

强化学习是指机器与环境的互动过程中，人类不必直接提供解决方案，而是通过计算机不断地尝试于错误中学习，称为试误法 (Trial and Error），自我学习一段时间后，计算机就可以找到最佳的行动策略。打个比方，就像是训练狗接飞盘一样，人们不会教狗如何接飞盘，而是口主人不断地抛出飞盘让狗练习，如果它成功接到飞盘，就给予食物奖励，反之就不给奖励，经过反复练习 $\sqrt{\Pi}$ ，方可完成，如图15.1所示。因此，强化学习并不是单一阶段（One $\mathrm{S t e p} )$ 的算法，而是多阶段，反复求解，类似于梯度下降法的求解过程。

根据维基百科的概述，强化学习涉及的学术领域相当多，包括博弈论（Game Theory 自动控制、作业研究、信息论、仿真优化、群体智慧（Swarm lntelligence 统计学以及遗传算法等，同时它的应用领域也是非常广泛，举例如下。

## 第15章强化学习

## 第15章

## 强化学习

![](figures/960-5-FIGURE.jpg)

图15.1 训练狗接飞盘

(1）下棋、电玩游戏策略（Game Playing O

甚至于残酷的战争，只要应用领域是能在模拟环境下尝试与错误，并且需要人工智能提供行动的决策辅助，都是强化学习可以发挥的领域。

(2）制造/医疗/服务机器人的控制策略（Robotic Motor Control)

(3) 一
$$
\mathbb{B} \mathbb{B} \mathbb{B} \mathbb{B} \setminus\mathrm{( A d-p l a c e m e n t \, O p t i m i z a t i o n )} \setminus\mathbb{O}
$$
广告投放策

(4）金融投资交易策略（Stock Market Trading Strategies

(5）运输路线的规划（Transporation Routing) O

(6）库存管理策略（Inventory Management) 生产排程 (Production Schedluling

强化学习的理论基础为马尔可夫决策过程（Markov Decision Processes, MDP），主要是指所有的行动决策都会基于当时所处的状态及行动后会带来的奖励，而状态与奖励是由环境所决定的， 因此就形成图15.2所示的示意图。

(1）代理人行动后，环境会依据行动更新状态，并给予奖励。

(2）代理人观察所处的状态及之前的行动，决定下次的行动。

(1）代理人或称智能体 $( \mathrm{A g e n t} )$ ：也就是实际行动的主人翁，比如游戏中的玩家（Player）、下棋者、机器 $\mathcal{A}$ 、金融投资者、接飞盘的狗等，他主要的任务是与环境互动，并根据当时的状态与之前得到的奖惩，来决定 $\mathrm{F-}$ 步的行动。代理人可能 $\pi\pi-$ 个，如果有多 $\uparrow$ ，状况就会复杂许多，这称为多代理人（Multi-
$$
\mathrm{A g e n t} )
$$

## 15-1 强化学习的基础

![](figures/962-5-FIGURE.jpg)

图15.2马尔可夫决策过程的示意图

各个专有名词的定义如下。

(2）环境（Environment) 根据代理人的行动（Action) 给予立即的奖励或惩罚，一律称为奖励，它也会决定代理人所处的状态。

(3）状态：指代理人所处的状态，如围棋的棋局、游戏中玩家/敌 $\mathcal{A} /$ 宝物的位置、能力和金额。有时候代理人只能观察到局部的状态，如扑克牌游戏 $2 1$ 点（Black Jack），庄家有一张牌盖牌玩家是看不到的，所以，状态也被称为观察（Observation)

(4）行动 $\mathrm{( A c t i o n )}$ ：代理人依据环境所提示的状态与奖励而做出的决策。

整个过程就是代理人与环境互动的过程，可以以行动轨迹
$$
( \mathrm{T r a j e c t o r y} )
$$
来表示：{S $A_{0}, R_{1}, S_{1}, A_{1}, R_{2}, S_{2},..., S_{p} A_{p} R$ Rn,Sp
$$
A_{t+1}, \, R_{t+2}, \, S_{t+2} \}
$$

行动轨迹：行动 $( A_{0} )$  $\varGamma$ ，代理 $\mathcal{A}$ 会得到奖励 $( R_{1} ) \mathrm{\ subscript {,}}$ 状态 (S），之后再采取下一步的行动（A, 不停循环 $( A_{t} R_{t+1},$ 2 $\mathrm{S}_{t+1} \ldots)$ 直至终点或中途比赛失败/胜利为 $\amalg$ 。

马尔可夫决策过程（以下简称MDP）的假设是代理人的行动决策是「累计获得的奖励最大化」，也称为「报酬」（Return）。 注意，报酬并不是指每一种状态下的最大奖励，比方说，下棋时我们会为了诱敌进入陷阱，而故意牺牲某些棋子，以求得最后的胜利。同样的道理，MDP是追求长期的最大报酬，而非每一步骤的最大奖励（短期利益）。强化学习类似于优化求解，目标函数是报酬，希望找到报酬最大化时应采取的行动策略 $( \mathrm{P o l i c y} )$ ，对比于其中： $\boldsymbol{S}$ : O $\boldsymbol{A}$ ：行动。 $\boldsymbol{R}$ ：奖励。 $\boldsymbol{S}_{\mathrm{t}}$ ：t是时间点
状态。

神经网络求解，神经网络目标是最小化损失函数，对各神经元的权重求解。

MDP 是由马尔可夫奖励过程（Markov Reward Process
MRP）加上行动转移矩阵（Action Transition matrix）所组成，接着我们依序讲解这两个概念。

在说明MR ${\bf P}$ 之前，先从MRP 的基础开始讲起，马尔可夫过程（Markov Process, $\mathrm{M P )}$ ，也称为马尔可夫链（Markov
Chain），主要内容为描述状态之间的转换。例如，假设天气的变化状态有两种，晴天和雨天，一个典型的马尔可夫链的状态转换图如图15.3所示。

也可以用表15.1来表示，称为状态转换矩阵（State Transiion matrix

![](figures/964-4-FIGURE.jpg)

图15.3：马尔可夫链的示意图

表 $1 5. 1$ 状态转换矩阵

|  | 晴天 | 雨天 |
| --- | --- | --- |
| 晴天 | 0.8 | 0.2 |
| 雨天 | 0.1 | 0.9 |

上面图表要表达的信息如下也就是说，明日天气会受今日天气的影响，换言之，下一个状态出现的概率会受到日前状态的影响，符合这种特性的模型就称为具有「马尔可夫性质 (Markov Property），即目前状态 $( S_{t} )$  $\P$ 受前一个状态 $( S_{t 1} )$ 影响，与之前的状态 $( S_{t 2},$  $S_{t_{3} \cdots} )$ 无关，也可以扩展受 $\boldsymbol{n}$ 个状态（S.. $S_{t \2},$  $S_{t_{3},..., S_{t-n}} )$ 影响，类似于时间序列。

因此，我们可从上面图表推测出n天后出现晴天或雨天的概 ※，例如

(1）今天是晴天，后天是晴天的概
$$
\overline{{k}}^{\overline{{\mathbf{z}}}}=0. 8 \times0. 8+0. 2 \times0. 1=
$$
0.66.

(2）今天是雨天，后天是晴天的林
$$
\nabla\overline{{\#}}=0. 1 \times0. 8+0. 9 \times0. 1=
$$
0.17.

再扩 $\not\Xi-\mathrm{T}$ ，马尔可夫链加上奖励，就称为马尔可夫奖励过程 (Markov Reward Process, MRP) 即 Markov Process +
Reward - Markov Reward Process，如图15.4所示

(1）今天是晴天，明天也是晴天的概率：0 $8$ . (2）今天是晴天，明天是雨天的概率： $0. 2_{\mathrm{c}}$ 
(3）今天是雨天，明天是晴天的概率：0.1.
(4）今天是雨天，明天也是雨天的概率：0.9.

1）今天是晴天，明天也是晴天的概率：0.8

 $( 2 )$ 今天是晴天，明天是雨天的概率：0.2

 $( 3 )$ 今天是雨天，明天是晴天的概率：0.1

(4）今天是雨天，明天也是雨天的概率：0 $9$ 

图 15.4是学生作息的状态转换，状态包括聊天（Chat） 喝咖啡（Coffee) 玩计算机（Computer）和在家（Home 我们依据转换概率和奖励，可算出每个状态的期望值，来表达每个状态的价值，如

理解 MRP 后，那么行动转移矩阵（Action Transition matrix) 又是什么呢？举例来说，走迷宫时，玩家往上/下/左/右走的概率可能不相等，又如玩剪刀石头布时，每个人的猜拳偏好都不尽相同， 假设第 $\ --$ 次双方平手，第二次出手可能就会参考第一次的结果来出拳，因此出剪刀/石头/布的概率又会有所改变，这就是行动转移矩阵，它在每个状态的转移矩阵值可能都不一样，如图15.5所示。

![](figures/966-2-FIGURE.jpg)

15.4，马尔可夫奖励过程冬

$$
\begin{array} {l} {{{\bf V_{\mathrm{\scriptsize~ ( o b a t )}}=-1^{\star} 0. 5+2^{\star} 0. 3+1^{\star} 0. 2=0. 3}}} \\ {{{\bf V_{\mathrm{\scriptsize~ ( b o m e )}}=2^{\star} 0. 7+1^{\star} 0. 1+3^{\star} 0. 2=2. 1}}} \\ {{{\bf V_{\mathrm{\scriptsize~ ( b o m e )}}=1^{\star} 0. 6+1^{\star} 0. 4=1. 0}}} \\ {{{\bf V_{\mathrm{\scriptsize~ ( b o m p u l e f )}}=5^{\star} 0. 5+\begin{array} {l} {{(-3 ) \quad^{\star} 0. 1+1^{\star} 0. 2+2^{\star} 0. 2=2. 8}} \end{array}}}} \end{array}
$$

我们可将行动转移矩阵理解为策略，若策略是固定的常数，则 MDP 就等于MRP，但是，通常我们面临的环境是多变的，策略不会一成不变，因此总而言之，强化学习的目标就是在MDP的机制 $\mathsf{T}$ ，要找出最佳的行动策略，而目的是希望获得最大的报酬。

![](figures/967-1-FIGURE.jpg)

图15.5、马尔可夫决策过程

接下来用数学式建立强化学习模型，将马尔可夫决策过程的示意图转为数学符号（见图15.6)

依据马尔可夫性质的假设， $S_{t+1}$ 只与前一个状态 $( S_{t} )$ 有关， $\pm$ 式简化为

(2）报酬：就以走迷宫为例，到达终点时，所累积的奖励总和称为报酬， 京 $\left( T \right)$ 的累积奖励。即
下式为从时间点走到终点

## 15-2 强化学习模型

![](figures/968-4-FIGURE.jpg)

图15.6强化学习模型

行动轨迹（trajectory）为

$$
\{S_{0}, A_{0}, R_{1}, S_{1}, A_{1}, R_{2}, S_{2} \dots S_{p} \, A_{p} R_{t+1}, S_{t+1}, A_{t+1}, R_{t+2}, S_{t+2}... \}
$$

(1）状态转移概率：达到状态 St+1的概率为

$$
p \, \left( S_{t+1} \right| S_{p} \, A_{p} S_{t 1}, A_{t 1}, S_{t 2}, A_{t 2}, S_{t 3}, A_{t 3}... )
$$

$$
p \ ( S_{t+1} | \ S_{t} \ A_{t} )
$$

$$
G_{t}=R_{t+1}+R_{t+2}+R_{t+3}+\cdots+R_{T} \,=\, \sum_{k=1}^{T} \, R_{t+k}
$$

例 $1$ ：以图15.8的走迷宫为例，目标是以最短路径到达终点， 故设定每走一步奖励为 $- 1$ ，即可算出每一个位置的报酬，计算方法是由终点倒推回起点，结果如图15.7中的数字所示。

（3）折扣报酬（Discount Return）：模型目标是追求报酬最大化，若迷宫很大的话要考虑的奖励 $( R_{\mathrm{i}} )$ 个数也会很多，所以为了简化模型，将每个时间的奖励乘以一个小于1的折扣因子
 $( \gamma)$ ，让越久远的奖励越不重要，避免要考虑太多的状态，类似复利的概念，报酬公式修正为

(4）状态值函数（State Value Function）：以图15.8所示的迷宫为例，玩家所在的位置就是状态，所以，图中的数字（报酬） 即是状态值，代表每个状态的价值。又加上要考虑从起点走到终点

![](figures/969-3-FIGURE.jpg)

冬 $1 5. 7$ 计算迷宫每一个位置的报酬

$$
\begin{array} {c} {{G_{i}=R_{i+1}+\gamma R_{i+2}+\gamma^{2} \, R_{i+3}+\dots+\gamma^{i+1} \, R_{i}=\displaystyle\sum_{i=1}^{\gamma} \gamma^{k+1} \, R_{i+1}}} \\ {{{}}} \\ {{\displaystyle\int_{\cal M} G_{t+1}=\boldsymbol{R}_{t+2}+\gamma\boldsymbol{R}_{t+3}+\gamma^{2} \, \boldsymbol{R}_{t+4}+\dots+\gamma^{k-2} \, \boldsymbol{R}_{T}}} \\ {{{}}} \\ {{\Delta_{\bar{K}} G_{t}=\boldsymbol{R}_{t+1}+\gamma\boldsymbol{G}_{t+1}}} \end{array}
$$

$$
G_{t}=R_{t+1}+\gamma R_{t+2}+\gamma^{2} \; R_{t+3}+\dots+\gamma^{T-1} \; {\bf R}_{T}=\sum_{k=1}^{T} \; \gamma^{k-1} \; R_{\; t+k}
$$

$$
\overline{{\chi}} \, G_{t+1}=R_{t+2}+\gamma R_{t+3}+\gamma^{2} \, R_{t+4}+\ldots+\gamma^{t-2} \, R_{T}
$$

的路径可能不 $\mathrm{l k-}$ 种，故状态值函数是每条路径报酬的期望值（平均数）

例 $2$ ：2 ，规则如下，起点为 $( 1, 1 )$ 终
举另一个迷宫游戏为例，
点为 $( 4, 3 )$ 或 $( 4, 2 )$ 走到 $( 4, 3 )$ 奖励为 $1$ ，走到 $( 4, 2 )$ 奖励为 $- 1$ ，每定一步奖励均为-0.04。

$$
( 1, 1 )
$$
状态期望值 $\mathfrak{l}=~ ( 0. 7 2+0. 7 2+~ (-1. 1 6 )$ ))/3=0.28/3=
0.09

![](figures/970-3-FIGURE.jpg)

图15.8另 $\underline{{\quad}}$ 个迷宫游戏

假设有三种走 $\grave{\lambda} \pm$ :

$$
\begin{array} {l l} {{\bigotimes}} & {{( 1, 1 ) \rightarrow( 1, 2 ) \rightarrow( 1, 3 ) \rightarrow( 1, 3 ) \rightarrow( 2, 3 ) \rightarrow( 3, 3 ) \rightarrow( 4, 3 )}} \\ {{}} & {{\vdots}} \\ {{\bigoplus}} & {{-0. 0 4 \rightarrow-0. 0 4 \rightarrow0. 0 4 \rightarrow-0. 0 4 \rightarrow( 1, 3 ) \rightarrow( 1, 3 ) \rightarrow( 1, 3 ) \rightarrow( 1, 3 ) \rightarrow( 3, 3 ) \rightarrow( 4, 3 )}} \\ {{}} & {{}} \\ {{\bigotimes}} & {{( 1, 1 ) \rightarrow( 1, 3 ) \rightarrow( 1, 3 ) \rightarrow( 2, 3 ) \rightarrow( 3, 3 )}} \\ {{}} & {{0. 0 4 \rightarrow( 1, 0 4 \rightarrow0. 0 4 \rightarrow( 3, 3 ) \rightarrow( 4, 3 )}} \\ {{}} & {{}} \\ {{\bigotimes}} & {{( 1, 1 ) \rightarrow( 2, 1 ) \rightarrow( 3, 1 ) \rightarrow( 3, 2 ) \rightarrow( 4, 2 )}} \\ {{\bigotimes}} & {{0. 0 4 \rightarrow( 2, 1 ) \rightarrow( 3, 1 ) \rightarrow( 3, 2 ) \rightarrow( 4, 2 )}} \\ {{}} & {{\vdots}} \\ {{\bigoplus}} & {{0. 0 4 \rightarrow( 0. 0 4 \rightarrow( 3, 0 4 \rightarrow( 3, 0, 0 4 \rightarrow-1 )}} \end{array}
$$

三条走法的报酬计算如下：

$$
\begin{array} {l} {\bigoplus1-0. 0 4 \times7=0. 7 2} \\ {\bigotimes1-0. 0 4 \times7=0. 7 2} \\ {\sum} \\ \end{array}
$$

(5） Bellman 方程式：特定策略下采取各种行动的概率，状态值函数的公式为

 $P_{s s^{\prime}}^{a}$ 为行动转移概率，即在状态s采取行动a，会达到状态s『的概率。

Bellman 方程式让我们可以从下一状态的奖励/状态值函数推算出目前状态的值函数。那么在口前状态下怎么会知道下一个状态的值函数呢 $\ref{f i g : c c}$ 不要忘 $\overline{{\jmath}}$ ，强化学习是以尝试错误（Trial and Error）的方式进行训练，我们可以从之前的训练结果推算目前回合的值函数。例如，要算第50回合的值函数，可以从1-49回合推算每一状态的期望值。

(6）行动值函数（Action Value Function 采取某一行动的值函数,类似状态值函数 $\big< \lambda$ 式为

状态值函数以 $V_{\pi}$ （s）表示，其中 $\pi$ 为特定策略。有

$$
V_{\pi} \ \ ( \mathbf{s} ) \ =E \ \left( G | \mathit{S=s} \right)
$$

$$
v_{\pi} ( s )=\sum_{a} \pi( a | s ) \: \sum_{s^{\prime}} {\cal P}_{s s^{\prime}}^{a} \: [ {\cal R}_{s s^{\prime}}^{a}+\gamma v_{\pi} ( s^{\prime} ) ]
$$

$$
\mp\mp:
$$

$$
s^{\prime} \hbar\mathrm{T}-\i\hbar\Phi;
$$

 $\pi$ (as）为采取特定策略时，在状态s采取行动a的概率

后面门的公式系依据 $G_{t}=R_{t+1}+\gamma G_{t+1}$ 转换而来

$$
q_{\pi} \big( s, a \big)=\sum_{s^{\prime}} {\cal P}_{s s^{\prime}}^{a} \ [ {\cal R}_{s s^{\prime}}^{a}+\gamma\sum_{a^{\prime}} \pi( a^{\prime} | s^{\prime} ) \ q_{\pi} ( s^{\prime}, a^{\prime} ) ]
$$

同样可以从下一状态的奖励/行动值函数推算出目前行动值函数。

 $( 7 )$ 状态值函数与行动值函数的关系可以由倒推图（Backup Diagram）来表 $\overline{{\Pi}}$ ，如图15.9和图15.10所示。

依照上述公式行动，不断更新所有状态值函数，之后在每一个状态 $\mathrm{F}$ ，以最大化状态或行动值函数为准则，采取行动，以获取最大报酬。

![](figures/972-3-FIGURE.jpg)

图15.9状态值函数与行动值函数的关系1

![](figures/972-5-FIGURE.jpg)

15.10 状态值函数与行动值函数的关系 $2$ 冬

从倒推图可以看出行动值函数的公式来源。 这一节我们把前面刚学到的理论整理 $- \mathrm{T}$ ，就能完成一个初阶的程序。回顾前面谈到的强化学习机制如图15.11所示。

采取面向对象设计 $( \mathrm{O O P} )$ 程序架构大致分为三个步骤，共有两个类别，介绍如下

(1）环境（Environment) 比如迷宫、游戏或围棋，它会给予玩家奖励并负责状态转换，若是单人游戏，环境还要担任玩家的对手，像是计算机围棋。职责（方法）列举如 $\mathrm{F}$ 。

## 15-3 简单的强化学习架构

![](figures/973-4-FIGURE.jpg)

 $1 5. 1 1$ 强化学习机制图

白强化学习机制，我们可以制定程序架构如图15.12所示

![](figures/973-7-FIGURE.jpg)

图15.12强化学习程序架构

①初始化（Initialization) 需定义状态空间（State Space 奖励办法、行动空间（Action Space 状态转换 (State Transition Definition)

2重置（Reset, 每一回合（Episode）结束时，需重新开始，重置所有变量

3步骤：代理人行动后，驱动行动轨迹的 $\mathrm{F-\#}$ ，而环境就会随之更新状态、给予奖励，并判断回合是否结束。

0行动：代理人依据既定的策略与目前所处的状态，采取行动,如上、下、左、右

2通常如果要订制特殊的策略，会使用继承代理人类别
(Agent Class 在衍生的类别中，构思写行动函数，并撰写策略逻辑。

范例1.建立简单的迷宫游戏：共有5个位置，玩家一开始站中
，每走一步扣分 $\mathbf{0. 2}$ ，走到左端点得-1分，走到右端点得1 间位置,
分，走到左右端点该回合即结束。

④渲染（Render）：更新显示的画面。

(2）代理人 $( \mathrm{A g e n t} )$ ：即玩家，职责（方法）列举如下

(3）进行实验时，先建立两个类别对象，触发环境各项方 $\grave{\imath} \pm$ ，等待玩家行动。

接下来，我们就依照上述架构来开发程序

![](figures/974-9-FIGURE.jpg)

请参阅程序：15 01 simple game.py

(1）加载相关库。程序代码如下 get observation：返回状态空间，本游戏假设有五种： $1$ 、 2、3、4、5。

get actions：返回行动空间，本游戏假设有两种： $- 1, ~ 1,$ . 只能往左或往右

step：触发 $\mathrm{F}-\pm$ ，根据传入的行动，更新状态，并计算奖励

 $( 2 )$ 建立环境类别。

initu：初始化，每回合结京后比赛重置

is done：判断比赛回台是否结束。

$$
\star\equiv\lfloor\pm\lfloor\exists\pm\rfloor\rceil\rceil
$$

(3）建立代理人类别：主要是制定行动策略，本范例采取随机策略。程序代码如下：

(4）定义好环境与代理人功能后，就可以进行实验了。程序代码如下：

| 4 | #环境类 | 餐别 |  |  |
| --- | --- | --- | --- | --- |
| 5 | class En | nvironment: |  |  |
| 6 | def | init (self): | # | 初始亿 |
| 7 |  | self.poistion= 3 | # | 玩家一开始站中间位置 |
| 8 |  |  |  |  |
| 9 | def | get observation(self): |  |  |
| 10 |  | #状态空间（State Space) | ）， | 有个位置 |
| 11 return [i for i in range(l, 6)] |
| 12 |
| 13 | def | get actions(self): |  |  |
| 14 return [-1, 1] #行动空/间(Action Space) |
| 15 |  |  |  |  |
| 16 | def | is done(self): |  | 判断比赛回台是古结兔 |
| 17 |  | #是否走到左石端点  |  |  |
| 1O AA | 欢上 | CLIHOcL•PL5LLI- 上77  | 一士 | OEL•L5LLI-2 |
| 20 64 | 开么 C | V燕 105 |  |  |
| Ll 90 | aeT | sLepselTy dcLlon: 共日不同公口生市  |  |  |
| ← 23 |  | N if celf.is done(). |  |  |
| 一一—5 24 raise Fxcention("Game ic over"Y |
| 25  |
| 26 |  | self.poistion t= actior | on |  |
| 27 if self.poistion == 1: |
| 28 |  | reward= -1  |  |  |
| 29 |  | elif self.poistion == | 5: |  |
| 30 |  | reward=1 |  |  |
| 31 |  | else: |  |  |
| 32 |  | reward = -0.2 |  |  |
| 33 |
| 34 |  | return action, reward |  |  |

| 37 | #代理人类别 |
| --- | --- |
| 38 | class Agent: |
| 39 | #初始亿 |
| 40 | def init （self): |
| 41 | pass |
| 42 |
| 43 | def action(self, env): |
| 44 | #取得状态 |
| 45 | current obs = env.get observation() |
| 46 | #随机行动 |
| 47 | return random.choice(env.get actions()) |

②执行结果：累计报酬： $- 1. 2$ ，白于采随机策略，因此，每次结果均不相同

| 50 | if name ==" main ". |
| --- | --- |
| 51 | #建立实验，含环境、代理人对象 |
| 52 | env = Environment() |
| 53 | agent = Agent() |
| 54 |  |
| 55 | #进行实验 |
| 56 | total reward=o # 累计报酬 |
| 57 | while not env.is done(): |
| 58 | #买取行动 |
| 59 | action s agent.action(env) |
| 60 |  |
| 61 | #更新下一步 |
| 62 | state, reward = env.step(action) |
| 63 |  |
| 64 | #计算累计报酬 |
| 65 | total reward t= reward |
| 66 |  |
| 67 | #显示累计报酬 |
| 68 | print(f"累计报酬:total reward:.4f}") |

)执行：py
$$
\operatorname{h o n} \mathrm{R L} 1 5 \_0 1 \_\mathrm{s i m p l e \_g a m e. p y_{o}}
$$

范例2.调用15 01 simple game.py，执行10 回合

请参阅程序：15 02 simple game test.py

(1）加载相关库。程序代码如下

建立实验，包含环境、代理人对象。程序代码如下： (2)

 $\begin{array} {c} {4} \\ {5} \\ {6} \\ \end{array}$ #建立实验，含环境、代理人对象 env = Environment(）
agent = Agent()

(3）进行实验。程序代码如下： 执行结果：可以看到每次的结果均不相同，这就是程序在尝试错误的证明。结果如下

范例3.以状态值函数最大者为行动依据，执行10回合，并将程序改成较有弹性，允许更多的节点。

![](figures/978-2-FIGURE.jpg)

| 累计报酬:0.8000 |  |
| 素 1 计报酬：-1.2000 |  |
| 电计报酬:0.4000 素  |  |
| 聚欢计报酬：-2.8000 |  |
| 聚长橙梨计千报酬:-1.2000 |  |
| 聚计报酬：-1.2000 |  |
| 蒙计报酬:-3.2000 |  |
| 累计报酬:0.8000 |
| 素晨计报酬:-1.6000 |  |
| 累计报酬:-1.2000 |  |

请参阅程序：

15 03 simple game with state value.ipynb.

(1）加载相关库。程序代码如下

| 1 | 相关库 |
| 2 | numpy as np |
| 3 | random |

## (2）参数设定。程序代码如下

get observation：取得状态值函数的期望值，以状态值函数最大者为行动依据。程序代码如下

(3）建立环境类别：增加以下函数

update state value：由终点倒推，更新状态值函数

(4）代理人类别：比较左右相邻的节点，以状态值函数最大者为行动方向，如果两个状态值一样大，就随机选择一个。程序代

| 1 2 3 4 5 6 7 8 9 10 11 12 | #外境类别 class Environment(): #初始亿 def init (self): #存储状态值函数，索引值101:不用，从1开始 self.state value = np.full((NODE COUNT+1), 0.0) self.state value[1]=-1 self.state value[NODE COUNT]=1 #更新久数，索引值[0]:不用，从1开始 self.state value count = np.full((NODE COUNT+1),0) self.state value count[1]=1  |
| 9 | 一 |
| 13 | self.state value countINODF COUNT1=1 |
| 上 1A | oc-rpcEclrEscL soJ- |
| 一 15 | #初始化 |
| 16 | def reset(self): |
| L 10 | 北日车易粉汤 selt.poistion= inc((I+NODECOUN)/2）动家才妇助千问证具 |
| 19 |  |
| 一 20 | def pet states(self): |
| 21 | #状态空间（State Space），共有5个位置 |
| 22 | return [i for i in range(1, 6)] |
| 23 |  |
| 4T 25 | cBscs-piD-/. returnT-111进行动空间（4rtionChace） |
| 25 A | return l-1, 1] # 7动E/(Action Space) |
| 26 07 | E1115.进当人塞同公旦不公吉 |
| 一 28 | 客汽6一公 #是否法到东方满点  |
| 29 | if self.poistion == l or self.poistion == NODE COUNT: |
| 30 | self.trajectory.append(self.poistion) |
| 31 | return True |
| 32 A | else: |
| 33 | return False |
| 34 |  |
| 2T 25 | 优步强 |
| 35 BC | 开少球 C11016 |
| 36 | def step(self, action): |
| 37 | #是否回台已结荣 |
| 38 | if self.is done(): |
| 39 | #不应该有机会执行到这里 |
| 40 AA | raise Exception("Game is over"y |
| 42 A2 | selt.trajectory.append(selt.poistion 15  |
| 43 | self.poistion t= action 电O0O |
| 44 | if self.poistion == 1: |
| 45 | reward= -1 |
| T 46 | elif self.poistion =- NODE COUNT: |
| 46 A7 | elit selt.polstion == NODE COUNI: |
| 48 | else: |
| 49 | 一一 reward = NORMAL REWARD |
| 50 |  |
| 20 51 | return self.noiction. reward |
| Dl 0 | return selt.poistion, reward |
| 52 |  |
| 54 | #倒推，更新状态值函数 |
| 55 | for i in range(len(self.trajectory)-1, -1, -1): |
| 56 | final value += NORMAL REWARD |
| 57 | self.state value[self.trajectory[i]] t= final value |
| 58 | self.state value count[self.trajectory[il] t= 1 |
| 59 |  |
| 60 | #取得状态值函数期望值 |
| 61 | def get observation(self): |
| 62 | meanl = self.state value / self.state value count |
| 63 | return meani |

执行结果：可以看出每次的结果均不相同，基本上训练了两次之 $\sqrt{\Xi}$ ，接下来的每一次都会往右走，这表示已经找到最佳策略，就是一直往右走。结果如 $\mathrm{F}$ :

## 码如下：

![](figures/981-2-FIGURE.jpg)

）建立实验，包含环境、代理人对象。程序代码如下： (5)

![](figures/981-4-FIGURE.jpg)

个执行结果：如图15.13所示，可以看出训练两次过后，即可找到最佳策略，就是一直往右走

②把节点数（NODE COUNT）参数放大为 $1 1$ ，执行结果： 如图15.14所示，可以看出训练几次后，就找到最佳策略，也是一直往右走。

![](figures/982-2-FIGURE.jpg)

## (6 。程序代码如下： 绘图。

![](figures/982-4-FIGURE.jpg)

![](figures/982-5-FIGURE.jpg)

图15.13 训练

结论：以状态值函数最大者为行动依据，训练模型，果然可以找到最佳解，模型就是每个状态的值函数，之后实际上线时即可加载模型执行。

![](figures/983-1-FIGURE.jpg)

图15.14修改参数原训练

根据前面的练习，可以了解到强化学习是在各种环境中寻找最佳策略，因此，为了节省开发者的时间，网络上有许多库设计了名式各样的环境，供大家实验，也能借由动画来展示训练过程，如 G $\mathrm{y m}$ 、Amazon SageMaker等库。

以下就来介绍 Gym库的用法，它是OpenAl开发的学习库提供了数十种不同的游戏，Gym 官网风（ttps://gym.openai.com/ 首页上展示了一些游戏画面。不过请读者留意，有些游戏在 Windows操作环境下并不能顺利安装，因为Gyn是以gco撰写的。

假如要修改Gym的程序代码，可直接从GitHub 下载源代码后，再自行安装

如需安装全部游戏，可执行下列指令。注意：这些指令只能在 inu×环境下执行，而且必须先安装相关软件工具，请参考Gym

## 15-4 Gym 库

安装指令如下：

$$
\mathrm{p i p ~ i n s t a l l ~ g y m}
$$

$$
\mathrm{g i t ~ c l o n e ~ h t t p s : / / \mathrm{g i t h u b. c o m / o p e n a i / g y m}}
$$

$$
\mathrm{c d \ g y m}
$$

$$
\mathrm{p i p ~ i n s t a l l ~-e ~.}
$$

在Windows操作环境下仅能安装Atari游戏，Atari为1967年开发的游戏机（见图15.15) 拥有 $\mathit{\Pi}$ 一种游戏，如打砖块
(Breakout) 桌球 $( \mathrm{P o n g} )$ 等：

(1）经典游戏（Classic Contral）和文字游戏（Toy Tex 属于小型的环境，适合初学者开发测试

(2）算法类（Agorithmic）：像是多位数的加法、反转顺序等，这对计算机来说非常简单，但使用强化学习方式求解是一大挑战。

(4）2D and 3D机器人（Robot, 机器人模拟环境，有些是要付费的，可免费试用 $3 0$ 天。但是在 Windows 操作环境安装会有 GitHtub 说明月（ttps:/gihub.com/openailgyminstalling-everything)

$$
\mathrm{p i p ~ i n s t a l l-e^{\prime}. [ a l l ]^{\prime}}
$$

$$
\mathrm{p i p ~ i n s t a l l ~-e^{\prime}. [ a t a r i ]^{\prime} ~}
$$

![](figures/985-7-FIGURE.jpg)

图 $1 5. 1 5$ Atari游戏机

Gym提供的环境分为以下四类

(3）Atari: Atari游戏机内的一些游戏问题，好在网络上有些文章提到了解决方案，需要的读者可以 Google搜索或参考「Install OpenAl Gym with Box2D and Mujoco in Windows 10」 2

接下来先认 $\i\pi-\mp\mathrm{G y m}$ 的架构，各位可能会觉得有点眼熟这是由于前面的范例刻意模仿Gym 的设计架构，因此两者的环境类别有些类似。

(1）reset)：比赛一开始或回合结束时，调用此方法重置环境。

(2) step (action) 传入行动，触动 $\mathrm{F-F}$ ，返回下列信息。

 $\bullet$ done：布尔值，True 表示比赛回合结束，False 表示比赛回合进行中。

Gym的环境类别包括以下方法

observation：环境更新后的状态

reward：行动后得到的奖励

into：为字典的数据类型，通常是除错信息。

(3） render()：渲染，即显示更新后的画面

 $( 4 ) \mathrm{\ c l o s e ( ) :}$ 关闭环境

$$
\mathfrak{F E} [ \mathfrak{H} ] \ 1. \mathtt{G y m} \lambda i \brack\mathfrak{H}_{\circ}
$$

请参阅程序：15 04 Gym.ipynb.

执行结果：有数百种的游戏，种类依每台机器安装的情形而有所不同。结果如 $\mathrm{T}$ :

(3）任意加载一个环境，如木棒小车（GartPole）游戏，并显示行动空间、状态空间/最大值/最小值。程序代码如下：

执行结果：行动空间有两个离散值（0为往左， $1$ 为往右)
状态空间为Box数据类型，表示连续型变量，维度大小为 $4$ 、分别代表小车位置（Cart Position）、小车速度（Cart Velocity）、木棒角度（Pole Angle）及木棒速度（Pole Velocity At Tio），另外显示四项信息的最大值和最小值。结果如下：

1）加载相关库。程序代码如下：

(2）显示已注册的游戏环境。程序代码如下：

![](figures/987-5-FIGURE.jpg)

['Copy-v0', 'RepeatCopy-vo', 'ReversedAddition-vo', 'ReversedAddition3-v0', 'DuplicatedInput-vo', 'Reverse-vo', 'CartPole-v O', "CartPole-v1', "MountainCar-vo', "MountainCarContinuous-vo', "Pendulum-vo', 'Acrobot-v1', "LunarLander-v2', "LunarLanderc ontinuous-v2', 'Bipedalwalker-v3', 'BipedalwalkerHardcore-v3', 'CarRacing-vo', 'Blackjack-vo", 'KellyCoinflip-vo', "KellyCoin flipGeneralized-vo', 'FrozenLake-vo', 'FrozenLake8x8-v0', 'CliffWalking-vo', 'NChain-v0', 'Roulette-v0', 'Taxi-v3', 'Guessing Game-vo', 'HotterColder-vo', "Reacher-v2', 'Pusher-v2', 'Thrower-v2', 'Striker-v2", "InvertedPendulum-v2', "InvertedDoublePen dulum-v2', "HalfCheetah-v2', 'HalfCheetah-v3', "Hopper-v2', "Hopper-v3', 'Swimmer-v2', 'Swimmer-v3', "Walker2d-v2', "Walker2d -v3', "Ant-v2', "Ant-v3', 'Humanoid-v2',  Humanoid-v3', 'HumanoidStandup-v2', "FetchSlide-v1', 'FetchPickAndPlace-v1', "Fetch Reach-v1', 'FetchPush-v1', 'HandReach-vo', 'HandManipulateBlockRotateZ-vO', 'HandManipulateBlockRotateZTouchSensors-vo', 'Han dManipulateBlockRotateZTouchSensors-v1', "HandManipulateBlockRotateParallel-vO', "HandManipulateBlockRotateParallelTouchSenso rs-vo', 'HandManipulateBlockRotateParallelTouchSensors-v1', "HandManipulateBlockRotateXYZ-vO', 'HandManipulateBlockRotateXYZT ouchSensors-vo', "HandManipulateBlockRotateXYZTouchSensors-v1', "HandManipulateBlockFull-vo", "HandManipulateBlock-vo', 'Hand ManipulateBlockTouchSensors-v0', "HandManipulateBlockTouchSensors-v1', "HandManipulateEggRotate-vo', "HandManipulateEggRotate TouchSensors-v0', 'HandManipulateEggRotateTouchSensors-v1', 'HandManipulateEggFull-vo', 'HandManipulateEg-vo', 'HandManipula

![](figures/987-7-FIGURE.jpg)

1#载人木棒小车(CartPole）游戏
2 env = gym.make("CartPole-v1")
3
4#环境的信息
5 print(env.action space)
6 print(env.observation space)
7 print('observation space 范围："） 8 print(env.observation space.high) 9 print(env.observation space.low)

执行结果：相较于木棒 $\prime\rfloor\omicron$ 车，打砖块就复杂许多，官网并未提供相关信息，必须从GitHub下载源代码，再观看程序说明。或是查看安装目录，Atari程序安装在 anaconda3Libsite
packagesatari py 日录 $\mathrm{T}$ ，Breakout 原始程序为
ale interiace\sro\games\supported\Breakout.opp。结果如下：

## (4）加载打砖块游戏，显示环境的信息。程序代码如下：

1#载人打砖块(Breakout）游陀
2 env = gym.make("Breakout-v")
3
4#环境的信息
5 print(env.action space)
6 print(env.observation space)
print('observation space 范围：） 8 print(env.observation space.high) 9 print(env.observation space.low)

![](figures/988-3-FIGURE.jpg)

(5）实验木棒小车游戏。程序代码如下

·执行结果：以随机的方式行动，可以看到小车时而前进，时而后退，如图15.16所示

| 1 | #载人木棒小车(CartPole）游屁 |
| 2 | env = gym.make("CartPole-v1") |
| 3 |  |
| 4 | #出赛回合结荣，重置 |
| 5 | observation = env.reset( |
| 6 | #将环境信息写人日志档 |
| 7 | with open("CartPole random.log", "w", encoding='utf8') as f: |
| 8 | #执行1000次行动 |
| 9 | forin range(1000): |
| 10 | #更新画面 |
| 11 | env.render() |
| 12 | #随机行动 |
| 13 | action s env.action space.sample() |
| 14 | #融动不一步 —8 |
| 15 | observation, reward, done, info = env.step(action) |
| 16 | #写人信息 |
| 17 | f.write(f"action={action}, observation={observation}," + |
| 18 | f"reward={reward}, done={done}, info=finfo}\n") |
| 19 | #比赛回台结荣，重置 |
| 20 | if done: |
| 21 | observation s env.reset |
| 22 | env.close) |

![](figures/989-2-FIGURE.jpg)

 $1 5. 1 6$ 木棒小车游戏图

PCarPole random.log日志文件的部分内容如下

| action-0, | observation=[ | 0.02797028 | -0.203182 | 0.00185102 | .28630734],reward=1.0, |
| action=l, | observation=[ | 0.02390664 | -0.00808649 | 0.00757717 | .00579121],reward=1.0, |
| action=l, | observation=[ | 0.02374491 | 0.18692597 | 0.00746134 | .296073851,reward=1.0, |
| action=0, | observation=  | 0.02748343 | -0.00830155 | 0.00153987 | .00104711],reward=1.0, |
| action=0, | observation=[ | 0.0273174 | -0.20344555 | 0.00151892 2 | .292121271,reward=1.0, |
| action=l, | observation=[ | 2.32484899 | 9-02-8.34528 | 672e-03 37.3 | 35014e-03 -8.22245954e |
| action=1, | observation=[ | 0.02308158 3 | 0.18667032 | 0.00735971 | .2904335],reward=1.0, |
| action=0, | observation=[ | 0.02681499 | -0.00855579 | 0.00155104 | .00456148],reward=1.0, |
| action=o, | observation=[ | 0.02664387 | -0.20369996 | 0.00164227 | .29773338], reward=1.0, |
| action=1, | observation=  | 0.02256988 | -0.00860145 | 0.00759693 | .005568841,reward=1.0, |
| action=0, | observation=[ | 0.02239785 | -0.20383153 | 0.00770831 | .30063898],reward=l.0, |

木棒小车的游戏规则说明如下。

（1）可控制小车往左（0）或往右（1 O 3两个版本， $\mathrm{V O}$ ：行动超过200步， $\mathrm{V 1}$ ：行动超过500 胜。

(5）如果连续100回合的平均报酬超过195步，即视为解题成功。

(3）done：布尔值，True 表示比赛回合结束，False 表示比赛回合进行中。

 $( 2 )$ 每定一步得一分。

(3）小车一开始定位在中心点，平衡杆是直立（Upright 的，在行驶中要保持平衡。

(4）游戏结束的条件，三择一。

①平衡杆偏差超过 $1 2^{\circ} \! \mathbb{C} \! \to\Pi\! \mathbb{X}.$ 

户离中心点24单 $1 \vec{\chi} \to$ 胜

Siep函数返回的内容如下

(1)obseration：环境更新后的状态

$$
\begin{matrix} \textcircled{1}, \textcircled{2} \not{=} \textcircled{3} \textcircled{3} \circ\\ \textcircled{2}, \textcircled{4} \not{=} \textcircled{3} \textcircled{3} \end{matrix}
$$
③木棒角度。 ④木棒速度。

$$
\emptyset\imath\jmath\b( \b+\b) \imath\b( \b+\b) \imath\b( \b+\b) \imath\b( \b+\b+\b) \imath\b( \b+\b) \imath\imath\mathbf{E}_{\circ}
$$

③木棒角度

④木棒速度

(2）reward：行动后得到的奖励

$$
( 4 ) \mathrm{\ i n f o} :
$$
为字典的数据类型，通常是除错信息，本游戏均不
返回信息

范例 $2$ 木棒 $\imath\rfloor$ 车实验。

请参阅程序:15 05 CartPole.ipynb

(1）加载相关库。程序代码如下

 $1$ #载人相关库
2 import gym
3 from gym import envs

(2）设定比赛回台数。程序代码如下

(3）实验：采用随机行动。程序代码如下：

![](figures/991-7-FIGURE.jpg)

执行结果：结果相当惨烈，没有一回合走超过200步，显示出对于强化学习而言这个游戏很有挑战性。结果如下：

(4）基于上述实验，传统的做法会针对问题提出对策，举例如下

① $\imath\rfloor\omicron$ 车距离中心点大于2.4单位就算输 $\overline{{\jmath}}$ ，所以设定每次行动采用一左一右的方式，尽量不偏离中心点

②由于小车的平衡杆角度偏差 $1 2^{\circ} \left. \mathrm{L l} \lambda\right.$ 上也算输，所以设定平衡杆角度若偏右 $8^{\circ}$ 以上，就往右前进，直到角度偏右小于 $8$ 度为 $\bot\L_{\circ}$ 

(5）首先建立 Agent类别，撰写act函数实现以上逻辑。程序代码如下：

| 回合 | 报酬 | 结果 |
| --- | --- | --- |
| 0 | 18.0 | Loss |
| 1 | 55.0 | Loss |
| 2 | 38.0 | Loss |
| 3 | 11.0 | Loss |
| 4 | 34.0 | Loss |
| 5 | 51.0 | Loss |
| 6 | 15.0 | Loss |
| 7 | 20.0 | Loss |
| 8 | 11.0 | Loss |
| 9 | 32.0 | Loss |
| 10 | 13.0 | Loss |
| 11 | 22.0 | Loss |
| 12 | 21.0 | Loss |
| 13 | 15.0 | Loss |
| 14 | 57.0 | Loss |
| 14 1C | 2I.U 110 | L055 1AAA |
| 15 | 11.0 | Loss |
| 16 | 46.0 | Loss |
| 17 | 16.0 | Loss |
| 18 | 16.0 | Loss |
| 19 | 10.0 | Loss |
| 20 | 17.0 | Loss |
| 21 | 13.0 | Loss |
| 22 | 15.0 | Loss |
| 23 | 19.0 | Loss |
| 24 | 14.0 | Loss |
| 25 | 32.0 | Loss |
| 26 | 14.0 | Loss |
| 27 | 10.0 | Loss |
| 28 | 21.0 | Loss |

3反之，偏左也采用类似处理。

![](figures/993-0-FIGURE.jpg)

## (6）以agent.act(observation）取代 env.action space.sample()。程序代码如下：

## (6）以 agent.act (observation）取代

env.action space.sample()。程序代码如 $\mathbf{F}$ :

![](figures/993-4-FIGURE.jpg)

(7）显示执行结果：虽然比起随机行动的方式，改良后的报多，但绝大多数的回合依然以失败告终。结果如下：
酬增加很多,

上述的解法还有一个缺点，就是不具有通用性，这个策略就算在木棒小车游戏口有效，也不能套用到其他游戏上，也没有自我学习的能力，无法随着训练次数的增加，使模型更加聪明与准确。

木棒 $\imath\rfloor\omicron$ 车是一款很简单的游戏，不过，要能成功解题并不如想象中容易，笔者应该以前面讲到的「状态值函数最大化」为策略来进行实验,但如此一来又会碰到以下难题。

(1）状态非单一变量，而是四个变量，包括 $\imath\rfloor\omicron$ 车的位置和速度、木棒的角度和速度。

(2）这四个变量都是连续型变量，然而计算状态值函数是针对每一状态，因此状态空间必须是离散的，仅有限的个数才能倒推计算出状态值函数。

| 回合 | 报酬 | 结果 |
| --- | --- | --- |
| 0 | 103.0 | Loss |
| 1 | 86.0 | Loss |
| 2 | 116.0 | Loss |
| 3 | 125.0 | Loss |
| 4 | 119.0 | Loss |
| 5 | 117.0 | Loss |
| 6 | 165.0 | Loss |
| 7 | 55.0 | Loss |
| 8 | 200.0 | Win |
| 9 | 88.0 | Loss |
| 10 | 99.0 | Loss |
| 11 | 45.0 | Loss |
| 12 | 90.0 | Loss |
| 13 | 69.0 | Loss |
| 14 | 75.0 | Loss |
| 14 4厂 | 12.U 7O | L055 1A汽_ |
| 15 | 70.0 | Loss |
| 16 | 48.0 | Loss |
| 17 | 107.0 | Loss |
| 18 | 98.0 | Loss |
| 19 | 51.0 | Loss |
| 20 | 51.0 | Loss |
| 21 | 93.0 | Loss |
| 22 | 122.0 | Loss |
| 23 | 91.0 | Loss |
| 24 | 100.0 | Loss |
| 25 | 92.0 | Loss |
| 26 | 121.0 | Loss |
| 27 | 65.0 | Loss |
| 28 | 128.0 | Loss |

这里我们先不急着进行实验，因为后续可以搭配更好的算法来解决上述问题，所以这个问题我们暂且告一段落。

以上两个问题可以使用以下技巧处理。

(1）四个变量混合列举出所有组合。

(2）将连续型变量分组，变成有限组别

虽然 $\mathsf{G y m}$ 有很多环境供开发者挑选，但万一还是没找到完全符合需求的环境该怎么办呢？这时可以利用扩充功能Wrapper，定制预设的环境，包括以下修改。

(1）修改 step()返回的状态：预设只会返回最新的状态，我们可以利用ObservationWrapper达成此一功能

(2）修改奖励值：可以利用RewardWrapper 修改预设的奖励值。

范例. $\mathbf{A}$ ctionWrapper示范：执行木棒小车游戏，原先小车固定往左定，但加上ActionWrapper $\imath_{\mathbf{H}}$ ，会有1/10的概率采取随机行动。

(2）建立一个RandomActionWrapper类别，继承
gym.ActionWrapper 基础类别，复写action)方法。epsilon变量为随机行动的概率，每次行动时（step）都会调用
RandomActionWrapper的 action()。程序代码如下

## 15-5 Gym扩充功能

(3）修改行动值：可以利用ActionWrapper导入行动值

请参阅程序：
$$
\arraycolsep=1. 5 p t \mathrm{''} 0 6 \_\mathrm{\bf~ A c t i o n \_~ W r a p p e r \_~ t e s t. p y_{o} ~}
$$

（1）加载相关库。程序代码如下

1#载人相关库 2 import gym 3 import random

(3）实验：step（0）表示固定往左走，但却会被
RandomActionWrapper的 action)所拦截，偶尔会出现随机行动。 程序代码如下：

| 5 | 然承gym.Actionwrapper 基础类别 |
| 6 | ass RandomActionWrapper(gym.Actionwrapper): |
| 7 | def init (self, env, epsilon=0.1): |
| 8 | super(RandomActionwrapper, self). init (env) |
| 9 | self.epsilon = epsilon # 随机行动的机率 |
| 10 |  |
| 11 | def action(self, action): |
| 12 | #随机数小F epsilon,采取随机行动 |
| 13 | if random.random()< self.epsilon: |
| 14 | print("Random!") |
| 15 | return self.env.action space.sample() |
| 16 | return action |

## (3）实验：step（0）表示固定往左走，但却会被

$$
\pi_{\pm}^{\sqsubset} \Xi\cap\pi_{\mp} \mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\mp\rightbullet
$$

| 19 | if name == 11 main |
| --- | --- |
| 20 | env = RandomActionwrapper(gym.make("CartPole-v0")) |
| 21 |  |
| 22 | forin range(50): |
| 23 | env.reset() |
| 24 | total reward = 0.0 |
| 25 | while True: |
| 26 | env.render() |
| 27 | #固定往左走 |
| 28 | print("往左走！"） |
| 29 | obs, reward, done,= env.step(o) |
| 30 | — total reward t= reward  |
| 31 | if done: |
| 32 | break |
| 33 |  |
| 34 | print(f"报酬：{total reward:.2f}") |
| 35 | env.close() |

执行结果：结果如下

往左走！
$$
\begin{array} {l} {\frac{\mathrm{R a n d o m}!} {\mid\frac{\mathrm{f}} {\mid\mathrm{f} \mid\mathrm{f} \mid\mathrm{f} \mid\mathrm{f} \mid\mid}}} \\ \end{array}
$$
$$
\begin{matrix} \frac{1} {1}=\frac{1} {1}=\frac{1} {1} \\ \frac{1} {1}=\frac{1} {1}=\frac{1} {1}=\frac{1} {1} \\ \frac{1} {1}=\frac{1} {1}=\frac{1} {1} \\ \frac{1} {1}=\frac{1} {1}=\frac{1} {1} \\ \frac{1} {1}=\frac{1} {1}=\frac{1} {1} \\ \frac{1} {1}=\frac{1} {1}=\frac{1} {1} \\ \frac{1} {1}=\frac{1} {1}=\frac{1} {1} \\ \frac{1} {1}=\frac{1} {1}=\frac{1} {1}=\frac{1} {1} \end{matrix}
$$
.00

范例 $2$ ，使用 wrappers.Monitor录像，将训练过程存成多段影片文件 $( \mathrm{\bf m p 4} )$ 

请参阅程序： ${\bf1 5 \_0 7}$ Record test.py

1）加载相关库。程序代码如下

1#藏人相关库 2 import gym

(2）录像：调用Monitor)。程序代码如下：

4#载人环境
5 env = gym.make("CartPole-v")
6
7#录影
8 env = gym.wrappers.Monitor(env, "recording", force=True)

## (3）实验。程序代码如下：

| 10 | #实验 |
| --- | --- |
| 11 | forin range(50): |
| 12 | total reward = 0.0 |
| 13 | obs = env.reset() |
| 14 |  |
| 15 | while True: |
| 16 | env.render() |
| 17 | action = env.action space.sample() |
| 18 | obs, reward, done,= env.step(action) |
| 19 | total reward t= reward |
| 20 | if done: |
| 21 | break |
| 22 |  |
| 23 | print(f"报酬:{total reward:.2f}") |
| 24 |  |
| 25 | env.close() |

执行结果：会将录像结果存在rcording文件夹

上面只是列举两个简单的范例，读者有兴趣可参阅官网说明

如果上述的行动转移概率（m）、状态转移概率 $( P )$ 均为已知，即环境是明确的（Deterministic），我们就可以利用Bellma 方程式计算出状态值函数、行动值函数，以反复的方式求解，这种解法称为动态规划（Dynamic Programming， DP），类似于程序 RL 15 03 simple game with state value.ioynb，不过，更为细致一点。

动态规划的概念，是将大问题切分成 $\prime\mathrm{J} \mathrm{\ t h} \vert\mathrm{\ensuremath{\Pi}}$ 题，然后逐步解决每个小问题，由于每 $\uparrow\eta$ 问题都很类似，因此整合起来就能解决大问题。例如斐波那契数列（Fibonacci），其中 $F_{n}=F_{n-1}+F_{n-2}$ 。要计算整个数列的话，可以设计一 $\hat{\prod} F_{n}=F_{n-1}+F_{n-2}$ 函数（小问题），以递归的方式完成整个数列的计算（大问题)

请参阅程序：15 08 Fibonacci Calculation.ipynb。程序代码如 $\mathrm{T}$ :

## 15-6动态规划

依据前面谈到的Bellman方程式

$$
v_{\pi} ( s )=\sum_{a} \pi( a | s ) \: \sum_{s^{\prime}} {\cal P}_{s s^{\prime}}^{a} \: [ {\cal R}_{s s^{\prime}}^{a}+\gamma v_{\pi} ( s^{\prime} ) ]
$$

## 范例1.费波那契数列的计算

1 def fibonacci(n):
2 if n == 0 or n ==1:
3 return n
4 else:
5 return fibonacci $( n-1 )$ +fibonacci(n-2) 6
7 list1=[]
8 for i in range(2, 20):
9 list1.append(fibonacci(i))
10 print(list1)

执行结果如下：每个小问题的计算结果都会被存储下来，作为下个小问题的计算基础。

(1）策略评估（Policy Evaluation 当玩家走完一回合后， 就可以更新所有状态的值函数，这就称为策略评估，即将所有状态重新评估 $\rightharpoonup\pi$ ，也称为预测（prediction)

(2）策略改善（Policy lmprovement）：依照策略评估的最新状态，采取最佳策略，以改善模型，也称为控制（control），通常都会依据最大的状态值函数行动，我们称之为贪婪 $( \mathrm{G r e e d y} )$ 策略，但其实它是有缺陷的，后面会再详加讨论。

最后，将两个步骤合并，循环使用，即是所谓的策略循环或策略迭代（Policy lteration 如图15.17所示。

图 15.17 (a) 表示先定 $- \Xi\unitmatrix} \unitlength( 6 p t \fbox{} ) \unitlength( 1 p t \mathrm{t r} ) \unitlength( 1 p t \mathrm{t r} ) \line( 1 p t \mathrm{t r} )$ ，进行评估，接着依评估结果采用贪婪策略行动，再评估，一直循环下去，直到收敛为 $\amalg$ 

[1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181]

强化学习将问题分成两个步骤。

![](figures/1000-7-FIGURE.jpg)

图15.17 策略循环或策略送代P（ioylteration

图 15.17 （b）：强调经过行动策略 $\pi\sqrt{\Xi}$ ，状态值函数会白原来的V变成 $\boldsymbol{V}_{\pi}$ ，之后再以 $V_{\pi} 1$ 乍为行动的依据

请参阅程序：1509 Policy Evaluation.ipynb，修改自Denny Britz 网站·，它是解答「Reinforcement Learning: An
Introduction」一书（http://incompleteideas.net/bookthe-book-2nd.html）的习题。

libtenvstgridworld.py，定义游戏规则，注意，Grid World 在网络上有许多不同的版本，游戏规则略有差异。程序代码如下：

## 范例2.。以图15.18所示的迷宫为例，执行策略评估

![](figures/1001-4-FIGURE.jpg)

图15.18 Grid World迷宫（左上角为起点，右下角为终点)）

(1）加载相关库

![](figures/1001-7-FIGURE.jpg)

(2）建立 Grid World迷宫环境：程序为

 $( 3 )$ 策略评估函数

theta：差异容忍值，前后两次状态值函数的最大差异 $\omicron\rfloor\omicron\mp$ 此数值，即停止更新

(4）呼叫策略评估函数：采随机策略，往上下/左/右走的概率 $( \pi)$ 均等。程序代码如下：

)依下列公式更新。

$$
v_{\pi} ( s )=\sum_{a} \pi( a | s ) \, \sum_{s^{\prime}} {\cal P}_{s s^{\prime}}^{a} \, \left[ {\cal R}_{s s^{\prime}}^{a}+\gamma v_{\pi} ( s^{\prime} ) \right]
$$

②参数说明如下。

discount factor：折扣因子，预设为1，即无折扣

③F均为 $1$ ，表示状态转移概率均等，定义在gridworld.py 中

$$
\star\Xi\equiv\pm\pm\exists\pm\pm\pm\Gamma:
$$

| 1 | #荣略评估函数 |
| 2 3 | def policy eval(policy, env, discount factor=1.o, theta=0.00001): #状态值函数初始化  |
| 5 | while True: |
| 6 | delta=o |
| 7 | #更新每个状态值的函数 |
| 8 | for s in range(env.nS) |
| 9 | v=0 |
| 10 | #计算每个行动后的状态值函数 |
| 11 | for a, action prob in enumerate(policy[s]): |
| 12 | #取得所有可能的下一状态值 |
| 13 | forprob. next state. reward. done in env.pIslTal: |
| 14 | 飞厂厂孑一4儿 #状态值函数公式，依照所有可能的不一状态值函数加点 |
| 15 | vt= action prob * prob * (reward + discount factor * V[next statel) |
| 16 |  汽尖——— #较更新前后的差值，取最大值  |
| 17 | delta = max(delta, np.abs(v - V[sl)) |
| 18 | VsT=V  |
| 19 | #若最大差值〈阈值，则停止评估 |
| 20 | if delta< theta: |
| 21 | break |
| 22 | return np.array() |

| 1 | # 随机荣略，概率约等 |
| 2 | random policy = np.ones s([env.ns, env.nA]) env.nA |
| 3 | # 评估 |
| 4 | V= policy eval(random policy, env) |

(5）显示状态值函数。程序代码如下：

$$
\pm: ( 6 ) \exists\Delta i \exists\ Z \equiv\Xi\Xi\Xi
$$
确：与书中的答案相对照。程序代码如

请参阅程序：15 10 Policy teration.ipynb，修改自Denny Britz 网站。

（2）建立 Grid World迷宫环境：程序在
ibenvstgridworld.py，定义游戏规则。注意：Grid World 在网络上有许多不同的版本，游戏规则略有差异。程序代码如下：

$$
\mathrm{T} :
$$

![](figures/1003-4-FIGURE.jpg)

完成策略循环中的评估后，接下来再结合策略改善

范例 $\textbf{3}$ .以上述的迷宫为例，执行策略循环

(1）加载相关库，前三个步骤与上例相同。程序代码如下：

![](figures/1003-8-FIGURE.jpg)

(2）建立Grid Word迷宫环境：程序在

1#环境
2 env = GridworldEnv()

(3）策略评估函数：程序代码如下 Done step ookahead：依下列公式计算每一种行动的值函数。

③调用 one step lookahead，计算下一步的行动值函数，找出最佳行动，并更新策略转移概率（n)

| 1 | #荣略评估函数 |
| 2 | def policy eval(policy, env, discount factor=1.0, theta=0.00001): 北一57  |
| 3 | #有松值数发亿 |
| 4 | v=np.zeros(env.ns) 自  |
| 5 | while rue:  |
| 6 7 | delta=0 从百北合设大店的商米行 |
| I O | #达每认术值火 C8166C |
| O a | ToSndgeEv·o G0  |
| 2 10 | V=U 格会公行为一的4大传人商米 |
| 10 11 | 开开7口4术日四人 GARAAtiAnAhIARImAnAtAInA1SAwr-1T. |
| 一 12 | 25 #取得所有可能的人一状态值  |
| 13 | for prob, next state, reward, done in env.PIslTal: |
| 14 | 厂一45 #状态值函数公式，依照所有可能的下一状态值函数加点 |
| 15 | vt= action prob * prob * (reward + discount factor * V[next statel) |
| 16 | #比较更新前后的差值，取最大值大大——— |
| 17 | delta = max(delta, np.abs(v - v[s])) |
| 18 | Vs=V |
| 19 | #若最大差值〈间值，则停止评估 |
| 20 | if delta < theta: |
| 21 | break |
| 22 | return np.array(V) |

## (4）定义策略改善函数。

$$
q_{\pi} \big( s, a \big)=\sum_{s^{\prime}} {\cal P}_{s s^{\prime}}^{a} \ [ {\cal R}_{s s^{\prime}}^{a}+\gamma\sum_{a^{\prime}} \pi( a^{\prime} | s^{\prime} ) \ q_{\pi} ( s^{\prime}, a^{\prime} ) ]
$$

2一开始采随机策略，进行策略评估，计算状态值函数。

④直到已无较佳的行动策略，则返回策略与状态值函数。

$$
\pi_{\pm}^{\Pi} \equiv1 \pm1 \pm3 \pm1 \mp\pm
$$

![](figures/1005-0-FIGURE.jpg)

## (5）执行策略循环。程序代码如下：

## (6）显示结果。程序代码如下：

1#显示结果
2 print("策略机率分配："）
3 print(policy)
4 print("")
5
6 print("4x4 策略概率分配（ $0=u p$ , l=right, 2=down, 3=left):") 7 print(np.reshape(np.argmax(policy, axis=l), env.shape)） 8 print"")
9
10 print("状态值函数:"）
11 print(v)
12 print("")
13
14 print("4x4 状态值函数："）
15 print(v.reshape(env.shape))
16 print("")

(7）验证答案是否正确：与书中的答案相对照。程序代码如下：

(8）因此，参考策略概率分布， $\mathcal{M}$ 左上角走到右下角的最佳路径如下：

执行结果：结果如下，分别得到 $\pi$  $\mathbf{P}$ ，也就是模型的参数。

## 策略概率分配：

策略概率分配：
[[1.0. 0. 0.] [0. 0. 0.1.] [0.0.0. 1.] [0.0. 1. 0.] [1. 0. 0. 0.] [1. 0. 0. 0.] [1.0. 0. 0.1
$$
\begin{array} {c c c c} {{\left[ \begin{array} {l l l} {0} & {0} & {1} & {0} \\ {1} & {0} & {0} & {0} \\ \end{array} \right]}} & {{0}} & {{0}} \\ {{\left[ \begin{array} {l l l} {1} & {0} & {0} & {0} & {0} \\ {1} & {0} & {0} & {0} \\ \end{array} \right]}} & {{0}} \\ {{\left[ \begin{array} {l l l} {0} & {1} & {0} & {0} & {0} \\ {0} & {0} & {1} & {0} \\ {[ \begin{array} {l l l} {0} & {0} & {1} & {0} \\ {1} & {0} & {0} & {0} \\ {[ \begin{array} {l l l} {0} & {1} & {0} & {0} \\ {0} & {1} & {0} & {0} \\ {[ \end{array} \begin{array} {l l l} {0} & {0} & {0} \\ {0} & {1} & {0} & {0} \end{array} \right]}} \\ \end{array}}} \end{array}
$$

4x4 策略概率分配 $( 0=\operatorname{u p}$ , 1-right, 2-down, 3-left):

$$
\begin{matrix} \left[ \begin{matrix} [ 0 ] 3 ] & {3} & {2} \\ \left[ 0 ] \textcircled{0} \right] \textcircled{0} \textcircled{0} \left[ \textcircled{0} \textcircled{0} \right] \left[ \textcircled{0} \end{matrix} \right] \left[ \textcircled{0} \begin{matrix} {0} & {1} & {2} \\ {1} & {1} & {0} \\ \end{matrix} \right] \right]
$$

$$
y \pm1 \equiv\pm\pm\pm\pm\pm
$$

[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1. 0.]

4x4状态值函数：
[
$$
\begin{array} {c} {{[ \begin{array} {l l l l} {\end{array} \begin{array} {l l l} {\end{array}}} & {\end{array}}} & {[ \begin{array} {l l l} {\end{array} \begin{array} \end{array} \begin{array} {l l l} {\end{array}}} & {\end{array} \begin{array} \end{array}} {-\begin{array} \end{array} \begin{array} {l l l} {[ \begin{array} {l l l} {\end{array}}} & {-\begin{array} \end{array} \end{array} \begin{array} \end{array} \begin{array} {l} {-\begin{array} {l} \end{array}} & {-\begin{array} \end{array} \begin{array} {l} {-\begin{array} \end{array}} {-\begin{array} \end{array}} {-\begin{array} \end{array}} \end{array}} & {-\begin{array} \end{array} \end{array}} & {-\begin{array} \end{array} \begin{array} \end{array}} \end{array} \end{array} \begin{array} \end{array}} \end{array} \begin{array} \end{array}} \end{array} \begin{array} {l} {[ \begin{array} {l l l} {\begin{array} {1} & {-\begin{array} {1} & {-\begin{array} \end{array} \end{array}} \end{array} \begin{array} {l} {-\end{array} \begin{array} \end{array} \end{array}} \end{array} \begin{array} \end{array} \end{array} \end{array}} \end{array}}} \\ \end{array} \end{array}
$$

$$
\mathrm{T :}
$$

| 1 | # 验证答案是否正确 |
| 2 | expected V= np.array([ 0, -1, -2, -3, -1, -2, -3, -2, -2, -3, -2, -1, -3, -2, -1, 0]) |
| 3 | np.testing.assert a array almost equal(v, expected v, decimal=2) |

$$
\begin{matrix} \left[ \begin{matrix} \textcircled{0} & {3} & {2} \\ \end{matrix} \right]} \\ \left[ \begin{matrix} {\textcircled{0}} & {\textcircled{0}} & {\textcircled{2}} \\ \left[ \begin{matrix} {\textcircled{0}} & {\textcircled{0}} & {1} & {2} \\ \end{matrix} \right] \right] \\ [ \textcircled{0} & {1} & {1} & {\textcircled{0}} \\ \end{matrix}
$$

采用策略循环时，在每次策略改善前,必须先做一次策略评估，执行循环，更新所有状态值函数，直至收敛，非常耗费时间， 所以，考虑到状态值函数与行动值函数的更新十分类似，我们可以将其二者合并，以改善策略循环的缺点，称之为值循环（Value lteration)

请参阅程序：15 11 Value lteration.ipynb，修改自 Denny Britz 网站

(3）定义值循环函数：直接以行动值函数取代状态值函数将策略评估函数与策略改善函合而为一。程序代码如下：

## 15-7 值循环

范例。以上述的迷宫为例，使用值循环。

(1）加载相关库，前两个步骤与上例相同。程序代码如下：

1#载人构关库
2 import numpy as np
3 from lib.envs.gridworld import GridworldEnv

(2）建立 Grid World迷宫环境。程序代码如下：

 $1$ #环境
 $2 ~$ 
$$
\mathrm{e n v} \,=\, G r \, \mathrm{i d w o r \, l \,} \mathrm{d E n v} ( \, )
$$

![](figures/1008-0-FIGURE.jpg)

## (4）执行值循环。程序代码如下

## (5）显示结果：与策略循环结果相同。程序代码如下：

| 1 | # 显示结果  |
| 2 | print("策略概率分配："） |
| 3 | print(policy) |
| 4 | print"") |
| 5 |  |
| 6 | print("4x4 策略概率分配（O=up, 1-right, 2=down, 3=left):") |
| 7 | print(np.reshape(np.argmax(policy, axis=1), env.shape)) |
| 8 | print("") |
| 9  |
| 10 | print("状态值函数："） print(v) print""")  |
| 11 |
| 12 print("") |
| 13  |
| 14 | print("4x4 状态值函数：") |
| 15 | print(v.reshape(env.shape)) |
| 16 | print("") |

(6）验证答案是否正确：与书中的答案相对照。程序代码如下：

(7）因此，参考策略概率分布， $\mathcal{M}$ 左上角走到右下角的最佳路径如下：

执行结果：结果如下，分别得到 $\pi$  $\mathbf{P}$ ，也就是模型的参数。

## 策略概率分配：

东哈概举分配：
[[1. 0. 0. 0.] 0.0.0. 1.1
$$
\begin{array} {c c c c} {{\left[ 0.} & {0.} & {0.} & {1.} \\ {\left[ 0.} & {0.} & {1.} & {0. \right]} \\ {\left[ 1.} & {0.} & {0.} & {0.} \\ {\left[ 1.} & {0.} & {0.} & {0.} \\ {[ 1.} & {0.} & {0.} & {0. \right]} \\ {[ 1.} & {0.} & {0.} & {0. \right]} \\ {[ 0.} & {0.} & {1.} & {0.} \\ {[ 1.} & {0.} & {0.} & {0.} \\ {[ 1.} & {0.} & {0.} & {0.} \\ {[ 1.} & {0.} & {0.} & {0.} \\ \end{array} \right]} \\ \end{array}
$$
[0. 0. 1.0.] [1.0.0. 0.] [0.1. 0. 0.] [0. 1. 0. 0.] [1. 0. 0. 0.]1

4x4 策略概率分配 $( 0 )=u p$ , lright, 2=down, 3=left):
$$
\begin{matrix} \left[ \begin{matrix} {\textcircled{0}} & {3} & {3} & {2} \\ \end{matrix} \right]} \\ \end{matrix} \begin{matrix} \left[ \textcircled{0} & {0} & {0} & {2} \\ \left[ \textcircled{0} & {0} & {1} & {2} \\ [ \textcircled{0} & {1} & {1} & {0} \\ \end{matrix} \right] \right]
$$

4x4 策略概率分配 $( 0=u p$ , 1-right, 2-down, 3-left):

$$
\begin{matrix} \left[ \begin{matrix} [ 0 ] 3 ] & {3} \\ [ 0 ] \textcircled{0} \textcircled{0} \end{matrix} \right] \\ \left[ \begin{matrix} {0} & {\textcircled{0}} & {0} & {2} \\ [ \textcircled{0} \end{matrix} \right] \textcircled{0} \end{matrix}
$$
]

状态值函数：
[0. -1. $- 2.-3.-1.-2.-3.-2.-2.-3.-2.-1.-3.-2.-1.-1.$ ] 4x4状态值函数：
 $\begin{matrix} \left[ \, \begin{matrix} \, 0 \,, \,-1 \,, \,-2 \,, \,-3 \,, \end{matrix} \, \right]} \\ {\, \left[ \,-1 \,, \,-2 \,, \, \,-3 \,, \, \,-2 \,, \, \, \right]} \\ {\, \left[ \,-2 \,, \, \,-3 \,, \, \,-2 \,, \, \,-1 \,, \, \, \right]} \\ {\, \left[ \,-3 \,, \, \,-2 \,, \, \,-1 \,, \, \, \, \, \, \emptyset\,, \, \right] \, ]} \end{matrix}$ 

$$
x \pm1 \equiv\pm\pm\pm\pm\pm
$$

$$
4 \times4 \times\frac{1} {1} \times\frac{1} {1} \equiv\frac{7} {1 2} \times\frac{1} {3} \times
$$

$$
\mathrm{T :}
$$

![](figures/1009-12-FIGURE.jpg)

$$
\begin{array} {c} {\left[ \textcircled{O} \begin{matrix} {3} & {3} \\ {\textcircled{O}} & {\textcircled{O}} \\ \end{matrix} \right]} \\ {\left[ \textcircled{O}} & {\textcircled{O}} & {1} & {2 \right]} \\ {[ \textcircled{O}} & {1} & {1} & {\textcircled{O}} \\ \end{array}
$$

(1）适合定义明确的问题，即策略转移概率、状态转移概率均为已知的状况

(2）适合中小型的模型，状态空间不超过百万个，如国棋， 状态车 $\Xi[ \mathbf{\dot{H}} ]=3^{1 9 \times1 9} {=} 1. 7 4 {\times} 1 0^{1 7 2},$ 、状态值函数更新就会执行太久。另 $\flat\Gamma$ ，可能会有大部分的路径从未是过，导致样本代表性不足，进而引发维数灾难（Curse of Dimensionality)

 $\overline{{z}}$ b态规划的优缺点如下

我们在玩游戏时，通常不会知道策略转移概率、状态转移概率，其他应用领域也是如此，这称为无模型（Model Free）学习在这样的情况 $\mathrm{F}$ ，动态规划就无法派上用场。因此，有学者就应用蒙特卡洛（Monte Carlo, MC）算法，透过仿真的方式估计出了转移概率。

根据维基百科“描述，它命名的白来相当有趣，二战时期，美国研发核武器的团队发明了此计算方式，而发明人之一的斯塔尼斯拉夫·乌拉姆，因为他的叔叔经常在摩纳哥的蒙特卡洛赌场输钱， 故而将其方法取名为蒙特卡洛。蒙特卡洛算法主要就是使用随机数生成器产生数据，进而估计实际问题的答案。

）如图15.19所示，假设有一个正方形与圆形，圆形半径为 $\boldsymbol{r},$ 则圆形面积为 $\pi r^{2}$ ，正方形面积为 $( 2 r ) \quad^{2}=4 r^{2} \mathrm{~_{o} ~}$ 

## 15-8蒙特卡洛

范例1。以蒙特卡洛算法求圆周率（m O

请参阅程序：15 12 MC Pilipynba

（1）加载相关库。程序代码如下

1#载人相关库 2 import random

## （2）计算圆周率（n） O 2在正方形的范围内随机产生一千万个点，计算落在圆形内的点数。

执行结果：得到n= 3.1418028，与正确答案3.14159相差不 $\grave{\chi}$ ，如果仿真更多的点，结果就会更相近。

从以上的例子延伸，假如转移概率未知，我们也可以利用蒙特卡洛算法估计转移概率，利用随机策略去走迷宫，根据结果计算转移概率。

![](figures/1012-3-FIGURE.jpg)

 $1 5. 1 9$ 计算圆周率图

3落在圆形内的点数/一千万 $\equiv\pi r^{2} \, / \, 4 r^{2} {=} \pi/ 4$ 

④化简后n-4×（落在圆形内的点数） $\l\ -\mp\pi\o_{\circ}$ 

$$
\pi\equiv\Gamma\pm\Gamma\pm\pm\pm\Gamma\Gamma:
$$

![](figures/1012-8-FIGURE.jpg)

为了避免无聊，我们换另一款游戏「21点扑克牌」实验，读者如不熟悉游戏规则，可参照维基百科的规则说明
https://h.wikipedia.org/wiki/二十一点。

请参阅程序:15 13 Blackjack Policy Evaluation.ipynb 修改自Denny Britz 网站。（1）加载相关库。程序代码如下：

(2）建立环境：程序为lib\envsblackjack.py。程序代码如下:

(3）试玩：采用的策略为如果玩家手上点数超过（≥）20点， 才不补牌（Stick），反之都跟庄家要一张牌（Hit），策略并不合理，通常超过 $1 6$ 点就不补牌 $\overline{{\jmath}}$ ，若考虑更周详的话，会再视庄家的点数，才决定是否补牌，读者可自行更改策略，观察实验结果的变 $\L$ 。采用这个不合理的策略，是要测试各种算法是否能有效提升胜率。程序代码如下：

范例2.实验21点扑克牌之策略评估

![](figures/1013-5-FIGURE.jpg)

执行结果：结果如下，读者若不熟悉玩法，可以观察下列过程。

(4）定义策略评估函数：主要是通过既定策略计算状态值函数。

实验1000口合，记录玩牌的过程，然后计算每个状态的平均值函数。

![](figures/1014-3-FIGURE.jpg)

![](figures/1014-4-FIGURE.jpg)

》状态为Tuple 数据类型，内含：玩家的总点数0·31、庄家亮牌的点数1~11（A）、玩家是否拿 $\mathbf{A}$ ，维度大 $\imath\rfloor\omicron$ 为（32, $1 1, 2 )$ 其中 $\mathbf{A}$ 可为1点或11点，由持有者自行决定。

④注意：在一回合中每个状态有可能被走过两次以上。例如， 一开始持有 $\mathbf{A}$ 、 $5$ ，玩家视 $\mathbf{A}$ 为11点，加总为 $1 6$ 点，后来补牌后抽到 10点，改视 $\mathbf{A}$ 为 $1$ 点，加总也是16 $\breve{\pi}$ ，故 $1 6$ 点这个状态被经历两次。所以，在计算状态值函数时，有两种方式，即「首次访问」（First Visit》 $\mathcal{R}$ 「每次访问」（Every Visit）。首次访问只计算第一次访问时的报酬，而每次访问则计算所有访问的平均报酬， 本程序采用首次访问。

3行动只有两种：0为不补牌， $1$ 为补牌。

$$
\pi_{\pm}^{\Pi} \equiv\Gamma_{\pm}^{\Chi} \pm\Pi\Gamma:
$$

| 1 | #荣略评 | 估函数 |
| 2 3 4 5 6 | def polic retur retur V= | .cy eval(policy, env, num episodes, discount factor=1.0): #记录每一个状态的报酬 rns sum = defaultdict(float) rns count = defaultdict(float） # 记录每一个状态的访问个数 defaultdict(float）#状态值函数  |
| 0 7 | #实 | 验台 |
| 8 | forj | i episode in range(1, num episodes + 1): |
| 9 |  # | 一一上一5 #每1000回合显示除错信息  |
| 10 | 3 | 一 if i episode %1000=0: |
| 一 11 | - | 一一 print(f"\r Ji episodel/fnum episodes 回合".end="" |
| 一一 12 |  | 一L-F svs.stdout.flush(）#清除画面  |
| 一 13 |  |  |
| 一 14 | 1 | #回台(enisode)数据结构为性列，每一项月含state. action. reward |
| 二 15 | 6 | 8 episode=T  |
| 一 16 |  C | FLJ statosenv.reset |
| 一 17 |  -大 | 5 #开始依等路玩牌，易多100步强，中涂分出胜务即结表 |
| 18 | 1 | for t in range(100): |
| 19 |  | action = policy(state) |
| 20 |  | next state, reward, done,= env.step(action) |
| 16 17 18 19 20 21 22 23 24 25 A一 | S t ， | state = env.reset( #开始依策略玩牌，最多100步鉴，中途分出胜负即结荣 for t in range(100): action = policy(state) next state, reward, done, = env.step(action) episode.append((state, action, reward)) if done: break state = next state 本业国一一白限台公  |
| 22 |  | 厂 if done:  |
| 24 |  | state - next state |
| 25 |  |  |
| 27 | s | states in episode - set([tuple(x[O]) for x in episode]) |
| 28 | 1 | #计算每一状态的值函数 |
| 29 | 1 | for state in states in episode: |
| 30 |  | #龙出每一步骤内的首灾访问(First Visit) |
| 31 |  | first occurence idx = next(i for i,x in enumerate(episode) |
| 32 |  | if x[o]= state) |
| 33 |  | #算票计报酬(G) |
| 34 |  | G= sum([x[2]*(discount factor**i) for i,x in |
| 35 |  | enumerate(episode[first occurence idx:])]) |
| 36 |  | #计算状态值函数 |
| 37 |  | returns sum[state] t= G |
| 38 |  | returns count[state] t= 1.0 |
| 39 |  | V[state] = returns sum[state] / returns count[state] |
| 40 |  |  |
| 41 | retur | irn V |

## (5）制定策略：与前面策略相同。程序代码如下：

| 1 | #买相司装略 |
| --- | --- |
| 2 | def sample policy(observation): |
| 3 | score, dealer score, usable ace = observation |
| 4 | 超过20点，不补牌(stick），否则都跟庄家要一张牌(hit) |
| 5 | return o if score e=20else1 |

## (6）分别实验1.000与50000日合。程序代码如下：

1#实验10000回合
2 V 10k = policy eval(sample policy, env, num episodes=10000) 3 plotting.plot value function(V 10k, title="10,000 Steps")
4
5 #实验500,000 回台
6 $\mathsf{v}$ 50Ok = policy eval(sample policy, env, num episodes=500000) 7 plotting.plot value function(V 500k, title"500,000 Steps"

个执行结果：显示各状态的值函数，分成持有 $\mathbf{A}$ （比较容易获胜）及未持有 $\mathbf{A}$ 。如图15.20 可以看到当玩家持有的分数很
所示，
高时，胜率会明显提升。下列彩色图表可参考程序执行结果。

②持有 $\mathbf{A}$ ，但分数不高时，胜率也有明显提升，如图15.21所示。

接着进行值循环，结合策略评估与策略改善。另外，还要介绍一个新的策略e-greedy。之前策略改善时都是采用贪婪策略，它有

![](figures/1017-3-FIGURE.jpg)

图15.20实验10000口台

![](figures/1017-5-FIGURE.jpg)

图15.21、持有 $\mathbf{A}$ 但分数不高

$$
\begin{array} {l} {\textcircled{3} \not{\equiv} \not{\equiv} \not{\oplus} 5 0 0 0 0 0 \parallel\Xi\not{\oplus} \boxed{\Xi} \not{\ni} \mathbb{B} \mathbb{B} \mathbb{B} \not{\equiv} \mathbb{B} \mathbb{B} \mathbb{B} \mathbb{B}_{\mathbb{B}},} \\ \end{array}
$$

一个弱点，就是一旦发现最大值函数的路径后，贪婪策略会一直走相同的路径，这样便失去了找到更好路径的潜在机会，举例来说， 家庭聚餐时，都会选择最好的美食餐 $\sqrt{\Pi}$ ，若为了不踩雪，每次都去之前最好吃的餐厅用餐，其他新开的餐厅就永远没机会被发现 $\overline{{\jmath}}$ ， 这就是所谓的「探索与利用』（Exploration and Exploitation），而 e-greedy所采取的方式就是除了采用最佳路径之外，还保留一个比例去探索，不完全走既有的老路，如冬15.22所示。下面我们就尝试这种新策略与蒙特卡洛算法结合。

请参阅程序：15 14 Blackjack Value Iteration.ipynb，修改自Denny Briz 网站。

老地方(利用)：有质量保证。新开的餐厅(探索):有惊喜？

![](figures/1018-3-FIGURE.jpg)

冬 $1 5. 2 2$ 探索与利用

范例3.实验21点扑克牌之值循环

(1）加载相关库。程序代码如下 (2）建立环境：程序为 ib\envsblackjack.py。程序代码如下:

(3）定义e-greedy策略：若e0.1，则10次行动有 $1$ 次采取随机行动。程序代码如下：

(4）定义值循环函数：与上例的程序逻辑几乎相同，主要差
_
异是将状态值函数改为行动值函数。程序代码如下：

| 1 | 境 |
| 2 | = BlackjackEnv() |

| 1 | #s-greedy策路 |
| 2 | def make epsilon greedy policy(Q, epsilon, nA): |
| 3 | def policy fn(observation): |
| 4 | #每个行动的概率初始化，均为8/n |
| 5 | A=np.ones(nA, dtype=float) * epsilon / nA |
| 6 | best action = np.argmax(Qfobservationl) |
| 7 | #最佳行动的概率再加1-8 |
| 8 | A[best actionl t= (1.0 - epsilon) |
| 9 | return A |
| 10 | return policy fn |

执行结果：上个范例只有当玩家的分数接近 $2 0$ 分的时候，值函数特别高，然而，这个策略即使在低分时也有不差的表现，胜率

| 1 2 3 4 5 | #值值环函数 returns sum = defaultdict(float) def value iteration(env, num episodes, discount factor-1.0, epsilon=0.1): #记录每一个状态的报酬 returns count = defaultdict(float) # 记录每一个状态的访问个数 Q= defaultdict(lambda: np.zeros(env.action space.n)）# 行动值函数 |
| 6 7 8 o | #买用c-greedy荣略 policy = make epsilon greedy policy(Q, epsilon, env.action space.n) |
| 10 | #头验回台  |
| 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 | #每1000 回台显示六除绪信息 if i episode % 1000 == 0: print(f"\r {i episode}/{num episodes}回合.",end"") sys.stdout.flush(）#清除画面 #回台(episode)数据结构为阵列，每一项目含state、action、reward episode = [] state s env.reset() #开始依策略玩牌，最多100步摄，中途分出胜负助结束 for t in range(100): probs = policy(state) action = np.random.choice(np.arange(len(probs)), p=probs) next state, reward, done, -= env.step(action) episode.append((state, action, reward)) if done: break state = next state  |
| 26 | if done: 厂 |
| 一 30 | 丝我出走过的所有状茶 |
| 20 31 | TCCHC cain enicodo - cot(r(tunla(vo1) y11) fory in enicodel) |
| 二一 32 | LLLF for state, action in sa in episode:  |
| 24 33 | iocysc- -roclncPlovc. #状态、行动动组合初始化  |
| 34 | sa pair = (state, action) |
| 一 35 | FJ #我出每一步警内的首力访问(FirstVisit） |
| 一 36 | 一 first occurence idx = next(i for i.x in enumerate(episode) |
| 37 | 一 if xrol== state and x[1l== action) |
| 38 | #算累计报酬(G) |
| 39 | G=sum([x[2]*(discount factor**i) for i,x in |
| 40 | enumerate(episode[first occurence idx:)) |
| 41 | #计算行动值函数 |
| 42 | returns sum[sa pair] t= G |
| 43 | retunns count[sa pair] t= 1.0 |
| 44 | Q[state][action]= returns sum[sa pair] / returns count[sa pair] |
| 45 |
| 46 | return Q, policy |

## ，执行值循环。程序代码如下： (5)

(6）显示执行结果。程序代码如下：

| 1 | #显示结果 |
| --- | --- |
| 2 | V= defaultdict(float) |
| 3 | for state, actions in Q.items(): |
| 4 | action value = np.max(actions) |
| 5 | 一 V[state = action value  |
| 6 | plotting.plot value function(V, title="Optimal LValue Function") |

再来看另一种想法，目前为上的值循环在策略评估与策略改良 $\pm$ ，均采用同一策略，即e-greedy，而这次两者各自采用不同策略，即策略评估时采用随机策略，尽可能走过所有路径，在策略改良时，改为采用贪婪策略，尽量求胜。所以，采用同一策略，我们称为 $\mathrm{O n-p o l i c y}$ ，采用不同策略则称为Of-polcy。

请参阅程序：15 15 Blackijac $\mathsf{c o f f \_P o l i c y. i p y n b},$ 。修改自 Denny Britz 网站。

(2）建立环境：程序为 ib\envsblackjack.py。程序代码如下:

。程序执行结果可参考图15.23 比起上例明显提升。

![](figures/1021-4-FIGURE.jpg)

图15.23：e-grey策略执行结果

范例4.实验21点扑克牌之off-poley值循环

(1）加载相关库。程序代码如下

![](figures/1021-8-FIGURE.jpg)

D重要性加权拍样（Weighted mportance Samping 以值函数大小作为随机抽样比例的分母

(3）定义随机策略在评估时使用。程序代码如下

| 1 | 随机策胎 |
| --- | --- |
| 2 | def create random policy(nA): |
| 3 | A= np.ones(nA, dtype=float) / nA |
| 4 | def policy fn(observation): |
| 5 | return A |
| 6 | return policy fn |

(4）定义贪婪策略在改良时使用。程序代码如下

| 1 | #贪婪策略 |
| --- | --- |
| 2 | def create greedy policy(Q): |
| 3 | def policy fn(state): |
| 4 | #每个行动的机率初始化，均为0 |
| 5 | A = np.zeros like(Q[statel, dtype=float) |
| 6 | best action = np.argmax(Qstatel) |
| 7 | #最佳行动的机率=1 |
| 8 | A[best actionl= 1.0 |
| 9 | return A |
| 10 | return policy fn |

(5）定义值循环策略，使用重要性加权抽样

②依重要性加权抽样，值函数公式为

$$
Q ( S_{t}, A_{t} ) \gets Q ( S_{t}, A_{t} )+\frac{W} {C ( S_{t}, A_{t} )} \left[ G-Q ( S_{t}, A_{t} ) \right]
$$

$$
\pi\equiv\Gamma\pm\pi\pm\pm\pm\Gamma\Gamma:
$$

![](figures/1023-0-FIGURE.jpg)

## 执行值循环：评估时采用随机策略。程序代码如下： (6)

| 1 | # 执行值值环 |
| 2 | random policy = create e random policy(env.action space.n) |
| 3 | Q, policy = mc control importance s sampling(env, num episodes=500000, |
| 4 | behavior policy=random policy) |

## (7）显示执行结果。程序代码如下：

1#显示结果
2 V = defaultdict(float)
3 for state, actions in Q.items():
4 action value = np.max(actions)
5 V[state]= action value
6 plotting.plot value function(V, title="Optimal Value Function'"` 执行结果：玩家分数在低分时胜率也明显提升。程序执行结果可参考图15.24

这一节我们学会了运用蒙特卡洛算法，还有探索与利用、
On/Off Policy，使模型胜率提高了不少，这些概念不只可以应用在蒙特卡洛算法。 $\L$ ，也能套用到后续其他的算法中，读者可视项口不同的需求来选择。

![](figures/1024-2-FIGURE.jpg)

图 $1 5. 2 4$ 显示执行结果

蒙特卡洛算法必须先完成一些评估后，才能计算值函数，接着依据值函数计算出状态转移概率，有以下缺点

(2）假如状态空间很大的话，还是一样要走到终点，才能开始下行动决策，速度实在太慢，如围棋，根据统计，每下一盘棋平均约需150手，而且围棋共有 $3^{1 9 \times1 9} {\div} 1. 7 4 {\times} 1 0^{1 7 2} \uparrow)$ 犬态，就算使用探索也很难测试到每个状态，计算值函数。

于是学者提出时序差分（Temporal Dilference， TD）算法，通过边走边计算值函数的方式，解决上述问题

值函数每次加上下一状态值函数与目前状态值函数的差额，以目前的行动产生的结果代替Bellman $\big< \lambda$ 式的期望值，另外，再乘以学习率（Learning Rate） $\alpha$ 。这种走一步更新一次的做法称为TD (0），如果是走n步更新一次的做法称为TD $( \lambda)$ 

以倒推图比较动态规划、蒙特卡洛及时序差分算法的做法，如图15.25所示

## 15-9 时序差分

 $( 1 )$ 每个回合必须走到终点，才能够倒推每个状态的值函数。

 $\boldsymbol{\imath}$ 直函数更新 $\angle< \pm>$ 

$$
v ( s_{t} )=v ( s_{t} )+\alpha[ r_{t+1}+\gamma v ( s_{t+1} )-v ( s_{t} ) ]
$$

(1）动态规划：逐步搜寻所有的下一个可能状态，计算值函数期望值。

(2）蒙特卡洛：试走多个回台，再以回推的方式计算值函数期望值。

先介绍 $\mathrm{S A R S A}$ 算法，它的名字是行动轨迹中 $5$ 个元素 $s_{t}$  $a_{t}$ 2 $r_{t+1}, \, s_{t+1}, \, a_{t+1}$ 的缩写，如图15.26所示，意味着每走一步更新一次。

接着我们就来进行算法的实践，再介绍一款新游戏Windy Gridworld，与原来的 Grid World 有些差异，变成了10x7个格子 (3）时序差分：每走一步更新一次值函数

![](figures/1026-5-FIGURE.jpg)

图15.25动态规划、蒙特卡洛及时序差分算法的倒推图时序差分有以下两种算法。

图15.25动态规划、蒙特卡洛及时序差分算法的倒推图

时序差分有以下两种算法。

(1）SARSA算法：On Policy的时序差分。 (2）Q-learning算法：Off Policy的时序差分。

（1）SARSA算法：On Poliey的时序差分

(2) Q-learning算法：O Policy的时序差分

![](figures/1026-12-FIGURE.jpg)

图15.26 SARSA

第 $4$ 、5、6、9列的风力 $1$ 级，第 $7$  $8$ 列的风力2级，会把玩家往上吹 $1$ 格和 $2$ 格，而起点（x）与终点（T）的位置如图15.27所 $\overline{{\Pi}}$ 。

请参阅程序:15 16 SARSA.ipynb，修改自Denny Britz 网站。

$$
\begin{array} {c c c c c c c c c c c} {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} \\ {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} \\ {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} \\ {{\times}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{\top}} & {{0}} & {{0}} \\ {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} \\ {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} \\ {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} & {{0}} \end{array}
$$

$$
\boxed{\A4} 1 5. 2 7 \quad\mathrm{W i n d y ~ G r i d w o r l d}
$$

范例1.实验Wind $\mathbf{Y}$ Gridworld Z SARSA 策略

1）加载相关库。程序代码如下

![](figures/1027-6-FIGURE.jpg)

## (2）建立 Windy Gridworldi环境。程序代码如下：

1#建立环境
2
$$
2 \quad{\mathrm{e n v}} \,=\, {\mathrm{w i n d y G r i d w o r ~ I d E n v}} \, ( \cdot)
$$

(3）试玩，一律往右走。程序代码如下 30：第30个点，表示起始点为第 $3$ 行第0列，索引值从 $0$ 开始算

| 1 | #试玩 |  |  |
| --- | --- | --- | --- |
| 2 | print(env.reset()) | # | 重置 |
| 3 | env.render() | # | 更新画面 |
| 4 |  |  |  |
| 5 | print(env.step(1)) env.render()  | # | 走下一步 |
| 6 | # | 更新画面 |
| 7 |
| 8 | print(env.step(1)) | # | 走下一步 |
| 9 env.render() #更新画面 |
| 10 |
| 11 | print(env.step(1)) env.render()  | # | 走下一步 |
| 一 12 | # | 二更新画面 |
| N 13  |
| 14 | print(env.step(1)) | # | 法下一步 |
| 15 | 厂 env.render  | # | 更新画面 |
| 16 |
| 17 | print(env.step(1)) | # | 走下一步 |
| 18 | env.renderO  | # | 更新画面 |
| 19 |
| 20 | print(env.step(1)) | # | 走下一步 |
| 21 | env.render) | # | 更新画面 |

## 执行结果如下:

0000000000
0000000000
0000000000
xo0000oToo
0000000000
0000000000
0000000000
移至第 $3$ 行第 $1$ 列，奖励为-1。
$$
( 3 1,-1. 0, \mathrm{F a l s e}, \{\mathrm{p r o b} \4 : \ 1. 0 \} )
$$

$$
( 3 1,-1. 0, \mathrm{F a l s e}, \{\mathrm{p r o b^{\prime} : \} 1. 0 \} )
$$

(5）定义SARSA策略：走一步算一步，然后采用e-greedy 策略，决定行动。程序代码如下：

$$
\begin{array} {c} {0 0 0 0 0 0 0 0 0} \\ {0 0 0 0 0 0 0 0 0} \\ {0 0 0 0 0 0 0 0 0} \\ \end{array}
$$
$$
\omicron\times0 \ 0 \ 0 \omicron\omicron\textrm{T o o}
$$
$$
\begin{matrix} \circ0 0 0 0 0 0 0 0 \\ \circ0 0 0 0 0 0 0 \\ \circ0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
$$

$$
0 0 0 0 0 0 0 0 0
$$

$$
0 0 0 0 0 0 0 0 0
$$

$$
0 0 0 0 0 0 0 0 0
$$

$$
\omicron\times0 \omicron\circ\omicron\textrm{T o o}
$$

$$
0 0 0 0 0 0 0 0 0
$$

$$
0 0 0 0 0 0 0 0 0
$$

$$
0 0 0 0 0 0 0 0 0
$$

$$
( 3 3,-1. 0, \mathrm{F a l s e}, \{\mathrm{p r o b^{\prime} \! : \} 1. 0 \} )
$$

$$
( 2 4,-1. 0, \mathrm{F a l s e}, \{\mathrm{l p r o b^{\prime} \! : \ 1. 0} \} )
$$
→往上吹一格

$$
( 1 5,-1. 0, \mathrm{F a l s e}, \{\mathrm{l p r o b} \4 : \ 1. 0 \} )
$$
→往上吹一格

(4）定Xe-greeo $\mathbf{y}$ 策略：与前百相同。程序代码如下：

| 1 | #定义E-greedy荣路 |
| 2 | def make epsilon greedy policy(Q, epsilon, nA): |
| 3 | def policy fn(observation): |
| 4 | #每个行动的概率初始化，均为：/n |
| 5 | A= np.ones(nA, dtype=float） )*epsilon / nA |
| 6 | best action - np.argmax(O[observationl) |
| 7 | 一5 #最佳行动的概率再加1-8  |
| 8 | A[best actionl t= (1.0 - epsilon) |
| 9 | return A |
| 10 | return policy fn |

| 4 5 6 7 8 9 10 | Q= defaultdict(lambda: np.zeros(env.action space.n)) #记录所有回合的长度及奖励 stats = plotting.EpisodeStats( episode lengths=np.zeros(num episodes） episode rewards=np.zeros(num episodes)) #使用c-greedy荣路  |
| 39 40 41 42 | #更新状态值 td target - reward + discount factor * Q[next state][next action] td delta - td target - Q[state][action] Q[statel[action] t= alpha * td delta  |
| 22 40 41 42 43 44 45 46 47 48 49 50 | #74E td target - reward + discount factor * Q[next state][next action] td delta - td target - Q[state][action] Q[state][action] +- alpha * td delta if done: break action - next action state s next state return , stats  |

## ）执行SARSA策略200回合。程序代码如下： (6)

## (7）显示执行结果。程序代码如下：

2每一回台走到终点的距离：刚开始的时候要走很多步才会到终点，不过执行到大约第50 回合后就逐渐收敛 $\overline{{\jmath}}$ ，每回合几乎都步数相同，如图15.28所示

3每一回合的报酬：每回合获得的报酬越来越高，如图15.29 所示

D执行结果：共有三张图表

![](figures/1031-3-FIGURE.jpg)

图15.28执行结果（距离)

![](figures/1031-5-FIGURE.jpg)

15.29执行结果（报酬) 图

④累计的步数与回合对比：呈现曲线上升的趋势，即每回合到达终点的步数越来越少，如图15.30所示。

接着说明时序差分的第二种算法Q-learning，它与 SARSA的差别是采用Of Policy，评估时使用s-greedy策略，改良时选择 greed $\mathbf{y}$ 策略。再介绍另一款游戏Clif Walking，同样也是迷宫，最下面一排除了起点（x）与终点（T）之外，其他都是陷阱 $( \mathrm{C} )$ 
踩到陷阱即终上游戏,如图15.31所示。

请参阅程序：15 17 Q learning.pynb，修改自Denny Britz 网站。

![](figures/1032-3-FIGURE.jpg)

图15.30执行结果（累计步数与回合)

![](figures/1032-5-FIGURE.jpg)

15.31 CIif Waking 游戏图

tCliff Walking 之 Q-learning 策略。 范例2.实验

1）加载相关库。程序代码如下

036：第36个点，表示起始点为第 $3$ 行第 $0 \ {\it\Sigma} \mathrm{I J}$ ，但程序却是在第 $3$ 行第 $6 \not> \mathrm{l j}$ ，应该是程序逻辑有问题，但不影响测试，就不除错 $\overline{{\jmath}}$ ，如图15.32所示。

(2）建立CHit Waking环境。程序代码如下：

1#建立环境
$$
\mathrm{2 \ \ \ e n v \ =\ C I i f f w a l k i n g E n v \left( \right)}
$$

(3）试玩：随便走。程序代码如下：

| 1 | #试玩 |  |  |
| 2 | print(env.reset()) | # | 重置 |
| 3 | env.renderO | # | 更新画面 |
| 4 |  |  |  |
| 5 | print(env.step(0)) env.render()  | # | 往上走 |
| 6 | # | 更新画面 |
| 7 |
| 8 | print(env.step(1)) | # | 往右走 |
| 9 env.render() Y |
| 10 |
| 11 | print(env.step(1)) env.render()  | # | 往石走 |
| 12 |  |  |
| 13 |
| 14 | print(env.step(2)) | # | 往下走 |
| 15 | env.render) |  |  |

## 执行结果：

![](figures/1033-6-FIGURE.jpg)

冬 $1 5. 3 2$ 执行结果36

2走到最 $\Xi-\pm$ ，移至第 $3$ 行第2列，走到陷阱，奖励-100. 如图15.33所示

(5）定义Q-learning策略：评估时采s-greed $\mathbf{y}$ 策略，改良时选择greedy策略。程序代码如下：

![](figures/1034-2-FIGURE.jpg)

图1533、执行到最后一步结果

(4）定义e-greed $\mathbf{y}$ 策略：与前面相同。程序代码如下：

| 1 | #定义E-greedy装路 |
| 山 2 3 4 8 7 8 9 | 开小EN def make epsilon greedy policy(Q, epsilon, nA): def policy fn(observation): #每个行动的概率初始化，均为/n A= np.ones(nA, dtype=float) )*epsilon / nA best action = np.argmax(Q[observation]) #最佳行动的概率再加1-8 A[best action] t= (1.0 - epsilon) return A  |
| 7 | 一 #最佳行动的概率再加1-8  |
| 8 | A[best actionl t= (1.0 - epsilon) |
| 9 | return A |
| 10 | return policy fn |

| 1 2 3 4 | #定义QLearning荣路 def q learning(env, num episodes, discount factor-1.0, alpha=0.5, epsilon=0.1) #行动值函数初始化 o= defaultdict(lambda: np.zeros(env.action space.n))  |
| 4 5 6 7 8 9 | WCN Q= defaultdict(lambda: np.zeros(env.action space.n)) #记录所有回台的长度及奖励 stats - plotting.EpisodeStats( episode lengths=np.zeros(num episodes), episode rewards=np.zeros(num episodes))  |
| 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 | 厂口8 episode lengths=np.zeros(num episodes), episode rewards=np.zeros(num episodes)) #使用c-greedy策路 policy - make epsilon greedy policy(Q, epsilon, env.action space.n) #实验回台 for i episode in range(num episodes): #每100回台显示除错信息 if (i episode + 1) % 100==0: print(f"\r {(i episode + 1)}/{num episodes}回合.",end="") sys.stdout.flush(）#湾除画面 #开始依策略实验 state = env.reset() #每欢走一步就更新状态值  |
| LL 92 | #可大上机术店 本A十+6+618660十/1 |
| 42 2A | oTLcIL5•cuIL· 社使田-788,芳警  |
| 24 DC | #发t 8-greey家吃 限18 |
| 一 26 | 三上金二 #选择下一步行动  |
| 26 汽一 | #边学少信司 121881156 |
| 27 | action = np.random.choice(np.arange(len(action probs)), p=action probs) |
| 28 | 飞仁仁三仁仁百é它诊诊豆三豆Y厂Y纤耻口N皈 next state, reward, done,= env.step(action)  |
| 28 | next state, reward, done, -= env.step(action) |
| 29 | 出民7 |
| 30 | #更新长度及奖励 |
| 31 | stats.episode rewards[i episode] t= reward |
| 32 | stats.episode lengths[i episode]= t |
| 33 |  |
| 34 | #选择最佳行动 |
| 36 | #更新状态值一—8 |
| 二 38 39 40 41 42 43 44 45 46 | 上525-555345555—55-503 td delta= td target - Q[state][action] Q[state][action] t= alpha * td delta if done: break state s next state return Q, stats  |
| 43 |  |
| 44 | state - next state |
| 45 |  |

## ）执行Q-learning策略500回合。程序代码如下： (6)

## (7）显示执行结果。程序代码如下：

 $^\downarrow_{2}$ MnMilLtececlodcudcoeD
=

D执行结果：共有三张图表

2每一回合定到终点的距离：与SARSA 相同，刚开始要走很多步后才会到终点，但执行到大约第50 回合就逐渐收敛，之后的每个回合几乎都步数相同。由于，这两个范例的游戏不同，不能比较SARSA 与 Q-learning的效果，若要比较效果，可改用同一款游戏进行比较，如图15.34所示。

3每一回合的报酬：每回合获得的报酬越来越高，不过尚未收敛，如图15.35所示。

累计的步数与回合对比：呈现直线上升的趋势，即每回合到达终点的步数差不多，这可能是因为有陷阱的关系，应该分成胜败两种模式比较，会更清楚，如图15.36所示。

![](figures/1036-3-FIGURE.jpg)

图15.341执行结果（每一回台走到终点距离)

![](figures/1036-5-FIGURE.jpg)

图15.35、执行结果（每一回合报酬)

看了这么多的算法，不管是策略循环或值循环，总体而言，它们的逻辑与神经网络优化求解，其实有些相似，如图15.38所示。

![](figures/1037-1-FIGURE.jpg)

图15.36执行结果（累计步数与回台）

总体而言，SARSA与 Q-leaning的比较如图15.37所示

![](figures/1037-4-FIGURE.jpg)

图15.37 SARSA与 Q-leaning策略的比较

![](figures/1037-6-FIGURE.jpg)

图15.38策略循环与梯度下降法的比较

不管是使用状态值函数或是行动值函数，上述算法都是建立一个数组，来记录所有的对应值，之后就从数组中选择最佳的行动， 所以这类算法被称为表格型（Tabular）强化学习，做法简单直接， 但只适合离散型的状态，像木棒 $\imath\rfloor\omicron$ 车这种的连续型变量，就不适用，变通的做法有以下两种。

(2）使用概率分布或神经网络模型取代表格，以策略评估的训练数据，来估计模型的参数（权重），选择行动时，就依据模型推断出最佳预测值，而Deep Q-earning（DQN）即是利用神经网络的Q-learning算法。

另 ${\cal Z} \vdash$ 个研究的课题则是多人游戏的情境，不同于之前介绍的游戏，玩家都只有 $- 1 \vec{\Omega}$ ，现代的游戏设计重视人际之间的交流，可以有多位玩家同时参与一款游戏，这时就会产生协同合作或互相对抗的情境，玩家除了考虑奖励与状态外，也需观察其他玩家的状态，这种算法称为多玩家强化学习（Muli-agent Reinforcement Learning），如许多扑克牌游戏都属于多玩家游戏，它们之间的比较如图15.39所示。

## 15-10其他算法

1）将连续型变量进行分组，转换成离散型变量。 近年来，强化学习研究的环境越来越复杂，各种算法相继推陈出新，可参阅维基百科强化学习的介绍

(htosen.kiedia.o.g wikiReintorcement learning 具体详 $\Pi$ 表15.1

本书关于算法的介绍就到此告一段落，想了解更多内容的读者，可详阅强化学习的「Reinforcement Learning:An

![](figures/1039-3-FIGURE.jpg)

图 $1 5. 3 9$ 单玩家与多玩家强化学习的比较

(图片来源：「An Overview of Muli-Agent Reinforcement Learning from Game Theoretical Perspective l

表15.1 强化学习算法的比较

| Algorithm ← | Description $ | Policye | Action Space e | State Space e | Operator 0 |
| --- | --- | --- | --- | --- | --- |
| Monte Carlo | Every visit to Monte Carlo | Either | Discrete | Discrete S | iample-means |
| Q-learning | State-action-reward-state | Off-policy | Discrete | Discrete C | Q-value |
| SARSA | State-action-reward-state-action | On-policy | Discrete | Discrete C | -value |
| Q-learning - Lambda | State-action-reward-state with eligibility traces | Off-policy | Discrete | Discrete C | Q-value |
| SARSA - Lambda | State-action-reward-state-action with eligibility traces | On-policy | Discrete | Discrete C | Q-value |
| DQN | Deep Q Network | Off-policy | Discrete | Continuous C | Q-value |
| DDPG | Deep Deterministic Policy Gradient | Off-policy | Continuous | Continuous C | Q-value |
| A3C | Asynchronous Advantage Actor-Critic Algorithm | On-policy | Continuous | Continuous A | dvantage |
| NAF | Q-Learning with Normalized Advantage Functions | Off-policy | Continuous | Continuous A | dvantage |
| TRPO | Trust Region Policy Optimization | On-policy | Continuous | Continuous A | dvantage |
| PPO | Proximal Policy Optimization | On-policy | Continuous | Continuous A | dvantage |
| TD3 | Twin Delayed Deep Deterministic Policy Gradient | Off-policy | Continuous | Continuous C | Q-value |
| SAC | Soft Actor-Critic | Off-policy | Continuous | Continuous A | dvantage |

Introduction -书（itp:/incompleteideas.netbookthe-book.html)

接着我们就开始实战吧，拿最简单的井字游戏（Tic-Tac-Toe) 来练 $\supset$ ，包括如何把井字游戏转换为环境，如何定义状态及立即奖励，模型存盘等，如图15.40所示

请参阅程序： TicTacToe 1ticTacToe.py，程序修改自
Reintorcement Learning-implement TicTactoel,，程序撰写成类似Gym的架构

(2）定义环境类别：与前面的范例类似，主要就是棋盘重置 (Reset）、更新状态、给予奖励及判断输赢，胜负未分的时候， 计算机加0.1 $\acute{\jmath}$ ，而玩家加0.5 $\acute{\jmath}$ ，这样设定是希望计算机能尽快赢得胜利，读者可以试试看其他的给分方式，若胜负已定，则给1 分。

## 15-11 井字游戏

![](figures/1041-4-FIGURE.jpg)

 $1 5$ 40 井字游戏图

范例.实验井字游戏之Q-learning策略

(1）加载相关库。程序代码如下

| 1 | 载入相关库 |
| 2 | port numpy as np |
| 3 | port pickle  |
| 4 | port os |
| 5 |
| 6 | 参数设定 |
| 7 | ARD ROWS=3#行数 |
| 8 | ARD COLS=3#列数 |

(4）判断输赢：取得胜利的情况包括连成一列、一行或对角线。程序代码如下

(5）定义显示空位置、更新棋盘、给予奖励等函数。程序代码如下：

(3) 。程序代码如下环境初始化。

![](figures/1042-3-FIGURE.jpg)

| 27 | #判断输赢 |
| 28 | def is done(self): |
| 29 | #连成一列 |
| 30 | for i in range(BOARD ROWS): |
| 31 | if sum(self.board[i, :]) == 3: |
| 32 | self.isEnd = True |
| 33 | return 1 |
| 34 | if sum(self.board[i, :]) == -3: |
| 35 | self.isEnd = True |
| 36 return -1 |
| 37 |  |
| 38 | #连成一行 |
| 39 | for i in range(BOARD COLS): |
| 40 | if sum(self.board[:, i]) == 3: |
| 41 | self.isEnd = True |
| 42 | return 1 |
| 43 | if sum(self.board[:, i]) == -3: |
| 44 | self.isEnd = True |
| 44 A一 | seiT.Istna =rue |
| 46 汽一江迎华停名公 |
| 4/ a | 开连桃心用车 上上2广7966666- |
| 48 alag suml = sum([selT.board[l, I Tor l in range(BoAKD CoLS)JD 49 diag sum2 = sum([self.board[i, BOARD COLS - i - 1] for i in CQ AARAAIDADnAICTY |
| 49 uLdgsumz - Suml[selT.DodrL-, DUAD LL2rL--TorLLn 50 range(BOARD COLS)]) |
| 51 | diag sum = max(abs(diag sum1). abs(diag sum2)) D |
| 52 | if diag sum ==3: O一—— |
| 53 | — self.isEnd = True |
| 54 | if diag sum1 == 3 or diag sum2 == 3: |
| 55 | return 1 |
| 56 | else: |
| 57 | return -1 |
| 58 |
| 59 | #无空位置即算平手 |
| 60 | if len(self.availablePositions(O) == O: |
| 61 | self.isEnd = True |
| 62 | return o |
| 63 | self.isEnd = False |
| 64 | return None |

（7）训练：这是重点，本例训练50000回合后，可产生状态值函数表，将计算机和玩家的状态值函数表分别存盘（policy pl、 policy p2），policy p1 为先下子的策略模型，policy p2为后下子的策略模型。程序代码如 $\mathrm{T}$ :

| 66 | #显示空位置 |
| 67 | def availablePositions(self): |
| 68 | positions = [] |
| 69 | for i in range(BOARD ROWS): |
| 70 | for j in range(BOARD COLS): |
| 71 | if self.board[i, jl == 0: |
| 72 | positions.append((i, j)） |
| 73 | return positions |
| 74 |  |
| 75 | #更新棋盘 |
| 76 | def updateState(self, position): |
| 77 | self.board[position] = self.playerSymbol |
| 78 | # switch to another player |
| 79 | self.playerSymbol = -1 if self.playerSymbol == 1 else 1 |
| 80 |  |
| 81 | #给形奖励 |
| 82 | def giveReward(self): |
| 83 | result = self.is done) |
| 84 | # backpropagate reward |
| 85 | if result == 1: #第一玩家赢，P1加一分 |
| 86 | self.p1.feedReward(1) |
| 87 | self.p2.feedReward(o) |
| 88 | elif result == -1:#第玩家赢，P2加一分 |
| 89 | self.p1.feedReward(o) |
| 90 | self.p2.feedReward(1) |
| 91 | else: #胜负未分，第一玩家加0.1分，第玩家加0.5分 |
| 92 | self.p1.feedReward(0.1) |
| 93 | self.p2.feedReward(o.5) |

## (6）棋盘重置。程序代码如下

| 95 | #棋盘重置 |
| 96 | def reset(self): |
| 97 | self.board = np.zeros((BOARD ROWS,B BOARD COLS)） |
| 98 | self.boardHash = None |
| 99 | self.isEnd = False |
| 100 | self.playersymbol 1=1 |

(8）比赛：与训练逻辑类似，差别是玩家要自行输入行动。 程序代码如 $\mathrm{F}$ :

| 102 | #开始训练 |
| 103 | def play(self, rounds=100): |
| 104 | for i in range(rounds): |
| 105 | if i % 1000== 0: |
| 106 | print(f"Rounds {i}") |
| 107 |  |
| 108 | while not self.isEnd: |
| 109 | # PLayer 1 |
| 110 | positions - self.availablePositions() |
| 111 | p1 action - self.pl.chooseAction(positions, |
| 112 | self.board, self.playerSymbol) |
| 113 | # take action and upate board state |
| 114 | self.updateState(pl action) |
| 115 | board hash = self.getHash() |
| 116 | self.pl.addState(board hash) |
| 117 |
| 118 | #检查是否胜负已分 |
| 119 | win - self.is done() |
| 120 |
| 小5 121 | #胜务已分 |
| LLl 199 | 开江以 ifinienA-Nana. |
| 122 汽 | if win is not None: |
| 123 | self.giveReward() |
| 124 | self.p1.reset() |
| 1L4 19C | Sel·PL.eSeL7 A1C |
| LLD 19C | SELI•P2·IESEL cA1fnAcntI  |
| LLO 197 | oelIcoEL hrosL  |
| 127 | break |
| 一一 190 |  C A1  |
| 128 | else: |
| 129 | #PLayer 2 |
| 130 | positions = self.availablePositions() |
| 131 | 厂 p2 action = self.p2.chooseAction(positions, |
| 132 | self.board, self.playerSymbol) |
| 133 | self.updateState(p2 action) |
| 134 | board hash = self.getHash() |
| 135 | self.p2.addState(board hash) |
| 136 |
| 137 | win = self.is done() |
| 138 | if win is not None: |
| 139 | # self.showBoard(O |
| 140 | # ended with p2 either win or draw |
| 141 | self.giveReward() |
| 142 | self.pl.reset() |
| 143 | self.p2.reset() |
| 144 | self.reset() |
| 145 | break |

(8）比赛：与训练逻辑类似，差别是玩家要自行输入行动。

(10）计算机类别：包括计算机依最大值函数行动，比赛结束前存盘，比赛开始前载 $\lambda$ 文件

| 147 | #开始比赛 |
| 148 | def play2(self, start player=1): |
| 149 | is first = True |
| 150 | while not self.isEnd: |
| 151 | # Player 1 |
| 152 | positions = self.availablePositions() |
| 153 | if not (is first and start player==2): |
| 154 | pl action = self.p1.chooseAction(positions, |
| 155 | self.board, self.playerSymbol) |
| 156 | # take action and upate board state |
| 157 | self.updateState(pl action) |
| 158 | is first = False |
| 159 | self.showBoard() |
| 160 | # check board status if it is end |
| 161 | win = self.is done() |
| 162 | if win is not None: |
| 163 | if win = -1 or win == 1: |
| 164 4一 | princ(selT·pi.name,肚!) 1  |
| 1o2 1CC | else: A：+wT千 |
| 167 | self.reset() |
| 1oo 1C0 |  DIEGK 1  |
| 169 | else: |
| 170 | # Player 2 |
| 171 | positions = self.availablePositions() |
| 172 | p2 action = self.p2.chooseAction(positions) |
| 173 |
| 174 | self.updateState(p2 action) |
| 175 | self.showBoard() |
| 176 | win - self.is done() |
| 177 | if win is not None: |
| 178 | if win =- -1 or win == 1: |
| 179 | print(self.p2.name, ”胜!") |
| 180 | else: |
| 181 | print("平手!") |
| 182 | self.reset() |
| 183 | break |

## (9）显示棋盘目前的状态。程序代码如下：

| 185 | #显示棋盘目前状态 |
| 186 | def showBoard(self): |
| 187 | #p1: xp2:0 |
| 188 | for i in range(O, BOARD ROWS): |
| 189 | print('- |
| 190 | out=' |
| 191 | for j in range(O, BOARD COLS): |
| 192 | if self.board[i, jl== 1: |
| 193 | token='x' |
| 194 | if self.board i, jl == -1: |
| 195 | token='o' |
| 196 | if self.board i, jl== 0: |
| 197 | token= |
| 198 | out t= token t |
| 199 | print(out) |
| 200 | print('- |

11) 。程序代码如下初始化。

| 202 | 计算机类别 |
| 203 | ss Player: |
| 204 | def init (self, name, exp rate=0.3): |
| 205 | self.name = name |
| 206 | self.states= [] # record all positions taken |
| 207 | self.lr=0.2 |
| 208 | self.exp rate = exp rate |
| 209 | self.decay gamma = 0.9 |
| 210 | self.states value= {} #state -> value |
| 211 |  |
| 212 | def getHash(self, board): |
| 213 | boardHash = str(board.reshape(BOARD COLS * BOARD ROWS)) |
| 214 | return boardHash |

## 12）计算机依最大值函数行动。程序代码如下

| 216 | #计算机依最大值函数行动 |
| 217 | def chooseAction(self, positions, current board, symbol): |
| 218 | if np.random.uniform(o, 1) <= self.exp rate: |
| 219 | # take random action |
| 220 | idx = np.random.choice(len(positions)) |
| 221 | action = positions[idx] |
| 222 | else: |
| 223 | value max= -999 |
| 224 | for p in positions: |
| 225 | next board = current board.copy() |
| 226 | next board[p] = symbol |
| 227 | next boardHash = self.getHash(next board) |
| 228 | value - o if self.states value.get(next boardHash) is None \ |
| 229 | else self.states value.get(next boardHash) |
| 230 |
| 231 | #依最大值函数行动 |
| 232 | if value >= value max: |
| 233 | value max = value |
| 234 action=p |
| 235 | # print("} takes action }".format(self.name, action)) |
| 236 | return action |

## 13）更新状态值函数。程序代码如下：

| 238 | #更新状态值函数 |
| 239 | def addState(self, state): |
| 240 | self.states.append(state) |
| 241 |  |
| 242 | #重置状态值函数 |
| 243 | def reset(self): |
| 244 | self.states = [] |
| 245 |  |
| 246 | #比赛结荣，倒推状态值函数 |
| 247 | def feedReward(self, reward): |
| 248 | for st in reversed(self.states): |
| 249 | if self.states value.get(st) is None: |
| 250 | self.states value[st]=O |
| 251 | self.states value[st] t= self.lr * (self.decay gamma * reward |
| 252 - self.states value[st]) |
| 253 | reward = self.states valuest] |

14）存盘、载入文件。程序代码如下：

$$
\overline{{\ss}} : \qquad( 1 6 ) \cong\overline{{\ss}} \pm\pm\pm\pm\emptyset
$$
」：说明如何输入位置。程序代码如

| 255 | #存盘 |
| --- | --- |
| 256 | def savePolicy(self): |
| 257 | fw = open(f'policy {self.name}', 'wb') |
| 258 | pickle.dump(self.states value, fw) |
| 259 | fw.close( |
| 260 |
| 261 | #载人文件 |
| 262 | def loadPolicy(self, file): |
| 263 | fr = open(file, 'rb') |
| 264 | self.states value = pickle.load(fr) |
| 265 | fr.close() |

## 15）玩家类别：自行输入行动。程序代码如下：

| 267 | 玩家类别 |
| 268 | lass HumanPlayer: |
| 269 | def init (self, name): |
| 270 | self.name = name |
| 271 |  |
| 272 | #行动 |
| 273 | def chooseAction(self, positions): |
| 274 | while True: |
| 275 | position = int(input("输入位置(1~9):")） |
| 276 | row = position // 3 |
| 277 | col= (position% 3) - 1 |
| 278 | if col<0: |
| 279 | row -= 1 |
| 280 | col=2 |
| 281 | # print(row, col) |
| 282 | action = (row, col) if action in positions: return action  |
| 283 |
| 284 |
| 285 |
| 286 | #状态值函数更新 |
| 287 | def addState(self, state): |
| 288 pass |
| 289 |
| 290 | #比赛结荣，倒推状态值函数 def feedReward(self, reward): pass  |
| 291 |
| 292 |
| 293 |
| 294 | def reset(self): |
| 295 | pass |

$$
\mathrm{T} :
$$

| 297 | # | 可说明输人规则 |
| --- | --- | --- |
| 298 | de | irst draw(): |
| 299 |  | v='\n' |
| 300 |  | o=0 |
| 301 |  | ory in range(3): |
| 302 |  | for x in range(3): |
| 303 |  | idx=y*3tx |
| 304 |  | no+=1 |
| 305 |  | rv t= str(no) |
| 306 |  | ifx<2: |
| 307 |  | rv += |
| 308 |  | rv += '\n" |
| 309 |  | ify < 2: |
| 310 |  | rv += |
| 311 |  | eturn rv |

②提供执行参数 $2$ ，让玩家先下子，否则一律由计算机先下子。指令如 $\mathrm{F}$ :

执行结果：输入位置的号码如下

$$
1 \, | \, 2 \, | \, 3
$$
$$
\cdots
$$
$$
4 \mid5 \mid6
$$
$$
\cdots
$$
$$
7 1 8 1 9
$$

$$
( 1 7 ) ~ \pm\bar{\kappa} \bar{\kappa} \bar{\kappa}_{\circ}
$$

0若已训练过，就不会再训练了，若要重新训练，可将 policy-pl、 policy p2文件删除。

$$
\mathrm{p y t h o n \, t i c T a c T o e. p y} \ 2
$$

$$
\star\Xi\equiv\lfloor\pm\Pi\exists\pm\Pi\rceil\Gamma:
$$

| 313 | if | F name == main"：#主程序 |
| --- | --- | --- |
| 314 |  | import sys |
| 315 |  | if len(sys.argv) > 1: |
| 316 |  | start player = int(sys.argv[1]) |
| 317 |  | else: |
| 318 |  | start player= 1 |
| 319 |  |  |
| 320 |  | #产生对象 |
| 321 |  | p1 = Player("p1") |
| 322 |  | p2 = Player("p2") |
| 323 |  | env = Environment(p1,p2) |
| 324 |  |  |
| 325 |  | #训练 |
| 326 |  | if not os.path.exists(f'policy p{start player}'): |
| 327 |  | print("开始训练...") |
| 328 |  | env.play(50000) |
| 329 |  | p1.savePolicy(O |
| 330 |  | D2.savePolicyO |
| 331 |
| 332 print(first draw()) #棋盘说明 |
| 333 一 |
| 334 |  | #载人训练成果 |
| 335 |  | p1= Player("computer", exp rate=0) |
| 336 |  | p1.loadPolicy(f'policy p{start player}') |
| 337 |  | p2=HumanPlayer("human") |
| 338 |  | env = Environment(p1, p2) |
| 339 |
| 340 |  | #开始比赛 |
| 341 |  | env.play2(start player) |

执行结果：笔者试了几回合，计算机获胜的概率较大。

$$
\begin{array} {l} {{\mathrm{I}}=-\mathrm{I} \quad\mathrm{I} \quad\mathrm{I} \quad\mathrm{I} \quad\quad\mathrm{I} \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\mid\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\
$$

透过 $1 5-4$ 节的实验，我们了解到木棒小车的状态是连续型变量，无法以数组存储所有状态的值函数，而这一节我们就来看看如何使用行动者与评论者（Actor Critic）算法进行大棒小车实验。

Actor Critic 类似于 GAN，主要分为两个神经网络，行动者在评论者的指导 $\mathrm{F}$ ，优化行动决策，而评论者则负责评估行动决策的好坏，并主导值函数模型的参数更新，详细的说明可参阅「Keras 官网说明（httos://keras.io/examples/rl/actor critic cartpole/) $\lfloor\mathcal{Y} .$ 下的程序也来自于该网页。

执行结果：若报酬超过195 $\acute{\jmath}$ ，即停止，表示模型已非常成熟，部分执行结果如下

官网也提供了两段录制的动画，分别为训练初期与后期的比较，可以看出后期的木棒小车已经行驶得相当稳定

## 15-12木棒小车

请参阅程序 $1 5 \bsmile1 8 \bsmile\mathrm{A c t o r \_C r i t i c. p y_{o}}$ 

执行指令：
$$
\mathrm{p y t h o n} ~ 1 5 \b~ 1 8 \b~ \mathrm{A c t o r} \b~ \mathrm{C r i t i c. p y_{o}}
$$

在763回合成功达成目标

running reward: 173.41 at episode 680 H楼丰艺 running reward: 188.73 at episode 700 running reward: 186.98 at episode 710 running reward: 179.31 at episode
running reward: 181.53 at episode 730 running reward: 187.06 at episode 740 running reward: 190.73 at episode 750 running reward: 194.45 at episode 760 Solved at episode 763

(1）训练初期：
$$
\mathrm{h t t p s : / l. i m g u r. c o m / 5 g C s s k H. g i f_{o}}
$$
(2）训练后期：hts:/.ingur.com/sziZUD.gif.

（1）训练初期：
$$
\mathrm{h t t p s : / l. i m g u r. c o m / 5 g C s s k H. g i f_{o}}
$$

(2）训练后期
$$
\mathrm{h t t p s :} / / \mathrm{i. i m g u r. c o m / 5 z i i Z U D. g i f_{o}}
$$

强化学习的应用范围越来越广泛，如自动驾驶车或无人搬运车 (Automated Guided Vehicle, $\mathrm{A G V}$ 都会在不久的将来大行其道，它们都是利用摄像机侦测前方的障碍，但更核心的部分是利用强化学习，来采取最佳行动决策。以无人搬运车为例，我们只要模仿Grid World 游戏的做法，将办公室/工厂/医院平面图制成类似于迷宫的路径，进行仿真训练后，将模型移植到机器 $\mathcal{A}$ ，就可以驱动机器人从 $\mathbf{A}$ 点送货至 $\mathbf{B}$ 点，如图15.41所示。

不仅如此，在股票投资、脑部手术等其他领域，也都看得到强化学习的身影，虽然，其理论较为深奥，且需要较扎实的程序基础，但是，它的好处是不用搜集大量的训练数据，也不需标记数据。

## 15-13 总结

![](figures/1052-3-FIGURE.jpg)

图15.41各厂牌的无人搬运车

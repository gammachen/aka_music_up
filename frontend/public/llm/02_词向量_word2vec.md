### **Word2Vec 模型详解及实际例子说明**

---

#### **1. Word2Vec 的核心思想**
Word2Vec 是一种将单词映射到连续向量空间的无监督学习方法，其核心目标是通过分析文本中单词的上下文信息，学习单词的分布式表示（即词向量）。通过这种方式，具有相似语义或用法的单词在向量空间中会更接近。例如：
- **语义相似性**：如“猫”和“狗”在向量空间中距离较近。
- **语义类比**：如“国王” - “男人” + “女人” ≈ “女王”。

---

#### **2. Word2Vec 的两种模型**
Word2Vec 包含两种核心模型：**CBOW（Continuous Bag of Words）** 和 **Skip-Gram**。它们通过不同的输入输出方式学习词向量。

---

### **2.1 CBOW 模型（连续词袋模型）**
#### **原理**
CBOW 模型通过**上下文单词预测中心词**，即根据目标词的上下文信息推断中心词。其假设是：中心词的语义由上下文单词决定。

#### **结构与流程**
1. **输入**：中心词的上下文单词（窗口内的单词）。
2. **输出**：预测中心词。
3. **计算步骤**：
   - 将上下文单词的词向量求平均或加权平均，得到上下文向量。
   - 通过一个简单的神经网络（隐层和输出层）计算中心词的概率分布。
   - 使用 **Softmax** 层输出每个单词的概率，选择概率最高的单词作为预测结果。

#### **例子**
- **句子**： "我喜欢吃苹果"。
- **窗口大小**：2（中心词为“喜欢”）。
- **上下文单词**：["我", "吃", "苹果"]。
- **CBOW 的任务**：根据“我”、“吃”和“苹果”预测中心词“喜欢”。

#### **数学表示**
- 输入层：上下文单词的 one-hot 编码。
- 隐层：将上下文单词的词向量平均后输入。
- 输出层：通过 Softmax 计算目标词的概率。

---

### **2.2 Skip-Gram 模型**
#### **原理**
Skip-Gram 模型通过**中心词预测上下文单词**，即根据中心词推断其周围可能的上下文单词。其假设是：中心词的上下文单词由中心词的语义决定。

#### **结构与流程**
1. **输入**：中心词。
2. **输出**：预测中心词的上下文单词。
3. **计算步骤**：
   - 将中心词的词向量输入神经网络。
   - 输出层通过 Softmax 计算每个上下文单词的概率分布。
   - 选择概率最高的单词作为预测结果。

#### **例子**
- **句子**： "我喜欢吃苹果"。
- **窗口大小**：2（中心词为“喜欢”）。
- **上下文单词**：["我", "吃", "苹果"]。
- **Skip-Gram 的任务**：根据“喜欢”预测“我”、“吃”和“苹果”。

#### **数学表示**
- 输入层：中心词的 one-hot 编码。
- 隐层：直接使用中心词的词向量。
- 输出层：通过 Softmax 计算上下文单词的概率。

---

### **3. 训练过程与优化**
#### **目标函数**
两种模型的目标都是最大化目标词的条件概率：
- **CBOW**：最大化中心词在上下文下的概率：
  \[
  \max \prod_{t=1}^T P(w_t | \text{context}(w_t))
  \]
- **Skip-Gram**：最大化上下文单词在中心词下的概率：
  \[
  \max \prod_{t=1}^T \prod_{-C \leq j \leq C} P(w_{t+j} | w_t)
  \]

#### **优化方法**
1. **Softmax 的问题**：直接计算 Softmax 的计算复杂度高（词汇量 V 维）。
2. **解决方案**：
   - **负采样（Negative Sampling）**：仅对一小部分样本（真实上下文和随机负样本）进行更新。
   - **层次 softmax**：利用霍夫曼树加速计算。

---

### **4. 实际例子：词向量的语义关系**
#### **例子 1：语义相似性**
假设训练后的词向量：
- **"国王"** 和 **"王后"** 的向量距离相近。
- **"苹果（水果）"** 和 **"香蕉"** 的向量距离相近。
- **"苹果（公司）"** 和 **"三星"** 的向量距离相近。

#### **例子 2：词类比任务**
通过向量运算验证语义关系：
- **"国王" - "男人" + "女人" ≈ "王后"**：
  \[
  \text{vec("国王")} - \text{vec("男人")} + \text{vec("女人")} \approx \text{vec("王后")}
  \]

#### **例子 3：词向量应用**
- **文本分类**：将句子表示为词向量的平均值，输入分类模型。
- **推荐系统**：根据用户历史词向量推荐相关词汇。

---

### **5. 优缺点对比**
#### **CBOW vs. Skip-Gram**
| **特性**         | **CBOW**                          | **Skip-Gram**                     |
|-------------------|-----------------------------------|-----------------------------------|
| **适用场景**      | 大数据集，计算效率高              | 小数据集，适合低频词              |
| **训练速度**      | 快                               | 慢                               |
| **语义捕捉**      | 更好地捕捉全局语义（如高频词）    | 更好地捕捉局部语义（如低频词）    |

#### **Word2Vec 的局限性**
1. **固定词向量**：无法处理一词多义（如“苹果”作为水果和公司）。
2. **无法处理 OOV（未登录词）**：未在训练数据中出现的单词无法得到向量。
3. **依赖上下文窗口**：长距离依赖关系建模能力有限。

---

### **6. 实现示例（使用 Gensim 库）**
```python
from gensim.models import Word2Vec
import jieba

# 示例语料库
sentences = [
    "我喜欢吃苹果",
    "苹果公司发布新手机",
    "香蕉是水果的一种",
    "国王和王后统治国家"
]

# 分词处理
corpus = [list(jieba.cut(sentence)) for sentence in sentences]

# 训练 Word2Vec 模型（CBOW）
model = Word2Vec(
    corpus,
    vector_size=100,  # 词向量维度
    window=5,         # 上下文窗口大小
    min_count=1,      # 最小词频
    sg=0              # 0 表示 CBOW，1 表示 Skip-Gram
)

# 获取词向量
apple_fruit_vector = model.wv["苹果"]  # 水果“苹果”的向量
apple_company_vector = model.wv["苹果"]  # 公司“苹果”的向量（可能与水果向量相近，因无法区分多义词）

# 语义相似性
print(model.wv.most_similar("国王"))  # 输出与“国王”最相似的词，如“王后”、“皇帝”等

# 词类比任务
result = model.wv.most_similar(positive=['国王', '女人'], negative=['男人'])
print(result[0][0])  # 输出可能为“王后”
```

---

### **7. 总结**
- **核心思想**：通过上下文信息学习词向量，捕捉语义和语法关系。
- **模型选择**：CBOW 适合大数据集，Skip-Gram 适合小数据集或低频词。
- **应用场景**：词相似度计算、文本分类、推荐系统等。
- **局限性**：固定向量无法处理多义词，需依赖后续模型（如 ELMo、BERT）改进。

通过实际例子和代码示例，可以直观理解 Word2Vec 如何将单词映射到向量空间，并捕捉其语义关系。
### 使用性能剖析

当获得服务器或查询的剖析报告后，如何使用这些报告？好的剖析报告能够将潜在的问题显示出来，但最终的解决方案还需要用户来决定（尽管报告可能会给出建议）。优化查询时，用户需要对服务器如何执行查询有较深的了解。剖析报告能够尽可能多地收集需要的信息、给出诊断问题的正确方向，以及为其他诸如 `EXPLAIN` 等工具提供基础信息。

#### **其他可能性**

设想一下正在分析慢查询日志，发现了一个很简单且通常情况下都非常快的查询，但有几次非常不合理地执行了很长时间。手工重新执行一遍，发现也非常快，然后使用 `EXPLAIN` 查询其执行计划，也正确地使用了索引。然后尝试修改 `WHERE` 条件中使用不同的值，以排除缓存命中的可能，也没有发现有什么问题，这可能是什么原因呢？

#### **诊断间歇性问题**

间歇性的问题，比如系统偶尔停顿或者慢查询，很难诊断。有些幻影问题只在没有注意到的时候才发生，而且无法确认如何重现，诊断这样的问题往往要花费很多时间，有时候甚至需要好几个月。在这个过程中，有些人会尝试以不断试错的方式来诊断，有时候甚至会想要通过随机地改变一些服务器的设置来侥幸地找到问题。

**尽量不要使用试错的方式来解决问题**：
- 这种方式有很大的风险，因为结果可能变得更坏。
- 这也是一种令人沮丧且低效的方式。
- 如果一时无法定位问题，可能是测量的方式不正确，或者测量的点选择有误，或者使用的工具不合适。

#### **单条查询问题还是服务器问题**

发现问题的蛛丝马迹了吗？如果有，则首先要确认这是单条查询的问题，还是服务器的问题。这将为解决问题指出正确的方向。

- **服务器问题**：
  - 如果服务器上所有的程序都突然变慢，又突然都变好，每一条查询也都变慢了，那么慢查询可能就不一定是原因，而是由于其他问题导致的结果。
  - 服务器的问题非常常见。在过去几年，硬件的能力越来越强，配置16核或者更多CPU的服务器成了标配，MySQL在SMP架构的机器上的可扩展性限制也就越来越显露出。
  - 尤其是较老的版本，其问题更加严重，而目前生产环境中的老版本还非常多。新版本MySQL依然也还有一些扩展性限制，但相比老版本已经没有那么严重，而且出现的频率相对小很多，只是偶尔能碰到。这是好消息，也是坏消息：好消息是很少会碰到这个问题；坏消息则是一旦碰到，则需要对MySQL内部机制更加了解才能诊断出来。当然，这也意味着很多问题可以通过升级到MySQL新版本来解决。

- **单条查询问题**：
  - 如果服务器整体运行没有问题，只有某条查询偶尔变慢，就需要将注意力放到这条特定的查询上面。

#### **如何判断是单条查询问题还是服务器问题**

如果问题不停地周期性出现，那么可以在某次活动中观察到，或者整夜运行脚本收集数据，第二天来分析结果。大多数情况下都可以通过三种技术来解决，下面将一一道来。

##### **使用 `SHOW GLOBAL STATUS`**

这个方法实际上就是以较高的频率（比如一秒执行一次）`SHOW GLOBAL STATUS` 命令捕获数据，问题出现时，可以通过某些计数器（比如 `Threads_running`、`Threads_connected`、`Questions` 和 `Queries`）的“尖刺”或者“凹陷”来发现。这个方法比较简单，所有人都可以使用（不需要特殊的权限），对服务器的影响也很小，所以是一个花费时间不多却能很好地了解问题的好方法。

**示例命令及其输出**：
每秒捕获一次 `SHOW GLOBAL STATUS` 的数据，输出给 `awk` 计算并输出每秒的查询数、`Threads_connected` 和 `Threads_running`（表示当前正在执行查询的线程数）。这三个数据的趋势对于服务器级别偶尔停顿的敏感性很高。一般发生此类问题时，根据原因的不同和应用连接数据库方式的不同，每秒的查询数一般会下跌，而其他两个则至少有一个会出现尖刺。在这个例子中，应用使用了连接池，所以 `Threads_connected` 没有变化。但正在执行查询的线程数明显上升，同时每秒的查询数相比正常数据有严重的下跌。

**如何解析这个现象**：
- 凭猜测有一定的风险。
- 在实践中，有两个原因的可能性比较大：
  1. 服务器内部碰到了某种瓶颈，导致新查询在开始执行前因为需要获取老查询正在等待的锁而造成堆积。这一类的锁一般也会对应用服务器造成后端压力，使得应用服务器也出现排队问题。
  2. 服务器突然遇到了大量查询请求的冲击，比如前端的 `memcached` 突然失效导致的查询风暴。

**示例命令**：
```bash
mysql -e 'SHOW GLOBAL STATUS' | awk '
BEGIN { 
    prev_questions = 0; 
    prev_queries = 0; 
    prev_threads_running = 0; 
    prev_threads_connected = 0; 
}
NR == 1 { 
    prev_questions = $2; 
    prev_queries = $4; 
    prev_threads_running = $6; 
    prev_threads_connected = $8; 
}
NR > 1 { 
    questions = $2; 
    queries = $4; 
    threads_running = $6; 
    threads_connected = $8; 
    printf "%s\t%d\t%d\t%d\t%d\n", strftime("%Y-%m-%d %H:%M:%S"), questions - prev_questions, threads_connected - prev_threads_connected, threads_running - prev_threads_running, queries - prev_queries; 
    prev_questions = questions; 
    prev_queries = queries; 
    prev_threads_running = threads_running; 
    prev_threads_connected = threads_connected; 
}' -s 1
```

**解析**：
- 这个命令每秒输出一行数据，可以运行几个小时或者几天，然后将结果绘制成图形，这样就可以方便地发现是否有趋势的突变。
- 如果问题确实是间歇性的，发生的频率又较低，也可以根据需要尽可能长时间地运行此命令，直到发现问题再回头来看输出结果。大多数情况下，通过输出结果都可以更明确地定位问题。

##### **使用 `SHOW PROCESSLIST`**

这个方法是通过不停地捕获 `SHOW PROCESSLIST` 的输出，来观察是否有很多线程处于不正常的状态或者有其他不正常的特征。例如查询很少会长时间处于“statistics”状态，这个状态一般是指服务器在查询优化阶段如何确定表关联的顺序——通常都是非常快的。另外，也很少会见到大量线程报告当前连接用户是“未经验证的用户（Unauthenticated user）”，这只是在连接握手的中间过程中的状态，当客户端等待输入用于登录的用户信息的时候才会出现。

**使用 `SHOW PROCESSLIST` 命令时，在尾部加上 `\G` 可以垂直的方式输出结果**：
```bash
mysql -e 'SHOW PROCESSLIST\G' | grep State: | sort | uniq -c | sort -rn
```

**如果要查看不同的列，只需要修改 `grep` 的模式即可**。在大多数案例中，`State` 列都非常有用。从这个例子的输出中可以看到，有很多线程处于查询执行的结束部分的状态，包括“freeing items”、“end”、“cleaning up” 和“logging slow query”。事实上，在案例中的这台服务器上，同样模式或类似的输出采样出现了很多次。大量的线程处于“freeing items”状态是出现了大量有问题查询的很明显的特征和指示。

**用这种技术查找问题**：
- 上面的命令行不是唯一的办法。如果 MySQL 服务器的版本较新，也可以直接查询 `INFORMATION_SCHEMA` 中的 `PROCESSLIST` 表；或者使用 `innotop` 工具以较高的频率刷新，以观察屏幕上出现的不正常查询堆积。
- 上面演示的这个例子是由于 `InnoDB` 内部的争用和脏块刷新所导致，但有时候原因可能比这个要简单得多。一个经典的例子是很多查询处于“Locked”状态，这是 `MyISAM` 的一个典型问题，它的表级别锁定，在写请求较多时，可能迅速导致服务器级别的线程堆积。

**要注意找到吞吐量突然下降时间段的日志**：
- 查询是在完成阶段才写入到慢查询日志的，所以堆积会造成大量查询处于完成阶段，直到阻塞其他查询的资源占用者释放资源后，其他的查询才能执行完成。
- 这种行为特征的一个好处是，当遇到吞吐量突然下降时，可以归咎于吞吐量下降后完成的第一个查询（有时候也不一定是第一个查询）。当某些查询被阻塞时，其他查询可以不受影响继续运行，所以不能完全依赖这个经验。

**再重申一次，好的工具可以帮助诊断这类问题，否则要人工去几百GB的查询日志中找原因**。

**从上面的输出可以看到有吞吐量突然下降的情况发生，而且在下降之前还有一个突然的高峰**：
- 仅从这个输出而不去查询当时的详细信息很难确定发生了什么，但应该可以说这个突然的高峰和随后的下降一定有关联。
- 不管怎么说，这种现象都很奇怪，值得去日志中挖掘该时间段的详细信息（实际上通过日志的详细信息，可以发现突然的高峰时段有很多连接被断开的现象，可能是有一台应用服务器重启导致的。所以不是所有的问题都是 MySQL 的问题）。

#### **理解发现的问题 (Making sense of the findings)**

可视化的数据最具有说服力。

#### **捕获诊断数据**

当出现间歇性问题时，需要尽可能多地收集所有数据，而不只是问题出现时的数据。虽然这样会收集大量的诊断数据，但总比真正能够诊断问题的数据没有被收集到的情况要好。

在开始之前，需要搞清楚两件事：
1. 一个可靠且实时的“触发器”，也就是能区分什么时候问题出现的方法。
2. 一个收集诊断数据的工具。

##### **诊断触发器**

触发器非常重要。这是在问题出现时能够捕获数据的基础。有两个常见的问题可能导致无法达到预期的结果：误报（false positive）或者漏检（false negative）。误报是指收集了很多诊断数据，但期间其实没有发生问题，这可能浪费时间，而且令人沮丧。而漏检则指在问题出现时没有捕获到数据，错失了机会，一样地浪费时间。所以在开始收集数据前多花一点时间来确认触发器能够真正地识别问题是划算的。

**好的触发器的标准是什么**：
- 像前面的例子展示的，`Threads_running` 的趋势在出现问题时会比较敏感，而没有问题时则比较平稳。
- 另外 `SHOW PROCESSLIST` 中线程的异常状态尖峰也是个不错的指标。
- 关键是找到一些能和正常时的阈值进行比较的指标。通常情况下这是一个计数，比如正在运行的线程的数量、处于“freeing items”状态的线程的数量等。
- 当要计算线程某个状态的数量时，`grep` 的 `-c` 选项非常有用：
  ```bash
  mysql -e 'SHOW PROCESSLIST\G' | grep "State: freeing items" -c
  ```

**选择一个合适的阈值很重要**：
- 既要足够高，以确保在正常时不会被触发；又不能太高，要确保问题发生时不会错过。
- 另外要注意，要在问题开始时就捕获数据，就更不能将阈值设置得太高。问题持续上升的趋势一般会导致更多的问题发生，如果在问题导致系统快要崩溃时才开始捕获数据，就很难诊断到最初的根本原因。如果可能，在问题还是涓涓细流的时候就要开始收集数据，而不要等到波涛汹涌才开始。
- 举个例子，`Threads_connected` 偶尔出现非常高的尖峰值，在几分钟时间内会从100冲到5000或者更高，所以设置阈值为4999也可以捕获到问题，但为什么非要等到这么高的时候才收集数据呢？如果在正常时该值一般不超过150，将阈值设置为200或者300会更好。

**回到前面关于 `Threads_running` 的例子**：
- 正常情况下，并发度不超过10。但是阈值设置为10并不是一个好注意，很可能会导致很多误报。即使设置为15也不够，可能还是会有很多正常的波动会到这个范围。当并发运行线程到15的时候可能也会有少量堆积的情况，但可能还没到问题的引爆点。但也应该在糟糕到一眼就能看出问题前就清晰地识别出来，对于这个例子，我们建议阀值可以设置为20。

**我们当然希望在问题确实发生时能捕获到数据，但有时候也需要稍微等待一下以确保不是误报或者短暂的尖峰**：
- 所以，最后的触发条件可以这样设置：每秒监控状态值，如果 `Threads_running` 连续5秒超过20，就开始收集诊断数据（顺便说一句，我们的例子中问题只持续了3秒就消失了，这是为了使例子简单而设置的。3秒的故障不容易诊断，而我们碰到过的大部分问题持续时间都会更长一些）。
- 所以我们需要利用一种工具来监控服务器，当达到触发条件时能收集数据。当然可以自己编写脚本来实现，不过不用那么麻烦，Percona Toolkit 中的 `pt-stalk` 就是为这种情况设计的。这个工具有很多有用的特性，只要碰到过类似问题就会明白这些特性的必要性。例如，它会监控磁盘的可用空间，所以不会因为收集太多的数据将空间耗尽而导致服务器崩溃。如果之前碰到过这样的情况，你就会理解这一点了。

##### **需要收集什么样的数据**

现在已经确定了诊断触发器，可以开始启动一些进程来收集数据了。但需要收集什么样的数据呢？就像前面说的，答案是尽可能收集所有能收集的数据，但只在需要的时间段内收集。包括系统的状态、CPU利用率、磁盘使用率和可用空间、`ps` 的输出采样、内存利用率，以及可以从 MySQL 获得的信息，如 `SHOW STATUS`、`SHOW PROCESSLIST` 和 `SHOW INNODB STATUS`。这些在诊断问题时都需要用到（可能还会有一些更多）。

**执行时间包括用于工作的时间和等待的时间**：
- 当一个未知问题发生时，一般来说有两种可能：服务器需要做大量的工作，从而导致大量消耗CPU，或者在等待某些资源被释放。
- 所以需要用不同的方法收集诊断数据，来确认是何种原因：剖析报告用于确认是否有太多工作，而等待分析则用于确认是否存在大量等待。如果是未知的问题，怎么知道将精力集中在哪个方面呢？没有更好的办法，所以只能两种数据都尽量收集。

---

### **总结**

通过上述方法，可以有效地诊断和解决间歇性问题。以下是关键点总结：

1. **确认问题类型**：
   - 单条查询问题还是服务器问题。

2. **使用 `SHOW GLOBAL STATUS`**：
   - 以较高的频率捕获数据，通过某些计数器的“尖刺”或“凹陷”来发现问题。

3. **使用 `SHOW PROCESSLIST`**：
   - 观察是否有很多线程处于不正常的状态或有其他不正常的特征。
   - 使用 `grep` 和 `sort` 命令来计算某个列值出现的次数。

4. **诊断触发器**：
   - 确保触发器能够真正地识别问题。
   - 设置合适的阈值，既要足够高以确保在正常时不会被触发，又不能太高以确保问题发生时不会错过。

5. **收集诊断数据**：
   - 尽可能多地收集所有数据，包括系统的状态、CPU利用率、磁盘使用率和可用空间、`ps` 的输出采样、内存利用率等。
   - 使用 `pt-stalk` 等工具来监控服务器，当达到触发条件时能收集数据。

通过这些方法，可以更有效地诊断和解决间歇性问题，确保服务器的稳定性和性能。
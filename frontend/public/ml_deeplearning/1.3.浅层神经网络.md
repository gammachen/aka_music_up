# 神经网络概述
## 逻辑回顾模型：
1. 输入特征数据；
2. 通过激活函数；
3. 输出预测标签数据；
<img src="../images/L1_week3_1.png">

## 逻辑回归vs多元线性回归
逻辑回归模型$\Leftrightarrow$多元线性回归模型输出端增加一个sigmod函数。

## 逻辑回归前向传播步骤：
1. 输入数据、特征权重、截距项；
2. 多元线性回归函数；
3. sigmod激活函数；
4. 损失函数loss function

$$
\left.
	\begin{array}{l}
	x\\
	w\\
	b
	\end{array}
	\right\}
	\implies{z={w}^Tx+b}
	\implies{\alpha = \sigma(z)}
	\implies{{L}(a,y)}
$$

## 神经网络模型
1. 神经网络有多个类似逻辑回归的单元组合而成；
2. 每个节点就是多元线性回归+类似sigmod函数的混合函数；
<img src="../images/L1_week3_2.png">

### 前向传播
第一层节点计算公式：
$$
\left.
	\begin{array}{r}
	{x }\\
	{W^{[1]}}\\
	{b^{[1]}}
	\end{array}
	\right\}
	\implies{z^{[1]}=W^{[1]}x+b^{[1]}}
	\implies{a^{[1]} = \sigma(z^{[1]})}
$$
第二层节点计算公式：
$$
\left.
	\begin{array}{r}
	\text{$a^{[1]} = \sigma(z^{[1]})$}\\
	\text{$W^{[2]}$}\\
	\text{$b^{[2]}$}\\
	\end{array}
	\right\}
	\implies{z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}}
	\implies{a^{[2]} = \sigma(z^{[2]})}
	\implies{{L}\left(a^{[2]},y \right)}
$$
### 反向传播
第一层节点计算公式：
$$
\left.
	\begin{array}{r}
	{x}\\
	{d}W^{[1]}\\
	{d}b^{[1]}
	\end{array}
	\right\}
	\impliedby{{d}z^{[1]}=d(W^{[1]}x+b^{[1]})}
	\impliedby{{d}\alpha^{[1]} = {d}\sigma(z^{[1]})}
$$
第二层节点计算公式：
$$
\left.
	\begin{array}{r}
	{da^{[1]} = {d}\sigma(z^{[1]})}\\
	dW^{[2]}\\
	db^{[2]}\\
	\end{array}
	\right\}
	\impliedby{dz}^{[2]}={d}(W^{[2]}\alpha^{[1]}+b^{[2]})
	\impliedby{{da}^{[2]} = {d}\sigma(z^{[2]})}
	\impliedby{{dL}\left(a^{[2]},y \right)}
$$

# 神经网络的表示
<img src="../images/L1_week3_3.png">

1. 输入层；
2. 隐藏层；
3. 输出层；

我们再引入几个符号，就像我们之前用向量$x$表示输入特征。这里有个可代替的记号$a^{[0]}$可以用来表示输入特征。$a$表示激活的意思，它意味着网络中不同层的值会传递到它们后面的层中，输入层将$x$传递给隐藏层，所以我们将输入层的激活值称为$a^{[0]}$；下一层即隐藏层也同样会产生一些激活值，那么我将其记作$a^{[1]}$，所以具体地，这里的第一个单元或结点我们将其表示为$a^{[1]}_{1}$，第二个结点的值我们记为$a^{[1]}_{2}$以此类推。所以这里的是一个四维的向量如果写成**Python**代码，那么它是一个规模为4x1的矩阵或一个大小为4的列向量，如下公式，它是四维的，因为在本例中，我们有四个结点或者单元，或者称为四个隐藏层单元；

$$
a^{[1]} =
	\left[
		\begin{array}{ccc}
		a^{[1]}_{1}\\
		a^{[1]}_{2}\\
		a^{[1]}_{3}\\
		a^{[1]}_{4}
		\end{array}
		\right]
$$

最后输出层将产生某个数值$a$，它只是一个单独的实数，所以的$\hat{y}$值将取为$a^{[2]}$。这与逻辑回归很相似，在逻辑回归中，我们有$\hat{y}$直接等于$a$，在逻辑回归中我们只有一个输出层，所以我们没有用带方括号的上标。但是在神经网络中，我们将使用这种带上标的形式来明确地指出这些值来自于哪一层。

<img src="../images/L1_week3_4.png">

最后，我们要看到的隐藏层以及最后的输出层是带有参数的，这里的隐藏层将拥有两个参数$W$和$b$，给它们加上上标$^{[1]}$($W^{[1]}$,$b^{[1]}$)，表示这些参数是和第一层这个隐藏层有关系的。

# 计算一个神经网络的输出
$x$表示输入特征，$a$表示每个神经元的输出，$W$表示特征的权重，上标表示神经网络的层数（隐藏层为1），下标表示该层的第几个神经元。这是神经网络的**符号惯例**。
## 神经网络的计算
<img src="../images/L1_week3_6.png">

两层的神经网络，我们从隐藏层的第一个神经元开始计算，神经元的计算与逻辑回归一样分为两步，小圆圈代表了计算的两个步骤。

第一步，计算$z^{[1]}_1,z^{[1]}_1 = w^{[1]T}_1x + b^{[1]}_1$。

第二步，通过激活函数计算$a^{[1]}_1,a^{[1]}_1 = \sigma(z^{[1]}_1)$。

隐藏层的第二个以及后面两个神经元的计算过程一样，只是注意符号表示不同，最终分别得到$a^{[1]}_2、a^{[1]}_3、a^{[1]}_4$，详细结果见下:

$z^{[1]}_1 = w^{[1]T}_1x + b^{[1]}_1, a^{[1]}_1 = \sigma(z^{[1]}_1)$

$z^{[1]}_2 = w^{[1]T}_2x + b^{[1]}_2, a^{[1]}_2 = \sigma(z^{[1]}_2)$

$z^{[1]}_3 = w^{[1]T}_3x + b^{[1]}_3, a^{[1]}_3 = \sigma(z^{[1]}_3)$

$z^{[1]}_4 = w^{[1]T}_4x + b^{[1]}_4, a^{[1]}_4 = \sigma(z^{[1]}_4)$
## 向量化计算
向量化推导：
$$
a^{[1]} =
	\left[
		\begin{array}{c}
		a^{[1]}_{1}\\
		a^{[1]}_{2}\\
		a^{[1]}_{3}\\
		a^{[1]}_{4}
		\end{array}
		\right]
		= \sigma(z^{[1]})
$$

$$
\left[
		\begin{array}{c}
		z^{[1]}_{1}\\
		z^{[1]}_{2}\\
		z^{[1]}_{3}\\
		z^{[1]}_{4}\\
		\end{array}
		\right]
		 =
	\overbrace{
	\left[
		\begin{array}{c}
		...W^{[1]T}_{1}...\\
		...W^{[1]T}_{2}...\\
		...W^{[1]T}_{3}...\\
		...W^{[1]T}_{4}...
		\end{array}
		\right]
		}^{W^{[1]}}
		*
	\overbrace{
	\left[
		\begin{array}{c}
		x_1\\
		x_2\\
		x_3\\
		\end{array}
		\right]
		}^{input}
		+
	\overbrace{
	\left[
		\begin{array}{c}
		b^{[1]}_1\\
		b^{[1]}_2\\
		b^{[1]}_3\\
		b^{[1]}_4\\
		\end{array}
		\right]
		}^{b^{[1]}}
$$
向量化公式：

$z^{[n]} = w^{[n]}x + b^{[n]}$

$a^{[n]}=\sigma(z^{[n]})$

## 总结

至此我们能够根据给出的一个单独的输入特征向量即一个样本，运用四行代码，两个迭代，计算出一个简单神经网络的输出。接下来我们实现，一次性计算整个训练集的输出。

# 多样本向量化
## 多样本
<img src="../images/L1_week3_8.png">

对于一个给定的输入特征向量$X$，这四个等式可以计算出$\alpha^{[2]}$等于$\hat{y}$。如果有$m$个训练样本,那么就需要重复这个过程。

用第一个训练样本$x^{(1)}$来计算出预测值$\hat{y}^{(1)}$，就是第一个训练样本上得出的结果。

然后，用$x^{(2)}$来计算出预测值$\hat{y}^{(2)}$，循环往复，直至用$x^{(m)}$计算出$\hat{y}^{(m)}$。

用激活函数表示法，如上图左下所示，它写成$a^{[2](1)}$、$a^{[2](2)}$和$a^{[2](m)}$。

【注】：$a^{[2](i)}$，$(i)$是指第$i$个训练样本而$[2]$是指第二层。

如果有一个非向量化形式的实现，而且要计算出它的预测值，对于所有训练样本，需要让$i$从1到$m$实现这四个等式：

$z^{[1](i)}=W^{[1](i)}x^{(i)}+b^{[1](i)}$

$a^{[1](i)}=\sigma(z^{[1](i)})$

$z^{[2](i)}=W^{[2](i)}a^{[1](i)}+b^{[2](i)}$

$a^{[2](i)}=\sigma(z^{[2](i)})$
## 向量化
### 特征矩阵
$$
X =
	\left[
		\begin{array}{c}
		\vdots & \vdots & \vdots & \vdots\\
		x^{(1)} & x^{(2)} & \cdots & x^{(m)}\\
		\vdots & \vdots & \vdots & \vdots\\
		\end{array}
		\right]
$$
## Z值
$$
Z^{[1]} =
	\left[
		\begin{array}{c}
		\vdots & \vdots & \vdots & \vdots\\
		z^{[1](1)} & z^{[1](2)} & \cdots & z^{[1](m)}\\
		\vdots & \vdots & \vdots & \vdots\\
		\end{array}
		\right]
$$
## A值
$$
A^{[1]} =
	\left[
		\begin{array}{c}
		\vdots & \vdots & \vdots & \vdots\\
		\alpha^{[1](1)} & \alpha^{[1](2)} & \cdots & \alpha^{[1](m)}\\
		\vdots & \vdots & \vdots & \vdots\\
		\end{array}
		\right]
$$
## 下一步
$$
\left.
		\begin{array}{r}
		\text{$z^{[1](i)} = W^{[1](i)}x^{(i)} + b^{[1]}$}\\
		\text{$\alpha^{[1](i)} = \sigma(z^{[1](i)})$}\\
		\text{$z^{[2](i)} = W^{[2](i)}\alpha^{[1](i)} + b^{[2]}$}\\
		\text{$\alpha^{[2](i)} = \sigma(z^{[2](i)})$}\\
		\end{array}
		\right\}
		\implies
		\begin{cases}
		\text{$A^{[1]} = \sigma(z^{[1]})$}\\
		\text{$Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$}\\ 
		\text{$A^{[2]} = \sigma(Z^{[2]})$}\\ 
		\end{cases}
$$
## 总结
对于$Z^{[i]}、A^{[i]}$:
1. 矩阵的水平索引对应当前层不同的样本；
2. 矩阵的垂直索引对应当前层不同的节点（神经元）;

# 向量化实现的解释
例：

$z^{[1](1)} = W^{[1]}x^{(1)} + b^{[1]}$

$z^{[1](2)} = W^{[1]}x^{(2)} + b^{[1]}$

$z^{[1](3)} = W^{[1]}x^{(3)} + b^{[1]}$

向量化表示：

$$
W^{[1]}  x =
		\left[
		\begin{array}{ccc}
		\cdots \\
		\cdots \\
		\cdots \\
		\end{array}
		\right]		
		\left[
		\begin{array}{c}
		\vdots &\vdots & \vdots & \vdots \\
		x^{(1)} & x^{(2)} & x^{(3)} & \vdots\\
		\vdots &\vdots & \vdots & \vdots \\
		\end{array}
		\right]
		=
		\left[
		\begin{array}{c}
		\vdots &\vdots & \vdots & \vdots \\
		w^{(1)}x^{(1)} & w^{(1)}x^{(2)} & w^{(1)}x^{(3)} & \vdots\\
		\vdots &\vdots & \vdots & \vdots \\
		\end{array}
		\right]
		=\\
		\left[
		\begin{array}{c}
		\vdots &\vdots & \vdots & \vdots \\
		z^{[1](1)} & z^{[1](2)} & z^{[1](3)} & \vdots\\
		\vdots &\vdots & \vdots & \vdots \\
		\end{array}
		\right]
		=
		Z^{[1]}
$$

# 激活函数
使用一个神经网络时，需要决定使用哪种激活函数用隐藏层上，哪种用在输出节点上。到目前为止，我们只用过sigmoid激活函数，但是，有时其他的激活函数效果会更好。

<img src="../images/L1_week3_9.jpg">

通常的情况下，使用不同的函数$g( z^{[1]})$，其中$g$指的是除了**sigmoid**函数以外的非线性函数。**tanh**函数即双曲正切函数是总体上都优于**sigmoid**函数的激活函数。

如图，$a = tanh(z)$的值域是位于+1和-1之间。
tanh公式：
$\large a= tanh(z) = \frac{e^{z} - e^{- z}}{e^{z} + e^{- z}}$

**sigmoid**函数和**tanh**函数两者共同的缺点是，在$z$特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于0，导致降低梯度下降的速度。

在机器学习另一个很流行的函数是：修正线性单元的函数（**ReLu**）
<img style="height:50%;width:50%" src="../images/155-ReLU激活函数.png"/>

$a=ReLU(z)=max( 0,z)$



















选择激活函数的经验法则：
>如果输出是0、1值（二分类问题），则输出层选择**sigmoid**函数，然后其它的所有单元都选择**Relu**函数。

这里也有另一个改进版本的**Relu**被称为**Leaky Relu**。

**Relu**和**Leaky Relu**的优点是：

第一，$ReLU$的使用ifelse判断，导数几乎恒定，而**sigmoid**函数需要进行浮点四则运算，在实践中，使用**ReLu**激活函数神经网络通常会比使用**sigmoid**或者**tanh**激活函数学习的更快。

第二，**sigmoid**和**tanh**函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度弥散，而**Relu**和**Leaky ReLu**函数大于0部分都为常熟，不会产生梯度弥散现象。(注：**Relu**进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性，而**Leaky ReLu**不会有这问题)

概括一下不同激活函数的过程和结论。

**sigmoid**激活函数：除了输出层是一个二分类问题基本不会用它。

**tanh**激活函数：**tanh**是非常优秀的，几乎适合所有场合。

**ReLu**激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用**ReLu**或者**Leaky ReLu**。

# 为什么需要非线性激活函数？
如果我们改变前面的式子，令：

(1) $a^{[1]} = z^{[1]} = W^{[1]}x + b^{[1]}$

(2) $a^{[2]} = z^{[2]} = W^{[2]}a^{[1]}+ b^{[2]}$
将式子(1)代入式子(2)中，则：
$a^{[2]} = z^{[2]} = W^{[2]}(W^{[1]}x + b^{[1]}) + b^{[2]}$

(3) $a^{[2]} = z^{[2]} = W^{[2]}W^{[1]}x + W^{[2]}b^{[1]} + b^{[2]}$
简化多项式得
$a^{[2]} = z^{[2]} = W^{'}x + b^{'}$

所以说，如果你是用线性激活函数或者叫恒等激励函数，那么神经网络只是把输入线性组合再输出。

# 激活函数的导数
在神经网络中使用反向传播的时候，需要计算激活函数的斜率或者导数。针对以下四种激活，求其导数如下：
## **sigmoid激活函数的导数**
<img src="../images/L1_week3_10.png">

求导如下：

$\frac{d}{dz}g(z) =-1\times{(\frac{1}{1 + e^{-z}})}^2\times e^{-z}\times -1 ={\frac{1}{1 + e^{-z}} (1-\frac{1}{1 + e^{-z}})}=g(z)(1-g(z))$
## **tanh激活函数**
<img src="../images/L1_week3_11.png">

求导如下：

$g(z) = tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$

$\frac{d}{{d}z}g(z) = 1 - (tanh(z))^{2}$
## **修正线性单元(ReLU)激活函数**
<img src="../images/L1_week3_12.png">
求导如下：

$$g(z) =max (0,z)$$

$$
g(z)^{'}=
  \begin{cases}
  0&if(z \leq 0)\\
  1&if(z > 0)
\end{cases}
$$
## **Leaky ReLU激活函数**

与**ReLU**类似
$$
g(z)=\max(0.01z,z) \\
g(z)^{'}=
\begin{cases}
0.01&{if(z \leq 0)}\\
1&{if(z > 0)}\\
\end{cases}
$$

# 神经网络的梯度下降
接下来，实现反向传播或者说梯度下降算法的方程组。
## 参数
单隐层神经网络会有$W^{[1]}$，$b^{[1]}$，$W^{[2]}$，$b^{[2]}$这些参数，还有个$n_x$表示输入特征的个数，$n^{[1]}$表示隐藏单元个数，$n^{[2]}$表示输出单元个数。
## 代价函数（成本函数）
公式：
$J(W^{[1]},b^{[1]},W^{[2]},b^{[2]}) = {\frac{1}{m}}\sum_{i=1}^mL(\hat{y}, y)$
## 正向传播方程
(1)$Z^{[i]} = W^{[i]}A^{[i-1]} + b^{[i]}$

(2)$A^{[i]} = g^{[i]}(Z^{[i]})$
## 反向传播方程

$dZ^{[2]} = A^{[2]} - Y , Y = \begin{bmatrix}y^{[1]} & y^{[2]} & \cdots & y^{[m]}\\ \end{bmatrix}$

$dW^{[2]} = {\frac{1}{m}}dZ^{[2]}A^{[1]T}$

${\rm d}b^{[2]} = {\frac{1}{m}}np.sum({d}Z^{[2]},axis=1,keepdims=True)$

$dZ^{[1]} = \underbrace{W^{[2]T}{\rm d}Z^{[2]}}_{(n^{[1]},m)}\quad*\underbrace{d{g^{[1]}}}_{activation \; function \; of \; hidden \; layer}*\quad\underbrace{(Z^{[1]})}_{(n^{[1]},m)}$

$dW^{[1]} = {\frac{1}{m}}dZ^{[1]}x^{T}$

${\underbrace{db^{[1]}}_{(n^{[1]},1)}} = {\frac{1}{m}}np.sum(dZ^{[1]},axis=1,keepdims=True)$


# 直观理解反向传播-逻辑回归
正向传播：

$$
\left.
	\begin{array}{l}
	{x }\\
	{w }\\
	{b }
	\end{array}
	\right\}
	\implies{z={w}^Tx+b}
	\implies{\alpha = \sigma(z)} 
	\implies{{L}\left(a,y \right)}
$$

反向传播：

$$
\large \underbrace{
	\left.
	\begin{array}{l}
	{x}\\
	{w}\\
	{b}
	\end{array}
	\right\}
	}_{dw=dz\cdot x, db =dz}
	\impliedby\underbrace{z=w^Tx+b}_{dz=da\cdot g^{'}(z),
	g(z)=\sigma(z),
	\frac{dL}{dz}=\frac{dL}{da}\cdot\frac{da}{dz},
	\frac{d}{ dz}g(z)=g^{'}(z)}
	\impliedby\underbrace{a = \sigma(z) 
	\impliedby{L(a,y)}}_{da=\frac{d}{da}L\left(a,y \right)=(-y\log{\alpha} - (1 - y)\log(1 - a))^{'}={-\frac{y}{a}} + {\frac{1 - y}{1 - a}{}} }
$$

# 随机初始化
当你训练神经网络时，权重随机初始化是很重要的。对于逻辑回归，把权重初始化为0当然也是可以的。但是对于一个神经网络，如果你把权重或者参数都初始化为0，那么梯度下降将不会起作用。

如，有两个输入特征，$n^{[0]} = 2$，2个隐藏层单元$n^{[1]}$就等于2。
因此与一个隐藏层相关的矩阵，或者说$W^{[1]}$是2\*2的矩阵，假设把它初始化为0的2\*2矩阵，$b^{[1]}$也等于 $[0\;0]^T$，把偏置项$b$初始化为0是合理的，但是把$w$初始化为0就有问题了。

如果这样初始化的话，你总是会发现$a_{1}^{[1]}$ 和 $a_{2}^{[1]}$相等，两个激活单元就会一样。因为两个隐含单元计算同样的函数，当你做反向传播计算时，这会导致$\text{dz}_{1}^{[1]}$ 和 $\text{dz}_{2}^{[1]}$也会一样，这样输出的权值也会一模一样，由此$W^{[2]}$等于$[0\;0]$；

<img src="../images/L1_week3_13.png">

所以说，如果这样初始化这个神经网络，那么这两个隐含单元就会完全一样，因此他们完全对称，也就意味着计算同样的函数，并且肯定的是最终经过每次训练的迭代，这两个隐含单元仍然是同一个函数。$dW$会是一个这样的矩阵，每一行有同样的值因此我们做权重更新把权重$W^{[1]}:={W^{[1]}-adW}$每次迭代后的$W^{[1]}$，第一行等于第二行。

这个问题的解决方法就是随机初始化参数：把$W^{[1]}$设为`np.random.randn(2,2)`(生成高斯分布)，通常再乘上一个小的数，比如0.01，这样把它初始化为很小的随机数。然后$b$没有这个对称的问题，所以可以把 $b$ 初始化为0，因为只要随机初始化$W$你就有不同的隐含单元计算不同的东西，因此不会有对称问题了。相似的，对于$W^{[2]}$你可以随机初始化，$b^{[2]}$可以初始化为0。

$W^{[1]} = np.random.randn(2,2)*0.01$

$b^{[1]} = np.zeros((2,1))$

$W^{[2]} = np.random.randn(2,2)\;*\;0.01$

$b^{[2]} = 0$

为什么是0.01，而不是100或者1000。我们通常倾向于初始化为很小的随机数。因为如果你用**tanh**或者**sigmoid**激活函数，或者说只在输出层有一个**Sigmoid**，如果（数值）波动太大，当你计算激活值时$z^{[1]} = W^{[1]}x + b^{[1]}\;,\;a^{[1]} = \sigma(z^{[1]})=g^{[1]}(z^{[1]})$如果$W$很大，$z$就会很大或者很小，因此这种情况下你很可能停在**tanh**/**sigmoid**函数的平坦的地方，这些地方梯度很小也就意味着梯度下降会很慢，因此学习也就很慢。

# 实现一个浅层神经网络
本节我们实现一个包含一个隐含层的浅层神经网络。这里我们能够直观的看到浅层神经网络模型和逻辑回归模型的不同之处。

本节我们能够学到：
1. 实现一个包含一个隐含层的二分类浅层神经网络；
2. 练习使用**$tanh$**等非线性激活单元；
3. 计算交叉熵损失值；
4. 实现模型的前向和后向传播；

## 库包和环境的配置


```python
import numpy as np
import matplotlib.pyplot as plt
import sklearn
import sklearn.datasets
import sklearn.linear_model
from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets
plt.rcParams['font.sans-serif'] = ['SimHei'] #指定默认字体,用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False #解决保存图像是负号'-'显示为方块的问题
%matplotlib inline
np.random.seed(1) 
```

## 数据集


```python
X,Y=load_planar_dataset()
```


```python
X.shape,Y.shape
```




    ((2, 400), (1, 400))




```python
plt.figure(figsize=(6,6))
plt.scatter(X[[0],:],X[[1],:],c=Y,s=40,cmap=plt.cm.Spectral)
```




    <matplotlib.collections.PathCollection at 0x233931ec940>




    
![png](1.3.%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_files/1.3.%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_19_1.png)
    


## 逻辑回归模型


```python
clf=sklearn.linear_model.LogisticRegressionCV(cv=3)
clf.fit(X.T,Y.T.ravel())
```




    LogisticRegressionCV(Cs=10, class_weight=None, cv=3, dual=False,
               fit_intercept=True, intercept_scaling=1.0, max_iter=100,
               multi_class='warn', n_jobs=None, penalty='l2',
               random_state=None, refit=True, scoring=None, solver='lbfgs',
               tol=0.0001, verbose=0)




```python
plt.figure(figsize=(6,6))
plot_decision_boundary(clf.predict,X,Y.ravel())
plt.title("逻辑回归")
```




    Text(0.5, 1.0, '逻辑回归')




    
![png](1.3.%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_files/1.3.%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_22_1.png)
    



```python
LR_predictions = clf.predict(X.T)
print('逻辑回归模型的准确率: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) +'%')
```

    逻辑回归模型的准确率: 47 %


## 神经网络模型
### **NN图示:**
<img src="images/classification_kiank.png" style="width:600px;height:300px;">

### **NN数学公式:**

$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\tag{1}$$ 
$$a^{[1] (i)} = \tanh(z^{[1] (i)})\tag{2}$$
$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\tag{3}$$
$$\hat{y}^{(i)} = a^{[2] (i)} = \sigma(z^{ [2] (i)})\tag{4}$$
$$y^{(i)}_{prediction} = \begin{cases} 1 & \mbox{if } a^{[2](i)} > 0.5 \\ 0 & \mbox{otherwise } \end{cases}\tag{5}$$

代价函数(成本函数): 
$$J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large\left(\small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right)  \large  \right) \small \tag{6}$$

### 建立神经网络的一般步骤：
1. 定义神经网络结构（输入层，隐含层和输出层）；
2. 初始化模型参数；
3. 迭代：
    * 执行前向传播；
    * 计算成本值；
    * 执行后向传播，获得梯度下降值；
    * 梯度下降，更新参数；

### 定义神经网络的结构
定义变量：
    - n_x: 输入层特征维数
    - n_h: 隐含层节点个数 (设置为4) 
    - n_y: 输出层标签维数


```python
n_x,n_h,n_y=X.shape[0],4,Y.shape[0]
n_x,n_h,n_y
```




    (2, 4, 1)



### 初始化模型参数


```python
def initialize_parameters(n_x, n_h, n_y):
    """
    Argument:
    - n_x: 输入层特征维数
    - n_h: 隐含层节点个数 (设置为4) 
    - n_y: 输出层标签维数

    Returns:
    params -- 参数字典:
                    W1 -- 隐含层权重矩阵 (n_h, n_x)
                    b1 -- 隐含层截距向量 (n_h, 1)
                    W2 -- 输出层权重矩阵 (n_y, n_h)
                    b2 -- 输出层截距向量 (n_y, 1)
    """
    np.random.seed(2)  # 固定随机种子
    W1 = np.random.randn(n_h, n_x) * 0.01
    b1 = np.zeros((n_h, 1))
    W2 = np.random.randn(n_y, n_h) * 0.01
    b2 = np.zeros((n_y, 1))
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}

    return parameters
```


```python
parameter=initialize_parameters(n_x,n_h,n_y)
parameter
```




    {'W1': array([[-0.00416758, -0.00056267],
            [-0.02136196,  0.01640271],
            [-0.01793436, -0.00841747],
            [ 0.00502881, -0.01245288]]), 'b1': array([[0.],
            [0.],
            [0.],
            [0.]]), 'W2': array([[-0.01057952, -0.00909008,  0.00551454,  0.02292208]]), 'b2': array([[0.]])}



### 迭代
#### 前向传播


```python
def forward_propagation(X, parameters):
    """
    Argument:
    X -- 特征数据集，输入数据 (n_x, m)
    parameters -- 参数字典
    
    Returns:
    A2 -- 预测值
    cache -- 中间变量： "Z1", "A1", "Z2" 和 "A2"
    """
    # 获取参数
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    # 前向传播
    Z1 = np.dot(W1,X) + b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2,A1) + b2
    A2 = sigmoid(Z2)
    # 中间变量
    cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}    
    return A2, cache
```

#### 计算代价-成本值
$$J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large{(} \small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \large{)} \small\tag{13}$$


```python
def compute_cost(A2, Y, parameters):
    """
    计算交叉熵损失值-成本值

    Arguments:
    A2 -- 标签预测值
    Y --  标签真实值
    parameters -- 参数

    Returns:
    cost -- 交叉熵损失值-成本值
    """
    m = Y.shape[1]  # 训练集样本数
    # 计算交叉熵损失值-成本值
    logprobs = Y*np.log(A2) + (1-Y) * np.log(1-A2)
    cost = -1/m * np.sum(logprobs)
    cost = np.squeeze(cost)
    return cost
```

#### 后向传播
<img src="images/grad_summary.png" style="width:600px;height:300px;">


```python
def backward_propagation(parameters, cache, X, Y):
    """
    后向传播    
    Arguments:
    parameters -- 参数字典
    cache -- 中间变量： "Z1", "A1", "Z2" 和 "A2"
    X -- 特征数据集，输入数据 (n_x, m)
    Y -- 标签数据集，(1, m)
    
    Returns:
    grads -- 对应参数的梯度下降值
    """
    m = X.shape[1]#样本数量
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    A1 = cache["A1"]
    A2 = cache["A2"]
    dZ2= A2 - Y
    dW2 = 1 / m * np.dot(dZ2,A1.T)
    db2 = 1 / m * np.sum(dZ2,axis=1,keepdims=True)
    dZ1 = np.dot(W2.T,dZ2) * (1-np.power(A1,2))
    dW1 = 1 / m * np.dot(dZ1,X.T)
    db1 = 1 / m * np.sum(dZ1,axis=1,keepdims=True)   
    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}    
    return grads
```

#### 梯度下降
1. 主要工作，使用梯度下降值：(dW1, db1, dW2, db2)，更新参数：(W1, b1, W2, b2)；
2. 梯度下降法则：$ \theta = \theta - \alpha \frac{\partial J }{ \partial \theta }$，$\alpha$表示学习率（步长）
<table style="border:none">
    <tr>
        <td><img src="images/sgd.gif" style="width:500;height:500;"></td>
        <td><img src="images/sgd_bad.gif" style="width:500;height:500;"></td>
    </tr>
</table>


```python
def update_parameters(parameters, grads, learning_rate = 1.2):
    """
    梯度下降更新参数    
    Arguments:
    parameters -- 参数字典
    grads --对应参数的梯度下降值    
    Returns:
    parameters -- 更新后的参数字典
    """
    # 获取参数值
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    # 后去梯度下降值
    dW1 = grads["dW1"]
    db1 = grads["db1"]
    dW2 = grads["dW2"]
    db2 = grads["db2"]
    # 更新参数值
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}    
    return parameters
```

### 功能组合


```python
def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):
    """
    Arguments:
    X -- 输入数据-特征数据集
    Y -- 真实标签数据集
    n_h -- 隐含层节点数
    num_iterations -- 梯度下降迭代次数
    print_cost -- if True, print the cost every 1000 iterations
    
    Returns:
    parameters -- 训练好的模型参数. 可以用来预测测试集.
    """    
    np.random.seed(3)
    
    n_x = X.shape[0]
    n_y = Y.shape[0]
    parameters = initialize_parameters(n_x, n_h, n_y)
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    # 迭代梯度下降
    for i in range(0, num_iterations):
        # 前向传播
        A2, cache = forward_propagation(X, parameters)        
        # 计算代价-成本值
        cost = compute_cost(A2, Y, parameters) 
        # 后向传播
        grads = backward_propagation(parameters, cache, X, Y) 
        # 梯度下降，梯度更新
        parameters = update_parameters(parameters, grads)
        if print_cost and i % 1000 == 0:
            print ("第%i次迭代的成本值: %f" %(i, cost))
    return parameters
```

### 预测
标签预测值 = $\hat y = \begin{cases}
      1 & \text{if}\ activation > 0.5 \\
      0 & \text{otherwise}
    \end{cases}$  


```python
def predict(parameters, X):
    A2, cache = forward_propagation(X, parameters)
    predictions = np.round(A2)  # 小数四舍五入为整数
    return predictions
```


```python
# 建立一个隐含层具有4个节点的神经网络
parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)
# 查看预测效果
plt.figure(figsize=(6,6))
plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y.ravel())
plt.title("{}个节点".format(n_h))
```

    第0次迭代的成本值: 0.693048
    第1000次迭代的成本值: 0.288083
    第2000次迭代的成本值: 0.254385
    第3000次迭代的成本值: 0.233864
    第4000次迭代的成本值: 0.226792
    第5000次迭代的成本值: 0.222644
    第6000次迭代的成本值: 0.219731
    第7000次迭代的成本值: 0.217504
    第8000次迭代的成本值: 0.219440
    第9000次迭代的成本值: 0.218553





    Text(0.5, 1.0, '4个节点')




    
![png](1.3.%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_files/1.3.%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_42_2.png)
    


### 调整隐含层节点个数


```python
plt.figure(figsize=(18, 18))
hidden_layer_sizes = [1, 2, 3, 5, 8, 13, 21,34,55]
n=np.sqrt(len(hidden_layer_sizes))
# hidden_layer_sizes = [1, 2, 3, 5]
for i, n_h in enumerate(hidden_layer_sizes):
    plt.subplot(n, n, i+1)
    plt.title('Hidden Layer of size %d' % n_h)
    parameters = nn_model(X, Y, n_h, num_iterations = 5000)
    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y.ravel())
    predictions = predict(parameters, X)
    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)
    print ("Accuracy for {} hidden units: {} %".format(n_h, accuracy))
plt.subplots_adjust(wspace=0,hspace=0)
```

    Accuracy for 1 hidden units: 67.5 %
    Accuracy for 2 hidden units: 67.25 %
    Accuracy for 3 hidden units: 90.75 %
    Accuracy for 5 hidden units: 91.25 %
    Accuracy for 8 hidden units: 91.0 %
    Accuracy for 13 hidden units: 91.0 %
    Accuracy for 21 hidden units: 90.75 %
    Accuracy for 34 hidden units: 90.5 %
    Accuracy for 55 hidden units: 90.25 %



    
![png](1.3.%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_files/1.3.%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_44_1.png)
    


## 模型在其他数据集上的表现


```python
noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()
datasets = {"noisy_circles": noisy_circles,
            "noisy_moons": noisy_moons,
            "blobs": blobs,
            "gaussian_quantiles": gaussian_quantiles}
dataset = "blobs"
X, Y = datasets[dataset]
X, Y = X.T, Y.reshape(1, Y.shape[0])
if dataset == "blobs":
    Y = Y%2
plt.figure(figsize=(6,6))
plt.scatter(X[0, :], X[1, :], c=Y.ravel(), s=40, cmap=plt.cm.Spectral);
```


    
![png](1.3.%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_files/1.3.%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_46_0.png)
    



```python
plt.figure(figsize=(18, 12))
hidden_layer_sizes = [1, 2, 3, 5, 8, 13]
for i, n_h in enumerate(hidden_layer_sizes):
    plt.subplot(2, 3, i+1)
    plt.title('Hidden Layer of size %d' % n_h)
    parameters = nn_model(X, Y, n_h, num_iterations = 5000)
    plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y.ravel())
    predictions = predict(parameters, X)
    accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100)
    print ("Accuracy for {} hidden units: {} %".format(n_h, accuracy))
plt.subplots_adjust(wspace=0,hspace=0)
```

    Accuracy for 1 hidden units: 67.0 %
    Accuracy for 2 hidden units: 67.0 %
    Accuracy for 3 hidden units: 83.0 %
    Accuracy for 5 hidden units: 83.0 %
    Accuracy for 8 hidden units: 82.5 %
    Accuracy for 13 hidden units: 82.5 %



    
![png](1.3.%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_files/1.3.%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_47_1.png)
    


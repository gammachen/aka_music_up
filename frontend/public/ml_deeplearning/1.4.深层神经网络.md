<font size=6>1.4.æ·±å±‚ç¥ç»ç½‘ç»œ</font>
# å›é¡¾
1. é€»è¾‘å›å½’
2. å‚æ•°åˆå§‹åŒ–
2. å‰å‘ä¼ æ’­ã€æ¿€æ´»å•å…ƒã€æŸå¤±å‡½æ•°ã€ä»£ä»·å‡½æ•°ã€åå‘ä¼ æ’­
3. æ¢¯åº¦ä¸‹é™-å‚æ•°æ›´æ–°
4. å‘é‡åŒ–
5. etc

## é€»è¾‘å›å½’å’Œæµ…å±‚ç¥ç»ç½‘ç»œ
<img src='../images/7c1cc04132b946baec5487ba68242362.png'>

## ç¥ç»ç½‘ç»œå±‚æ•°çš„å®šä¹‰
1. ä»å·¦åˆ°å³
3. å»é™¤è¾“å…¥å±‚
4. åŒ…å«è¾“å‡ºå±‚
4. é€»è¾‘å›å½’æ˜¯ä¸€ä¸ªä¸€å±‚çš„ç¥ç»ç½‘ç»œ
5. å±‚æ•°=éšå«å±‚æ•°+1
<img src='../images/be71cf997759e4aeaa4be1123c6bb6ba.png'>

## æ¡ˆä¾‹
1. $L:$è¡¨ç¤ºå±‚æ•°ï¼Œ$L=4$
2. $n^{[l]}:$è¡¨ç¤ºç¬¬$l$å±‚çš„èŠ‚ç‚¹(æˆ–ç‰¹å¾)æ•°ï¼Œeg.$n^{[1]}=5$,$n^{[2]}=5$ï¼Œ$n^{[3]}=3$ï¼Œ$n^{[4]}$=$n^{[L]}=1$ï¼ˆè¾“å‡ºå•å…ƒä¸º1ï¼‰ã€‚è€Œè¾“å…¥å±‚ï¼Œ${n}^{[0]}={n}_{x}=3$ã€‚
2. ${w}^{[l]}$æ¥è®°ä½œ*l*å±‚æƒé‡å‘é‡
3. ${z}^{[l]}$æ¥è®°ä½œ*l*å±‚å¤šå…ƒçº¿æ€§å›å½’å€¼
3. ${g}^{[l]}$æ¥è®°ä½œ*l*å±‚æ¿€æ´»å‡½æ•°
3. ${a}^{[l]}$æ¥è®°ä½œ*l*å±‚æ¿€æ´»å€¼
4. ...
<img src='../images/9927bcb34e8e5bfe872937fccd693081.png'>

## ç¬¦å·
<font size=3>
<ul>
<li>ä¸Šæ ‡ $^{(i)}$ ä»£è¡¨ç¬¬ $i$ ä¸ªè®­ç»ƒæ ·æœ¬</li>
<li>ä¸Šæ ‡ $^{[l]}$ ä»£è¡¨ç¬¬ $l$ å±‚</li>
<li>$m$ æ•°æ®é›†çš„æ ·æœ¬æ•°</li>
<li>ä¸‹æ ‡ $_x$ è¾“å…¥æ•°æ®</li>
<li>ä¸‹æ ‡ $_y$ è¾“å‡ºæ•°æ®</li>
<li>$n_x$ è¾“å…¥å¤§å°</li>
<li>$n_y$ è¾“å‡ºå¤§å° (æˆ–è€…ç±»åˆ«æ•°)</li>
<li>$n_h^{[l]}$ ç¬¬ $l$ å±‚çš„éšè—å•å…ƒæ•°</li>
<li>$L$ ç¥ç»ç½‘ç»œçš„å±‚æ•°</li>
<li>$n_x = n^{[0]}$</li>
<li>$n_y = n^{[L]}$</li>
</font>

# å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
## å‰å‘ä¼ æ’­
### å•ä¸ªæ ·æœ¬
$z^{[l]}=W^{[l]}\cdot{a}^{[l-1]}+{b}^{[l]}$

$a^{[l]}=g^{[l]}\left(z^{[l]}\right)$
### å…¨ä½“æ ·æœ¬é›†-å‘é‡åŒ–
${Z}^{[l]}={W}^{[l]}\cdot {A}^{[l-1]}+{b}^{[l]}$

${A}^{[l]}={g}^{[l]}({Z}^{[l]})$
## åå‘ä¼ æ’­
### å•ä¸ªæ ·æœ¬
1. è¾“å…¥ï¼š$da^{[l]}$
2. è¾“å‡ºï¼š$da^{[l-1]}$,$dw^{[l]}$,$db^{[l]}$

åå‘ä¼ æ’­çš„æ­¥éª¤å¯ä»¥å†™æˆï¼š

ï¼ˆ1ï¼‰$dz^{[l]}=d{a}^{[l]}*{g^{[l]}}'({z}^{[l]})$

ï¼ˆ2ï¼‰$dw^{[l]}=d{z}^{[l]}\cdot{a}^{[l-1]}~$

ï¼ˆ3ï¼‰$db^{[l]}=d{z}^{[l]}~~$

ï¼ˆ4ï¼‰$da^{[l-1]}=w^{\left [ l \right ] T}\cdot{dz}^{[l]}$

ï¼ˆ5ï¼‰$dz^{[l]}=w^{[l+1]T}dz^{[l+1]}\cdot{g^{[l]}}'({z}^{[l]})$

å¼å­ï¼ˆ5ï¼‰ç”±å¼å­ï¼ˆ4ï¼‰å¸¦å…¥å¼å­ï¼ˆ1ï¼‰å¾—åˆ°ï¼Œå‰å››ä¸ªå¼å­å°±å¯å®ç°åå‘å‡½æ•°ã€‚
### å…¨ä½“æ ·æœ¬é›†-å‘é‡åŒ–
ï¼ˆ6ï¼‰$d{Z^{[l]}}=d{A^{[l]}}*{g^{\left[ l \right]}}'\left({Z^{[l]}} \right)~~$

ï¼ˆ7ï¼‰$d{W^{[l]}}=\frac{1}{m}\text{}d{Z^{[l]}}\cdot {A^{\left[ l-1 \right]T}}$

ï¼ˆ8ï¼‰$d{b^{[l]}}=\frac{1}{m}\text{ }np.sum(d{z^{[l]}},axis=1,keepdims=True)$

ï¼ˆ9ï¼‰$d{A^{[l-1]}}={W^{\left[ l \right]T}}.d{Z^{[l]}}$

<img src='../images/53a5b4c71c0facfc8145af3b534f8583.png'>

# çŸ©é˜µçš„ç»´æ•°
## å•ä¸ªæ ·æœ¬
$w$çš„ç»´åº¦æ˜¯ï¼ˆä¸‹ä¸€å±‚çš„ç»´æ•°ï¼Œå‰ä¸€å±‚çš„ç»´æ•°ï¼‰ï¼Œå³${w^{[l]}}$: (${n^{[l]}}$,${n^{[l-1]}}$)ï¼›

$b$çš„ç»´åº¦æ˜¯ï¼ˆä¸‹ä¸€å±‚çš„ç»´æ•°ï¼Œ1ï¼‰ï¼Œå³:${b^{[l]}}$ : (${n^{[l]}},1)$ï¼›

${z^{[l]}}$,${a^{[l]}}$: $({n^{[l]}},1)$;

${dw^{[l]}}$å’Œ${w^{[l]}}$ç»´åº¦ç›¸åŒï¼Œ${db^{[l]}}$å’Œ${b^{[l]}}$ç»´åº¦ç›¸åŒï¼Œä¸”$w$å’Œ$b$å‘é‡åŒ–ç»´åº¦ä¸å˜ï¼Œä½†$z$,$a$ä»¥åŠ$x$çš„ç»´åº¦ä¼šå‘é‡åŒ–åå‘ç”Ÿå˜åŒ–ã€‚
## æ ·æœ¬é›†å‘é‡åŒ–
$Z^{[l]}$å¯ä»¥çœ‹æˆç”±æ¯ä¸€ä¸ªå•ç‹¬çš„$z^{[l](i)}$å åŠ è€Œå¾—åˆ°ï¼Œ${Z}^{[l]}=({z^{[l](1)}}ï¼Œ{z^{[l](2)}}ï¼Œ{z^{[l](3)}}ï¼Œ\dotsï¼Œ{z^{[l](m)}})$ï¼Œ

$m$ä¸ºè®­ç»ƒé›†å¤§å°ï¼Œæ‰€ä»¥$Z^{[l]}$çš„ç»´åº¦ä¸å†æ˜¯$({n^{[l]}},1)$ï¼Œè€Œæ˜¯$({n^{[l]}},m)$ã€‚

$A^{[l]}$ï¼š$(n^{[l]},m)$ï¼Œ$A^{[0]} = X =(n^{[l]},m)$

# å‚æ•°VSè¶…å‚æ•°
1. å‚æ•°:$w,b$
2. è¶…å‚æ•°:
    * **learning rate** $a$ï¼ˆå­¦ä¹ ç‡ï¼‰
    * **iterations**(æ¢¯åº¦ä¸‹é™æ³•å¾ªç¯çš„æ•°é‡)
    * $L$ï¼ˆç¥ç»ç½‘å’¯çš„å±‚æ•°ï¼‰ã€${n^{[l]}}$ï¼ˆéšè—å±‚å•å…ƒæ•°ç›®ï¼‰
    * **choice of activation function**ï¼ˆæ¿€æ´»å‡½æ•°çš„é€‰æ‹©ï¼‰

è¶…å‚æ•°å®é™…ä¸Šæ§åˆ¶äº†æœ€åçš„å‚æ•°$W$å’Œ$b$çš„å€¼ï¼Œæ‰€ä»¥å®ƒä»¬è¢«ç§°ä½œè¶…å‚æ•°ã€‚

# åˆ›å»ºæ·±å±‚ç¥ç»ç½‘ç»œ
## åº“åŒ…å’Œç¯å¢ƒé…ç½®


```python
import numpy as np
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei'] #æŒ‡å®šé»˜è®¤å­—ä½“,ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾
plt.rcParams['axes.unicode_minus'] = False #è§£å†³ä¿å­˜å›¾åƒæ˜¯è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜
%matplotlib inline
np.random.seed(1) 
```

## ä¸šåŠ¡å…¨æ™¯
<img src="images/final outline.png" style="width:800px;height:500px;">

## å‚æ•°åˆå§‹åŒ–


```python
def initialize_parameters_deep(layer_dims):
    """
    Arguments:
    layer_dims -- (list) ä»0åˆ°Lï¼Œæ¯ä¸€å±‚çš„ç‰¹å¾(èŠ‚ç‚¹)æ•°
    
    Returns:
    parameters -- (dictionary) æ¯ä¸€å±‚å‚æ•°å€¼ "W1", "b1", ..., "WL", "bL":
                    Wl -- ç¬¬lå±‚çš„æƒé‡çŸ©é˜µï¼Œshape= (layer_dims[l], layer_dims[l-1])
                    bl -- ç¬¬lå±‚çš„æˆªè·å‘é‡ï¼Œshape (layer_dims[l], 1)
    """   
    np.random.seed(1)
    parameters = {}
    L = len(layer_dims)# å±‚æ•°+1
    for l in range(1, L):
        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])/ np.sqrt(layer_dims[l-1])
        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))
    return parameters
```


```python
initialize_parameters_deep([3,2,1])
```




    {'W1': array([[ 0.93781623, -0.35319773, -0.3049401 ],
            [-0.61947872,  0.49964333, -1.32879399]]), 'b1': array([[0.],
            [0.]]), 'W2': array([[ 1.23376823, -0.53825456]]), 'b2': array([[0.]])}



## å‰å‘ä¼ æ’­
### å¤šå…ƒçº¿æ€§å‡½æ•°


```python
def linear_forward(A, W, b):
    """
    è®¡ç®—æŸå±‚çš„å¤šå…ƒçº¿æ€§å€¼
    Arguments:
    A -- å‰ä¸€å±‚çš„æ¿€æ´»å€¼ (æˆ–è€…è¾“å…¥å€¼)ï¼Œå½¢çŠ¶=(å‰ä¸€å±‚çš„èŠ‚ç‚¹æ•°, æ ·æœ¬æ•°ç›®)
    W -- å½“å‰å±‚æƒé‡çŸ©é˜µï¼Œå½¢çŠ¶= (å½“å‰å±‚èŠ‚ç‚¹æ•°, å‰ä¸€å±‚çš„èŠ‚ç‚¹æ•°)
    b -- å½“å‰å±‚æˆªè·å‘é‡ï¼Œå½¢çŠ¶= (å½“å‰å±‚èŠ‚ç‚¹æ•°, 1)

    Returns:
    Z -- å¤šå…ƒçº¿æ€§å›å½’å€¼ï¼Œå³å½“å‰å±‚æ¿€æ´»å‡½æ•°çš„è¾“å…¥å€¼ï¼Œåˆå«é¢„æ¿€æ´»å€¼
    cache -- ï¼ˆdictionaryï¼‰ å­˜å‚¨ "A", "W" and "b" ç­‰å€¼ï¼Œ ä»¥ä¾¿åé¢åå‘ä¼ æ’­ä½¿ç”¨
    """
    Z = np.dot(W,A) + b
    cache = (A, W, b)
    return Z, cache
```

### æ¿€æ´»å‡½æ•°


```python
def sigmoid(Z):  
    A = 1/(1+np.exp(-Z))
    cache = Z    
    return A, cache

def relu(Z):    
    A = np.maximum(0,Z)    
    cache = Z 
    return A, cache
```

### å‰å‘ä¼ æ’­å‡½æ•°


```python
def linear_activation_forward(A_prev, W, b, activation):
    """
    æ‰§è¡Œç¬¬lå±‚çš„å‰å‘ä¼ æ’­
    Arguments:
    A_prev -- å‰ä¸€å±‚çš„æ¿€æ´»å€¼ (æˆ–è€…è¾“å…¥å€¼)ï¼Œå½¢çŠ¶=(å‰ä¸€å±‚çš„èŠ‚ç‚¹æ•°, æ ·æœ¬æ•°ç›®)
    W -- å½“å‰å±‚æƒé‡çŸ©é˜µï¼Œå½¢çŠ¶= (å½“å‰å±‚èŠ‚ç‚¹æ•°, å‰ä¸€å±‚çš„èŠ‚ç‚¹æ•°)
    b -- å½“å‰å±‚æˆªè·å‘é‡ï¼Œå½¢çŠ¶= (å½“å‰å±‚èŠ‚ç‚¹æ•°, 1)
    activation -- å½“å‰å±‚çš„æ¿€æ´»å‡½æ•°ç±»å‹: "sigmoid" or "relu"

    Returns:
    A -- å¤šå…ƒçº¿æ€§å›å½’å€¼ï¼Œå³å½“å‰å±‚æ¿€æ´»å‡½æ•°çš„è¾“å…¥å€¼ï¼Œåˆå«é¢„æ¿€æ´»å€¼
    cache -- ï¼ˆdictionaryï¼‰ å­˜å‚¨ "linear_cache" and "activation_cache"ç­‰å€¼ï¼Œ ä»¥ä¾¿åé¢åå‘ä¼ æ’­ä½¿ç”¨
    """    
    if activation == "sigmoid":
        Z, linear_cache = linear_forward(A_prev,W,b)
        A, activation_cache = sigmoid(Z)  
    elif activation == "relu":
        Z, linear_cache = linear_forward(A_prev,W,b)
        A, activation_cache = relu(Z)
    cache = (linear_cache, activation_cache)
    return A, cache
```

### $L$å±‚å‰å‘ä¼ æ’­æ¨¡å‹


```python
def L_model_forward(X, parameters):
    """
    ç¥ç»ç½‘ç»œå‰å‘ä¼ æ’­
    
    Arguments:
    X -- ç‰¹å¾æ•°æ®é›†, å½¢çŠ¶=(ç‰¹å¾æ•°,æ ·æœ¬æ•°)
    parameters -- åˆå§‹åŒ–å‚æ•°ï¼šåˆå§‹åŒ–å‡½æ•°çš„è¾“å‡ºå€¼
    
    Returns:
    AL -- é¢„æµ‹å€¼ï¼šè¾“å‡ºå±‚çš„æ¿€æ´»å€¼
    caches -- ï¼ˆlistï¼‰ å­˜å‚¨:æ¯ä¸€å±‚çš„å¤šå…ƒçº¿æ€§å‡½æ•°å’Œæ¿€æ´»å‡½æ•°çš„è¿”å›å€¼
    """
    caches = []
    A = X
    L = len(parameters) // 2
    for l in range(1, L):
        A_prev = A 
        A, cache = linear_activation_forward(A_prev,parameters['W' + str(l)],parameters['b' + str(l)],activation = "relu")
        caches.append(cache)
    AL, cache = linear_activation_forward(A,parameters['W' + str(L)],parameters['b' + str(L)],activation = "sigmoid")
    caches.append(cache)            
    return AL, caches
```

## æˆæœ¬å‡½æ•°
æˆæœ¬å€¼$J$, è®¡ç®—å…¬å¼: $$-\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right))Â \tag{7}$$


```python
def compute_cost(AL, Y):
    """
    è®¡ç®—æ¨¡å‹æˆæœ¬å€¼.
    Arguments:
    AL -- æ ‡ç­¾é¢„æµ‹å€¼, å½¢çŠ¶=(æ ‡ç­¾ç»´æ•°=1, æ ·æœ¬æ•°=m)
    Y -- çœŸå®æ ‡ç­¾å€¼ï¼ˆ0 or 1ï¼‰, å½¢çŠ¶=(æ ‡ç­¾ç»´æ•°=1, æ ·æœ¬æ•°=m)
    Returns:
    cost -- äº¤å‰ç†µ
    """    
    m = Y.shape[1]
    cost = -1 / m * np.sum(Y * np.log(AL) + (1-Y) * np.log(1-AL),axis=1,keepdims=True)    
    cost = np.squeeze(cost) #è½¬ä¸ºæ ‡é‡å€¼
    return cost
```

## åå‘ä¼ æ’­
<img src="images/backprop_kiank.png" style="width:650px;height:250px;">

<img src="images/linearback_kiank.png" style="width:250px;height:300px;">
$$ dW^{[l]} = \frac{\partial \mathcal{L} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T}$$
$$ db^{[l]} = \frac{\partial \mathcal{L} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{[l](i)}$$
$$ dA^{[l-1]} = \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$$

### å¤šå…ƒçº¿æ€§å‡½æ•°å¾®åˆ†


```python
def linear_backward(dZ, cache):
    """
    ç¬¬lå±‚å¤šå…ƒçº¿æ€§å‡½æ•°çš„å¾®åˆ†
    Arguments:
    dZ -- ç¬¬lå±‚å¤šå…ƒçº¿æ€§å€¼çš„å¾®åˆ†
    cache -- ç¬¬lå±‚ï¼Œå‰å‘ä¼ æ’­çš„ç¼“å­˜å€¼-(A_prev, W, b)
    Returns:
    dA_prev -- ç¬¬l-1å±‚çš„æ¿€æ´»å€¼å¾—å¾®åˆ†, å½¢çŠ¶åŒA_prevï¼›
    dW -- å½“å‰å±‚æƒé‡çŸ©é˜µçš„å¾®åˆ†, å½¢çŠ¶åŒW
    db -- å½“å‰å±‚æˆªè·å‘é‡çš„å¾®åˆ†, å½¢çŠ¶åŒb
    """
    A_prev, W, b = cache
    m = A_prev.shape[1]
    dW = 1 / m * np.dot(dZ ,A_prev.T)
    db = 1 / m * np.sum(dZ,axis = 1 ,keepdims=True)
    dA_prev = np.dot(W.T,dZ) 
    return dA_prev, dW, db
```

### æ¿€æ´»å‡½æ•°å¾®åˆ†


```python
def relu_backward(dA, cache):
    Z = cache
    dZ = np.array(dA, copy=True)
    dZ[Z <= 0] = 0       
    return dZ
def sigmoid_backward(dA, cache):
    Z = cache    
    s = 1/(1+np.exp(-Z))
    dZ = dA * s * (1-s)    
    return dZ
```

### åå‘ä¼ æ’­å‡½æ•°å¾®åˆ†


```python
def linear_activation_backward(dA, cache, activation):
    """
    åå‘ä¼ æ’­    
    Arguments:
    dA -- å½“å‰å±‚æ¿€æ´»å€¼çš„å¾®åˆ†
    cache -- å½“å‰å±‚å‰å‘ä¼ æ’­çš„ç¼“å­˜å€¼ï¼š (linear_cache, activation_cache)
    activation --  å½“å‰å±‚çš„æ¿€æ´»å‡½æ•°ç±»å‹: "sigmoid" or "relu"    
    Returns:
    dA_prev -- ç¬¬l-1å±‚çš„æ¿€æ´»å€¼å¾—å¾®åˆ†, å½¢çŠ¶åŒA_prevï¼›
    dW -- å½“å‰å±‚æƒé‡çŸ©é˜µçš„å¾®åˆ†, å½¢çŠ¶åŒW
    db -- å½“å‰å±‚æˆªè·å‘é‡çš„å¾®åˆ†, å½¢çŠ¶åŒb
    """
    linear_cache, activation_cache = cache    
    if activation == "relu":
        dZ = relu_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)
    elif activation == "sigmoid":
        dZ = sigmoid_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)
    return dA_prev, dW, db
```

### $L$å±‚åå‘ä¼ æ’­æ¨¡å‹


```python
def L_model_backward(AL, Y, caches):
    """
    ç¥ç»ç½‘ç»œåå‘ä¼ æ’­    
    Arguments:
    AL -- æ ‡ç­¾é¢„æµ‹å€¼, å½¢çŠ¶=(æ ‡ç­¾ç»´æ•°=1, æ ·æœ¬æ•°=m)ï¼Œå‰å‘ä¼ æ’­çš„è¾“å‡ºå€¼
    Y -- çœŸå®æ ‡ç­¾å€¼ï¼ˆ0 or 1ï¼‰, å½¢çŠ¶=(æ ‡ç­¾ç»´æ•°=1, æ ·æœ¬æ•°=m)
    caches -- (list)å­˜å‚¨:æ¯ä¸€å±‚çš„å‰å‘ä¼ æ’­ä¸­å¤šå…ƒçº¿æ€§å‡½æ•°å’Œæ¿€æ´»å‡½æ•°çš„è¿”å›å€¼
    
    Returns:
    grads -- ï¼ˆdictionaryï¼‰æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ä¸‹é™å€¼ï¼Œå³æ‰€æœ‰å‚æ•°çš„å¾®åˆ†å€¼
    """
    grads = {}
    L = len(caches)
    m = AL.shape[1]
    Y = Y.reshape(AL.shape)
    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))
    current_cache = caches[L-1]
    grads["dA" + str(L)], grads["dW" + str(L)], grads["db" + str(L)] =\
    linear_activation_backward(dAL, current_cache, activation = "sigmoid")
    for l in reversed(range(L - 1)):
        current_cache = caches[l]
        dA_prev_temp, dW_temp, db_temp = \
        linear_activation_backward(grads["dA" + str(l+2)], current_cache, activation = "relu")
        grads["dA" + str(l + 1)] = dA_prev_temp
        grads["dW" + str(l + 1)] = dW_temp
        grads["db" + str(l + 1)] = db_temp
    return grads
```

### æ¢¯åº¦ä¸‹é™-æ›´æ–°å‚æ•°
$$ W^{[l]} := W^{[l]} - \alpha \text{ } dW^{[l]}$$
$$ b^{[l]} := b^{[l]} - \alpha \text{ } db^{[l]}$$


```python
def update_parameters(parameters, grads, learning_rate):
    """
    æ¢¯åº¦ä¸‹é™-æ›´æ–°å‚æ•°    
    Arguments:
    parameters -- æ›´æ–°å‰çš„å‚æ•°
    grads -- åå‘ä¼ æ’­çš„è¾“å‡º-æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ä¸‹é™å€¼    
    Returns:
    parameters -- æ›´æ–°åçš„å‚æ•°
    """    
    L = len(parameters) // 2
    for l in range(L):
        parameters["W" + str(l+1)] =  parameters["W" + str(l+1)] - learning_rate * grads["dW" + str(l + 1)]
        parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * grads["db" + str(l + 1)]
    return parameters
```

# åº”ç”¨æ·±å±‚ç¥ç»ç½‘ç»œå¯¹å›¾ç‰‡è¿›è¡Œåˆ†ç±»
## æ•°æ®


```python
import h5py
def load_data():
    train_dataset = h5py.File('datasets/train_catvnoncat.h5', "r")
    train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # è®­ç»ƒé›†çš„ç‰¹å¾
    train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # è®­ç»ƒé›†çš„æ ‡ç­¾

    test_dataset = h5py.File('datasets/test_catvnoncat.h5', "r")
    test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # æµ‹è¯•é›†çš„ç‰¹å¾
    test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # æµ‹è¯•é›†çš„æ ‡ç­¾

    classes = np.array(test_dataset["list_classes"][:])# åˆ†ç±»ç±»å‹
    
    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))
    
    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes
```


```python
train_x_orig, train_y, test_x_orig, test_y, classes = load_data()
```


```python
#å±•ç¤ºå›¾ç‰‡
index = 7
plt.figure(figsize=(3,3))
plt.imshow(train_x_orig[index])
print ("y = " + str(train_y[0,index]) + ". It's a " + classes[train_y[0,index]].decode("utf-8") +  " picture.")
```

    y = 1. It's a cat picture.



    
![png](1.4.%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_files/1.4.%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_34_1.png)
    



```python
train_x_orig.shape, train_y.shape, test_x_orig.shape, test_y.shape, classes.shape
```




    ((209, 64, 64, 3), (1, 209), (50, 64, 64, 3), (1, 50), (2,))



## æ•°æ®å¤„ç†
<img src="images/imvectorkiank.png" style="width:450px;height:300px;">

### æ•°æ®è§„æ•´


```python
train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T 
test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T
```


```python
train_x_flatten.shape,test_x_flatten.shape
```




    ((12288, 209), (12288, 50))



### æ•°æ®æ ‡å‡†åŒ–


```python
train_x = train_x_flatten/255.
test_x = test_x_flatten/255.
```


```python
train_x.max(),test_x.max()
```




    (1.0, 1.0)



## $L$å±‚ç¥ç»ç½‘ç»œæ¨¡å‹
### è®­ç»ƒ


```python
def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 2000, print_cost=False):
    """
    è®­ç»ƒğ¿ å±‚ç¥ç»ç½‘ç»œæ¨¡å‹    
    Arguments:
    X -- ç‰¹å¾æ•°æ®é›†, å½¢çŠ¶=(ç‰¹å¾æ•°,æ ·æœ¬æ•°)
    Y -- çœŸå®æ ‡ç­¾å€¼ï¼ˆ0 or 1ï¼‰, å½¢çŠ¶=(æ ‡ç­¾ç»´æ•°=1, æ ·æœ¬æ•°=m)
    layers_dims -- (list) ä»0åˆ°Lï¼Œæ¯ä¸€å±‚çš„ç‰¹å¾(èŠ‚ç‚¹)æ•°
    learning_rate -- æ¢¯åº¦ä¸‹é™æ—¶è®¾ç½®çš„å­¦ä¹ ç‡
    num_iterations -- è¿­ä»£æ¬¡æ•°
    print_cost -- æ˜¯å¦å±•ç¤ºæˆæœ¬å€¼çš„å˜åŠ¨æƒ…å†µï¼Œå¦‚æœå±•ç¤ºï¼Œæ¯ä¸€ç™¾æ¬¡é‡‡é›†ä¸€ä¸ªç‚¹    
    Returns:
    parameters -- æ¨¡å‹å­¦ä¹ åˆ°çš„æœ€ä¼˜å‚æ•°ï¼Œå¯ä»¥ç”¨ä½œé¢„æµ‹.
    """
    np.random.seed(1)
    costs = []# ä¿å­˜æ¯ä¸€æ¬¡æ¢¯åº¦ä¸‹é™æ—¶çš„æˆæœ¬å€¼    
    # å‚æ•°åˆå§‹åŒ–.
    parameters = initialize_parameters_deep(layers_dims)    
    # å¾ªç¯æ¢¯åº¦ä¸‹é™
    for i in range(0, num_iterations):
        # å‰å‘ä¼ æ’­
        AL, caches = L_model_forward(X, parameters)
        # æˆæœ¬å€¼
        cost = compute_cost(AL, Y)
        # åå‘ä¼ æ’­
        grads = L_model_backward(AL, Y, caches)
        # æ¢¯åº¦ä¸‹é™-æ›´æ–°å‚æ•°.
        parameters = update_parameters(parameters, grads, learning_rate)
        # Print the cost every 100 training example
        if print_cost and i % 100 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))
        if print_cost and i % 100 == 0:
            costs.append(cost)            
    # å±•ç¤ºæˆæœ¬å€¼çš„ä¸‹é™
    plt.figure(figsize=(5,4))
    plt.plot(np.squeeze(costs))
    plt.ylabel('æˆæœ¬å€¼')
    plt.xlabel('è¿­ä»£(100æ¬¡)')
    plt.title("å­¦ä¹ ç‡=" + str(learning_rate))
    plt.show()    
    return parameters
```

### é¢„æµ‹


```python
def predict(X, y, parameters):
    """
    ä½¿ç”¨ğ¿ å±‚ç¥ç»ç½‘ç»œæ¨¡å‹è®­ç»ƒçš„å‚æ•°è¿›è¡Œé¢„æµ‹    
    Arguments:
    X -- æµ‹è¯•ç‰¹å¾æ•°æ®
    y -- æµ‹è¯•æ ‡ç­¾æ•°æ®
    parameters -- è®­ç»ƒæ¨¡å‹å¾—åˆ°çš„æœ€ä¼˜å‚æ•°    
    Returns:
    p -- æµ‹è¯•é›†é¢„æµ‹å‡†ç¡®ç‡
    """    
    m = X.shape[1]
    n = len(parameters) // 2 # æ·±å±‚ç¥ç»ç½‘ç»œæ¨¡å‹çš„å±‚æ•°
    p = np.zeros((1,m))
    probas, caches = L_model_forward(X, parameters)
    # é¢„æµ‹å€¼ç¦»æ•£åŒ–
    for i in range(0, probas.shape[1]):
        if probas[0,i] > 0.5:
            p[0,i] = 1
        else:
            p[0,i] = 0
    print("å‡†ç¡®ç‡:"  + str(np.sum((p == y)/m)))
    return p
```

## ä¸¤å±‚ç¥ç»ç½‘ç»œçš„åº”ç”¨


```python
parameters = L_layer_model(train_x, train_y, [train_x.shape[0],7,1], num_iterations = 2500, print_cost = True)
```

    Cost after iteration 0: 0.695046
    Cost after iteration 100: 0.589260
    Cost after iteration 200: 0.523261
    Cost after iteration 300: 0.449769
    Cost after iteration 400: 0.420900
    Cost after iteration 500: 0.372464
    Cost after iteration 600: 0.347421
    Cost after iteration 700: 0.317192
    Cost after iteration 800: 0.266438
    Cost after iteration 900: 0.219914
    Cost after iteration 1000: 0.143579
    Cost after iteration 1100: 0.453092
    Cost after iteration 1200: 0.094994
    Cost after iteration 1300: 0.080141
    Cost after iteration 1400: 0.069402
    Cost after iteration 1500: 0.060217
    Cost after iteration 1600: 0.053274
    Cost after iteration 1700: 0.047629
    Cost after iteration 1800: 0.042976
    Cost after iteration 1900: 0.039036
    Cost after iteration 2000: 0.035683
    Cost after iteration 2100: 0.032915
    Cost after iteration 2200: 0.030472
    Cost after iteration 2300: 0.028388
    Cost after iteration 2400: 0.026615



    
![png](1.4.%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_files/1.4.%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_47_1.png)
    



```python
pred_train=predict(train_x,train_y,parameters)
```

    å‡†ç¡®ç‡:0.9999999999999998



```python
pred_test=predict(test_x,test_y,parameters)
```

    å‡†ç¡®ç‡:0.74


## å››å±‚ç¥ç»ç½‘ç»œçš„åº”ç”¨


```python
parameters = L_layer_model(train_x, train_y, [train_x.shape[0],20, 7, 5, 1], num_iterations = 2500, print_cost = True)
```

    Cost after iteration 0: 0.771749
    Cost after iteration 100: 0.672053
    Cost after iteration 200: 0.648263
    Cost after iteration 300: 0.611507
    Cost after iteration 400: 0.567047
    Cost after iteration 500: 0.540138
    Cost after iteration 600: 0.527930
    Cost after iteration 700: 0.465477
    Cost after iteration 800: 0.369126
    Cost after iteration 900: 0.391747
    Cost after iteration 1000: 0.315187
    Cost after iteration 1100: 0.272700
    Cost after iteration 1200: 0.237419
    Cost after iteration 1300: 0.199601
    Cost after iteration 1400: 0.189263
    Cost after iteration 1500: 0.161189
    Cost after iteration 1600: 0.148214
    Cost after iteration 1700: 0.137775
    Cost after iteration 1800: 0.129740
    Cost after iteration 1900: 0.121225
    Cost after iteration 2000: 0.113821
    Cost after iteration 2100: 0.107839
    Cost after iteration 2200: 0.102855
    Cost after iteration 2300: 0.100897
    Cost after iteration 2400: 0.092878



    
![png](1.4.%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_files/1.4.%E6%B7%B1%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C_51_1.png)
    



```python
pred_train=predict(train_x,train_y,parameters)
```

    å‡†ç¡®ç‡:0.9856459330143539



```python
pred_test=predict(test_x,test_y,parameters)
```

    å‡†ç¡®ç‡:0.8

